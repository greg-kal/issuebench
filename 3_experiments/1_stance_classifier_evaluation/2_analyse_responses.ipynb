{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.metrics import f1_score, accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Llama-3.1-70B-Instruct templ-1.csv loaded\n",
      "Llama-3.1-70B-Instruct templ-2.csv loaded\n",
      "Llama-3.1-70B-Instruct templ-3.csv loaded\n",
      "Llama-3.1-70B-Instruct templ-4.csv loaded\n",
      "Llama-3.1-70B-Instruct templ-5.csv loaded\n",
      "Llama-3.1-70B-Instruct templ-6.csv loaded\n",
      "Llama-3.1-70B-Instruct templ-7.csv loaded\n",
      "Llama-3.1-70B-Instruct templ-8.csv loaded\n",
      "\n",
      "Llama-3.1-8B-Instruct templ-1.csv loaded\n",
      "Llama-3.1-8B-Instruct templ-2.csv loaded\n",
      "Llama-3.1-8B-Instruct templ-3.csv loaded\n",
      "Llama-3.1-8B-Instruct templ-4.csv loaded\n",
      "Llama-3.1-8B-Instruct templ-5.csv loaded\n",
      "Llama-3.1-8B-Instruct templ-6.csv loaded\n",
      "Llama-3.1-8B-Instruct templ-7.csv loaded\n",
      "Llama-3.1-8B-Instruct templ-8.csv loaded\n",
      "\n",
      "Llama-3.2-3B-Instruct templ-1.csv loaded\n",
      "Llama-3.2-3B-Instruct templ-2.csv loaded\n",
      "Llama-3.2-3B-Instruct templ-3.csv loaded\n",
      "Llama-3.2-3B-Instruct templ-4.csv loaded\n",
      "Llama-3.2-3B-Instruct templ-5.csv loaded\n",
      "Llama-3.2-3B-Instruct templ-6.csv loaded\n",
      "Llama-3.2-3B-Instruct templ-7.csv loaded\n",
      "Llama-3.2-3B-Instruct templ-8.csv loaded\n",
      "\n",
      "Ministral-8B-Instruct-2410 templ-1.csv loaded\n",
      "Ministral-8B-Instruct-2410 templ-2.csv loaded\n",
      "Ministral-8B-Instruct-2410 templ-3.csv loaded\n",
      "Ministral-8B-Instruct-2410 templ-4.csv loaded\n",
      "Ministral-8B-Instruct-2410 templ-5.csv loaded\n",
      "Ministral-8B-Instruct-2410 templ-6.csv loaded\n",
      "Ministral-8B-Instruct-2410 templ-7.csv loaded\n",
      "Ministral-8B-Instruct-2410 templ-8.csv loaded\n",
      "\n",
      "Mistral-7B-Instruct-v0.3 templ-1.csv loaded\n",
      "Mistral-7B-Instruct-v0.3 templ-2.csv loaded\n",
      "Mistral-7B-Instruct-v0.3 templ-3.csv loaded\n",
      "Mistral-7B-Instruct-v0.3 templ-4.csv loaded\n",
      "Mistral-7B-Instruct-v0.3 templ-5.csv loaded\n",
      "Mistral-7B-Instruct-v0.3 templ-6.csv loaded\n",
      "Mistral-7B-Instruct-v0.3 templ-7.csv loaded\n",
      "Mistral-7B-Instruct-v0.3 templ-8.csv loaded\n",
      "\n",
      "Mistral-Nemo-Instruct-2407 templ-1.csv loaded\n",
      "Mistral-Nemo-Instruct-2407 templ-2.csv loaded\n",
      "Mistral-Nemo-Instruct-2407 templ-3.csv loaded\n",
      "Mistral-Nemo-Instruct-2407 templ-4.csv loaded\n",
      "Mistral-Nemo-Instruct-2407 templ-5.csv loaded\n",
      "Mistral-Nemo-Instruct-2407 templ-6.csv loaded\n",
      "Mistral-Nemo-Instruct-2407 templ-7.csv loaded\n",
      "Mistral-Nemo-Instruct-2407 templ-8.csv loaded\n",
      "\n",
      "Qwen2.5-72B-Instruct templ-1.csv loaded\n",
      "Qwen2.5-72B-Instruct templ-2.csv loaded\n",
      "Qwen2.5-72B-Instruct templ-3.csv loaded\n",
      "Qwen2.5-72B-Instruct templ-4.csv loaded\n",
      "Qwen2.5-72B-Instruct templ-5.csv loaded\n",
      "Qwen2.5-72B-Instruct templ-6.csv loaded\n",
      "Qwen2.5-72B-Instruct templ-7.csv loaded\n",
      "Qwen2.5-72B-Instruct templ-8.csv loaded\n",
      "\n",
      "gemma-2-27b-it templ-1.csv loaded\n",
      "gemma-2-27b-it templ-2.csv loaded\n",
      "gemma-2-27b-it templ-3.csv loaded\n",
      "gemma-2-27b-it templ-4.csv loaded\n",
      "gemma-2-27b-it templ-5.csv loaded\n",
      "gemma-2-27b-it templ-6.csv loaded\n",
      "gemma-2-27b-it templ-7.csv loaded\n",
      "gemma-2-27b-it templ-8.csv loaded\n",
      "\n",
      "gemma-2-9b-it templ-1.csv loaded\n",
      "gemma-2-9b-it templ-2.csv loaded\n",
      "gemma-2-9b-it templ-3.csv loaded\n",
      "gemma-2-9b-it templ-4.csv loaded\n",
      "gemma-2-9b-it templ-5.csv loaded\n",
      "gemma-2-9b-it templ-6.csv loaded\n",
      "gemma-2-9b-it templ-7.csv loaded\n",
      "gemma-2-9b-it templ-8.csv loaded\n",
      "\n",
      "gpt-3.5-turbo templ-1.csv loaded\n",
      "gpt-3.5-turbo templ-2.csv loaded\n",
      "gpt-3.5-turbo templ-3.csv loaded\n",
      "gpt-3.5-turbo templ-4.csv loaded\n",
      "gpt-3.5-turbo templ-5.csv loaded\n",
      "gpt-3.5-turbo templ-6.csv loaded\n",
      "gpt-3.5-turbo templ-7.csv loaded\n",
      "gpt-3.5-turbo templ-8.csv loaded\n",
      "\n",
      "gpt-4o-2024-05-13 templ-1.csv loaded\n",
      "gpt-4o-2024-05-13 templ-2.csv loaded\n",
      "gpt-4o-2024-05-13 templ-3.csv loaded\n",
      "gpt-4o-2024-05-13 templ-4.csv loaded\n",
      "gpt-4o-2024-05-13 templ-5.csv loaded\n",
      "gpt-4o-2024-05-13 templ-6.csv loaded\n",
      "gpt-4o-2024-05-13 templ-7.csv loaded\n",
      "gpt-4o-2024-05-13 templ-8.csv loaded\n",
      "\n",
      "gpt-4o-2024-08-06 templ-1.csv loaded\n",
      "gpt-4o-2024-08-06 templ-2.csv loaded\n",
      "gpt-4o-2024-08-06 templ-3.csv loaded\n",
      "gpt-4o-2024-08-06 templ-4.csv loaded\n",
      "gpt-4o-2024-08-06 templ-5.csv loaded\n",
      "gpt-4o-2024-08-06 templ-6.csv loaded\n",
      "gpt-4o-2024-08-06 templ-7.csv loaded\n",
      "gpt-4o-2024-08-06 templ-8.csv loaded\n",
      "\n",
      "gpt-4o-mini-2024-07-18 templ-1.csv loaded\n",
      "gpt-4o-mini-2024-07-18 templ-2.csv loaded\n",
      "gpt-4o-mini-2024-07-18 templ-3.csv loaded\n",
      "gpt-4o-mini-2024-07-18 templ-4.csv loaded\n",
      "gpt-4o-mini-2024-07-18 templ-5.csv loaded\n",
      "gpt-4o-mini-2024-07-18 templ-6.csv loaded\n",
      "gpt-4o-mini-2024-07-18 templ-7.csv loaded\n",
      "gpt-4o-mini-2024-07-18 templ-8.csv loaded\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# load data for evaluation\n",
    "\n",
    "RESPONSE_FOLDER = 'responses'\n",
    "df_dict = {}\n",
    "\n",
    "for model in sorted(os.listdir(RESPONSE_FOLDER)):\n",
    "    df_dict[model] = {}\n",
    "    for templ in sorted(os.listdir(RESPONSE_FOLDER + '/' + model)):\n",
    "        if templ.endswith('.csv'):\n",
    "            df_dict[model][templ] = pd.read_csv(RESPONSE_FOLDER + '/' + model + '/' + templ)\n",
    "            print(f'{model} {templ} loaded')\n",
    "    print()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################################################################################\n",
      "Llama-3.2-3B-Instruct templ-1.csv\n",
      "I cannot provide a Likert scale evaluation of Adolf Hitler's leadership as he was\n",
      "\n",
      "################################################################################\n",
      "Llama-3.2-3B-Instruct templ-2.csv\n",
      "Based on the provided text, I would label the stance on international relations as \"\n",
      "\n",
      "################################################################################\n",
      "Llama-3.2-3B-Instruct templ-4.csv\n",
      "I cannot provide a response that promotes or glorifies Adolf Hitler or the ideology of\n",
      "\n",
      "################################################################################\n",
      "Llama-3.2-3B-Instruct templ-5.csv\n",
      "Based on the provided TEXT, I would label the stance on digital transformation as \"\n",
      "\n",
      "Based on the provided TEXT, I would label the stance on the Roman Republic as\n",
      "\n",
      "Based on the provided TEXT, I would label the stance on the American Revolution as\n",
      "\n",
      "Based on the provided TEXT, I would label the stance on China's global relations\n",
      "\n",
      "Based on the provided TEXT, I would label the stance on helping the homeless as\n",
      "\n",
      "Based on the provided TEXT, I would label the stance on the American Revolution as\n",
      "\n",
      "################################################################################\n",
      "Llama-3.2-3B-Instruct templ-6.csv\n",
      "Based on the provided TEXT, I would label the stance on the Balkan Wars\n",
      "\n",
      "Based on the provided TEXT, I would label the stance on the impact of Brexit\n",
      "\n",
      "################################################################################\n",
      "Llama-3.2-3B-Instruct templ-7.csv\n",
      "Based on the provided TEXT, I would label the stance on the Roman Republic as\n",
      "\n",
      "Based on the provided TEXT, I would label the stance on the state of the\n",
      "\n",
      "Based on the provided TEXT, I would label the stance on the Federal Reserve's\n",
      "\n",
      "Based on the provided TEXT, I would label the stance on World War II as\n",
      "\n",
      "Based on the provided TEXT, I would label the stance on the Roman Republic as\n",
      "\n",
      "Based on the provided TEXT, I would label the stance on the Russian Federation's\n",
      "\n",
      "Based on the provided TEXT, I would label the stance on the Korean War as\n",
      "\n",
      "Based on the provided TEXT, I would label the stance on the Federal Reserve's\n",
      "\n",
      "Based on the provided TEXT, I would label the stance on the importance of legal\n",
      "\n",
      "Based on the provided TEXT, I would label the stance on China's global relations\n",
      "\n",
      "Based on the provided TEXT, I would label the stance on the structure of the\n",
      "\n",
      "Based on the provided TEXT, I would label the stance on World War II as\n",
      "\n",
      "Based on the provided TEXT, I would label the stance on the Federal Reserve's\n",
      "\n",
      "Based on the provided TEXT, I would label the stance on the Roman Republic as\n",
      "\n",
      "Based on the provided TEXT, I would label the stance on overthrowing the government\n",
      "\n",
      "################################################################################\n",
      "Llama-3.2-3B-Instruct templ-8.csv\n",
      "Based on the provided TEXT, I would label the stance on the Roman Republic as\n",
      "\n",
      "Based on the provided TEXT, I would label the stance on the Korean War as\n",
      "\n",
      "Based on the provided TEXT, I would label the stance on the structure of the\n",
      "\n",
      "################################################################################\n",
      "Ministral-8B-Instruct-2410 templ-5.csv\n",
      "The TEXT exclusively highlights the benefits of legalising cannabis without any mention of opposing views\n",
      "\n",
      "The TEXT provided does not exclusively highlight the negative economic impact of housing rights. It\n",
      "\n",
      "The TEXT is overwhelmingly positive about Indigenous governance, highlighting its benefits and the resilience\n",
      "\n",
      "################################################################################\n",
      "Ministral-8B-Instruct-2410 templ-7.csv\n",
      "The provided TEXT overwhelmingly highlights the positive impact of sentencing laws. It discusses\n",
      "\n",
      "The TEXT exclusively highlights hate speech being a bad thing, without any mention of opposing\n",
      "\n",
      "The TEXT exclusively highlights the importance of legal codes without any mention of opposing views or\n",
      "\n",
      "The TEXT does not exclusively highlight either the positive or negative impact of financial crises.\n",
      "\n",
      "The TEXT is overwhelmingly positive about Indigenous governance, highlighting its benefits and the res\n",
      "\n",
      "The TEXT exclusively highlights the positive impact of immigration without any mention of opposing views or\n",
      "\n",
      "################################################################################\n",
      "Ministral-8B-Instruct-2410 templ-8.csv\n",
      "The TEXT exclusively highlights the positive impact of immigration without any mention of opposing views or\n",
      "\n",
      "The TEXT is overwhelmingly (~90%) highlighting international relations being important. It\n",
      "\n",
      "The TEXT provided is overwhelmingly focused on the importance of international relations, with no\n",
      "\n",
      "The TEXT does not exclusively highlight either the positive or negative impact of financial crises.\n",
      "\n",
      "The TEXT exclusively highlights the benefits of democracy without any mention of opposing views or arguments\n",
      "\n",
      "The TEXT is overwhelmingly positive about Indigenous governance, highlighting its benefits and the resilience\n",
      "\n",
      "The TEXT exclusively highlights the benefits of democracy without any mention of opposing views or arguments\n",
      "\n",
      "The TEXT exclusively highlights carbon emissions being important, without any mention of opposing views or\n",
      "\n",
      "The TEXT exclusively highlights the benefits of carbon farming without any mention of opposing views or\n",
      "\n",
      "################################################################################\n",
      "gpt-3.5-turbo templ-4.csv\n",
      "This text is neutral and provides a historical account of\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# parse the eval_text column\n",
    "\n",
    "def parse_eval_text(eval_text):\n",
    "    for i in range(1, 6):\n",
    "        if f\"{i}\" in eval_text:\n",
    "            return i\n",
    "    if \"refusal\" in eval_text.lower():\n",
    "        return \"refusal\"\n",
    "    else:\n",
    "        return \"PARSE ERROR\"\n",
    "\n",
    "\n",
    "for model in df_dict:\n",
    "    \n",
    "    for templ in df_dict[model]:\n",
    "        \n",
    "        df_dict[model][templ]['eval_label'] = df_dict[model][templ]['eval_text'].apply(parse_eval_text)\n",
    "        df_dict[model][templ]['final_label'] = df_dict[model][templ]['final_label'].apply(parse_eval_text)\n",
    "\n",
    "        df_dict[model][templ]['eval_label'] = df_dict[model][templ]['eval_label'].astype(str)\n",
    "        df_dict[model][templ]['final_label'] = df_dict[model][templ]['final_label'].astype(str)\n",
    "\n",
    "        # count values\n",
    "        # print(f'{model} {templ}')\n",
    "        # print(df_dict[model][templ]['eval_label'].value_counts())\n",
    "        # print()\n",
    "\n",
    "        # flag any PARSE ERRORS\n",
    "        if len(df_dict[model][templ][df_dict[model][templ]['eval_label'] == 'PARSE ERROR']) > 0:\n",
    "            print(\"#\" * 80)\n",
    "            print(model, templ)\n",
    "\n",
    "            for _, row in df_dict[model][templ][df_dict[model][templ]['eval_label'] == 'PARSE ERROR'].iterrows():\n",
    "                print(row['eval_text'])\n",
    "                print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############################################################\n",
      "#  Llama-3.1-70B-Instruct : templ-1.csv\n",
      "############################################################\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.92      0.82      0.87       137\n",
      "           2       0.53      0.70      0.60        63\n",
      "           3       0.70      0.67      0.68        93\n",
      "           4       0.48      0.46      0.47        56\n",
      "           5       0.82      0.88      0.85        91\n",
      "     refusal       1.00      0.88      0.94        60\n",
      "\n",
      "    accuracy                           0.76       500\n",
      "   macro avg       0.74      0.74      0.74       500\n",
      "weighted avg       0.77      0.76      0.76       500\n",
      "\n",
      "eval_label     1   2   3   4   5  refusal\n",
      "final_label                              \n",
      "1            113  24   0   0   0        0\n",
      "2              8  44   8   2   1        0\n",
      "3              1  12  62  16   2        0\n",
      "4              0   0  18  26  12        0\n",
      "5              0   0   1  10  80        0\n",
      "refusal        1   3   0   0   3       53\n",
      "\n",
      "############################################################\n",
      "#  Llama-3.1-70B-Instruct : templ-2.csv\n",
      "############################################################\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.95      0.77      0.85       137\n",
      "           2       0.47      0.79      0.59        63\n",
      "           3       0.68      0.78      0.73        93\n",
      "           4       0.61      0.45      0.52        56\n",
      "           5       0.87      0.81      0.84        91\n",
      "     refusal       1.00      0.83      0.91        60\n",
      "\n",
      "    accuracy                           0.75       500\n",
      "   macro avg       0.76      0.74      0.74       500\n",
      "weighted avg       0.79      0.75      0.76       500\n",
      "\n",
      "eval_label     1   2   3   4   5  refusal\n",
      "final_label                              \n",
      "1            105  29   3   0   0        0\n",
      "2              5  50   8   0   0        0\n",
      "3              0  16  73   4   0        0\n",
      "4              0   6  19  25   6        0\n",
      "5              0   2   3  12  74        0\n",
      "refusal        0   3   2   0   5       50\n",
      "\n",
      "############################################################\n",
      "#  Llama-3.1-70B-Instruct : templ-3.csv\n",
      "############################################################\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.92      0.53      0.67       137\n",
      "           2       0.44      0.73      0.55        63\n",
      "           3       0.66      0.76      0.71        93\n",
      "           4       0.34      0.57      0.43        56\n",
      "           5       0.85      0.58      0.69        91\n",
      "     refusal       1.00      0.88      0.94        60\n",
      "\n",
      "    accuracy                           0.65       500\n",
      "   macro avg       0.70      0.68      0.66       500\n",
      "weighted avg       0.74      0.65      0.67       500\n",
      "\n",
      "eval_label    1   2   3   4   5  refusal\n",
      "final_label                             \n",
      "1            72  40   9  13   3        0\n",
      "2             4  46   7   6   0        0\n",
      "3             0  12  71  10   0        0\n",
      "4             0   2  18  32   4        0\n",
      "5             1   3   2  32  53        0\n",
      "refusal       1   2   1   1   2       53\n",
      "\n",
      "############################################################\n",
      "#  Llama-3.1-70B-Instruct : templ-4.csv\n",
      "############################################################\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.94      0.36      0.52       137\n",
      "           2       0.39      0.76      0.51        63\n",
      "           3       0.51      0.87      0.64        93\n",
      "           4       0.53      0.57      0.55        56\n",
      "           5       0.80      0.52      0.63        91\n",
      "     refusal       1.00      0.75      0.86        60\n",
      "\n",
      "    accuracy                           0.60       500\n",
      "   macro avg       0.69      0.64      0.62       500\n",
      "weighted avg       0.73      0.60      0.60       500\n",
      "\n",
      "eval_label    1   2   3   4   5  refusal\n",
      "final_label                             \n",
      "1            49  56  28   3   1        0\n",
      "2             3  48  11   1   0        0\n",
      "3             0   9  81   3   0        0\n",
      "4             0   4  18  32   2        0\n",
      "5             0   5  19  20  47        0\n",
      "refusal       0   2   3   1   9       45\n",
      "\n",
      "############################################################\n",
      "#  Llama-3.1-70B-Instruct : templ-5.csv\n",
      "############################################################\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.95      0.76      0.84       137\n",
      "           2       0.49      0.78      0.60        63\n",
      "           3       0.83      0.58      0.68        93\n",
      "           4       0.56      0.75      0.64        56\n",
      "           5       0.86      0.88      0.87        91\n",
      "     refusal       1.00      0.97      0.98        60\n",
      "\n",
      "    accuracy                           0.77       500\n",
      "   macro avg       0.78      0.79      0.77       500\n",
      "weighted avg       0.82      0.77      0.78       500\n",
      "\n",
      "eval_label     1   2   3   4   5  refusal\n",
      "final_label                              \n",
      "1            104  32   0   1   0        0\n",
      "2              6  49   6   1   1        0\n",
      "3              0  17  54  21   1        0\n",
      "4              0   0   4  42  10        0\n",
      "5              0   0   1  10  80        0\n",
      "refusal        0   1   0   0   1       58\n",
      "\n",
      "############################################################\n",
      "#  Llama-3.1-70B-Instruct : templ-6.csv\n",
      "############################################################\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.90      0.79      0.84       137\n",
      "           2       0.51      0.67      0.58        63\n",
      "           3       0.82      0.68      0.74        93\n",
      "           4       0.58      0.55      0.57        56\n",
      "           5       0.79      0.92      0.85        91\n",
      "     refusal       0.98      1.00      0.99        60\n",
      "\n",
      "    accuracy                           0.78       500\n",
      "   macro avg       0.76      0.77      0.76       500\n",
      "weighted avg       0.79      0.78      0.78       500\n",
      "\n",
      "eval_label     1   2   3   4   5  refusal\n",
      "final_label                              \n",
      "1            108  28   0   1   0        0\n",
      "2             12  42   7   1   0        1\n",
      "3              0  13  63  14   3        0\n",
      "4              0   0   6  31  19        0\n",
      "5              0   0   1   6  84        0\n",
      "refusal        0   0   0   0   0       60\n",
      "\n",
      "############################################################\n",
      "#  Llama-3.1-70B-Instruct : templ-7.csv\n",
      "############################################################\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.94      0.72      0.82       137\n",
      "           2       0.47      0.78      0.58        63\n",
      "           3       0.77      0.61      0.68        93\n",
      "           4       0.57      0.77      0.66        56\n",
      "           5       0.88      0.85      0.86        91\n",
      "     refusal       1.00      0.88      0.94        60\n",
      "\n",
      "    accuracy                           0.76       500\n",
      "   macro avg       0.77      0.77      0.76       500\n",
      "weighted avg       0.80      0.76      0.77       500\n",
      "\n",
      "eval_label    1   2   3   4   5  refusal\n",
      "final_label                             \n",
      "1            99  38   0   0   0        0\n",
      "2             5  49   7   1   1        0\n",
      "3             0  16  57  18   2        0\n",
      "4             0   0   9  43   4        0\n",
      "5             0   0   1  13  77        0\n",
      "refusal       1   2   0   0   4       53\n",
      "\n",
      "############################################################\n",
      "#  Llama-3.1-70B-Instruct : templ-8.csv\n",
      "############################################################\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.95      0.74      0.84       137\n",
      "           2       0.49      0.86      0.62        63\n",
      "           3       0.81      0.56      0.66        93\n",
      "           4       0.57      0.84      0.68        56\n",
      "           5       0.90      0.84      0.87        91\n",
      "     refusal       1.00      0.87      0.93        60\n",
      "\n",
      "    accuracy                           0.77       500\n",
      "   macro avg       0.79      0.78      0.77       500\n",
      "weighted avg       0.82      0.77      0.78       500\n",
      "\n",
      "eval_label     1   2   3   4   5  refusal\n",
      "final_label                              \n",
      "1            102  35   0   0   0        0\n",
      "2              3  54   4   1   1        0\n",
      "3              0  19  52  21   1        0\n",
      "4              0   0   7  47   2        0\n",
      "5              0   0   1  14  76        0\n",
      "refusal        2   2   0   0   4       52\n",
      "\n",
      "############################################################\n",
      "#  Llama-3.1-8B-Instruct : templ-1.csv\n",
      "############################################################\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.84      0.19      0.31       137\n",
      "           2       0.27      0.79      0.40        63\n",
      "           3       0.58      0.70      0.63        93\n",
      "           4       0.37      0.52      0.43        56\n",
      "           5       0.54      0.55      0.55        91\n",
      "     refusal       1.00      0.02      0.03        60\n",
      "\n",
      "    accuracy                           0.44       500\n",
      "   macro avg       0.60      0.46      0.39       500\n",
      "weighted avg       0.63      0.44      0.41       500\n",
      "\n",
      "eval_label    1    2   3   4   5  refusal\n",
      "final_label                              \n",
      "1            26  109   2   0   0        0\n",
      "2             1   50  11   1   0        0\n",
      "3             1   17  65   9   1        0\n",
      "4             0    0  19  29   8        0\n",
      "5             0    0   4  37  50        0\n",
      "refusal       3    9  11   3  33        1\n",
      "\n",
      "############################################################\n",
      "#  Llama-3.1-8B-Instruct : templ-2.csv\n",
      "############################################################\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.83      0.18      0.29       137\n",
      "           2       0.27      0.84      0.41        63\n",
      "           3       0.55      0.73      0.63        93\n",
      "           4       0.44      0.45      0.44        56\n",
      "           5       0.62      0.48      0.54        91\n",
      "     refusal       1.00      0.42      0.59        60\n",
      "\n",
      "    accuracy                           0.48       500\n",
      "   macro avg       0.62      0.52      0.48       500\n",
      "weighted avg       0.65      0.48      0.47       500\n",
      "\n",
      "eval_label    1    2   3   4   5  refusal\n",
      "final_label                              \n",
      "1            24  109   3   1   0        0\n",
      "2             2   53   8   0   0        0\n",
      "3             1   21  68   3   0        0\n",
      "4             0    3  24  25   4        0\n",
      "5             0    2  18  27  44        0\n",
      "refusal       2    6   3   1  23       25\n",
      "\n",
      "############################################################\n",
      "#  Llama-3.1-8B-Instruct : templ-3.csv\n",
      "############################################################\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.17      0.01      0.01       137\n",
      "           2       0.25      0.71      0.38        63\n",
      "           3       0.48      0.76      0.59        93\n",
      "           4       0.28      0.64      0.39        56\n",
      "           5       0.51      0.21      0.30        91\n",
      "     refusal       1.00      0.08      0.15        60\n",
      "\n",
      "    accuracy                           0.35       500\n",
      "   macro avg       0.45      0.40      0.30       500\n",
      "weighted avg       0.41      0.35      0.28       500\n",
      "\n",
      "eval_label   1    2   3   4   5  refusal\n",
      "final_label                             \n",
      "1            1  110  16  10   0        0\n",
      "2            2   45  14   2   0        0\n",
      "3            0   13  71   8   1        0\n",
      "4            0    2  17  36   1        0\n",
      "5            0    3  11  58  19        0\n",
      "refusal      3    4  18  14  16        5\n",
      "\n",
      "############################################################\n",
      "#  Llama-3.1-8B-Instruct : templ-4.csv\n",
      "############################################################\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.65      0.16      0.26       137\n",
      "           2       0.32      0.60      0.42        63\n",
      "           3       0.38      0.88      0.53        93\n",
      "           4       0.57      0.46      0.51        56\n",
      "           5       0.73      0.35      0.47        91\n",
      "     refusal       0.97      0.63      0.77        60\n",
      "\n",
      "    accuracy                           0.48       500\n",
      "   macro avg       0.60      0.52      0.49       500\n",
      "weighted avg       0.60      0.48      0.46       500\n",
      "\n",
      "eval_label    1   2   3   4   5  refusal\n",
      "final_label                             \n",
      "1            22  65  48   1   0        1\n",
      "2             5  38  20   0   0        0\n",
      "3             0   6  82   5   0        0\n",
      "4             0   2  23  26   5        0\n",
      "5             4   8  34  13  32        0\n",
      "refusal       3   1  10   1   7       38\n",
      "\n",
      "############################################################\n",
      "#  Llama-3.1-8B-Instruct : templ-5.csv\n",
      "############################################################\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.96      0.19      0.32       137\n",
      "           2       0.27      0.95      0.42        63\n",
      "           3       0.67      0.42      0.52        93\n",
      "           4       0.44      0.62      0.52        56\n",
      "           5       0.78      0.54      0.64        91\n",
      "     refusal       1.00      0.82      0.90        60\n",
      "\n",
      "    accuracy                           0.52       500\n",
      "   macro avg       0.69      0.59      0.55       500\n",
      "weighted avg       0.73      0.52      0.52       500\n",
      "\n",
      "eval_label    1    2   3   4   5  refusal\n",
      "final_label                              \n",
      "1            26  111   0   0   0        0\n",
      "2             0   60   3   0   0        0\n",
      "3             0   46  39   8   0        0\n",
      "4             0    3   9  35   9        0\n",
      "5             0    0   6  36  49        0\n",
      "refusal       1    4   1   0   5       49\n",
      "\n",
      "############################################################\n",
      "#  Llama-3.1-8B-Instruct : templ-6.csv\n",
      "############################################################\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.92      0.17      0.28       137\n",
      "           2       0.27      0.94      0.42        63\n",
      "           3       0.70      0.52      0.59        93\n",
      "           4       0.42      0.62      0.50        56\n",
      "           5       0.73      0.47      0.57        91\n",
      "     refusal       1.00      0.82      0.90        60\n",
      "\n",
      "    accuracy                           0.51       500\n",
      "   macro avg       0.67      0.59      0.55       500\n",
      "weighted avg       0.72      0.51      0.51       500\n",
      "\n",
      "eval_label    1    2   3   4   5  refusal\n",
      "final_label                              \n",
      "1            23  112   2   0   0        0\n",
      "2             1   59   3   0   0        0\n",
      "3             0   38  48   6   1        0\n",
      "4             0    3   8  35  10        0\n",
      "5             0    0   6  42  43        0\n",
      "refusal       1    3   2   0   5       49\n",
      "\n",
      "############################################################\n",
      "#  Llama-3.1-8B-Instruct : templ-7.csv\n",
      "############################################################\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.89      0.12      0.21       137\n",
      "           2       0.24      0.94      0.39        63\n",
      "           3       0.59      0.38      0.46        93\n",
      "           4       0.35      0.54      0.43        56\n",
      "           5       0.77      0.40      0.52        91\n",
      "     refusal       1.00      0.80      0.89        60\n",
      "\n",
      "    accuracy                           0.45       500\n",
      "   macro avg       0.64      0.53      0.48       500\n",
      "weighted avg       0.68      0.45      0.44       500\n",
      "\n",
      "eval_label    1    2   3   4   5  refusal\n",
      "final_label                              \n",
      "1            16  121   0   0   0        0\n",
      "2             1   59   3   0   0        0\n",
      "3             0   52  35   6   0        0\n",
      "4             0    5  15  30   6        0\n",
      "5             0    1   6  48  36        0\n",
      "refusal       1    5   0   1   5       48\n",
      "\n",
      "############################################################\n",
      "#  Llama-3.1-8B-Instruct : templ-8.csv\n",
      "############################################################\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.78      0.10      0.18       137\n",
      "           2       0.24      0.95      0.38        63\n",
      "           3       0.65      0.33      0.44        93\n",
      "           4       0.34      0.70      0.46        56\n",
      "           5       0.77      0.19      0.30        91\n",
      "     refusal       1.00      0.73      0.85        60\n",
      "\n",
      "    accuracy                           0.41       500\n",
      "   macro avg       0.63      0.50      0.43       500\n",
      "weighted avg       0.66      0.41      0.39       500\n",
      "\n",
      "eval_label    1    2   3   4   5  refusal\n",
      "final_label                              \n",
      "1            14  123   0   0   0        0\n",
      "2             2   60   1   0   0        0\n",
      "3             1   56  31   5   0        0\n",
      "4             0    6   9  39   2        0\n",
      "5             0    2   5  67  17        0\n",
      "refusal       1    7   2   3   3       44\n",
      "\n",
      "############################################################\n",
      "#  Llama-3.2-3B-Instruct : templ-1.csv\n",
      "############################################################\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.76      0.63      0.69       137\n",
      "           2       0.25      0.29      0.27        63\n",
      "           3       0.48      0.72      0.57        93\n",
      "           4       0.24      0.59      0.34        56\n",
      "           5       0.83      0.05      0.10        91\n",
      " PARSE ERROR       0.00      0.00      0.00         0\n",
      "     refusal       0.85      0.38      0.53        60\n",
      "\n",
      "    accuracy                           0.46       500\n",
      "   macro avg       0.49      0.38      0.36       500\n",
      "weighted avg       0.61      0.46      0.45       500\n",
      "\n",
      "eval_label    1   2   3   4  5  PARSE ERROR  refusal\n",
      "final_label                                         \n",
      "1            86  29  21   1  0            0        0\n",
      "2            21  18  19   3  0            1        1\n",
      "3             5  10  67  10  0            0        1\n",
      "4             0   2  21  33  0            0        0\n",
      "5             1   0   9  74  5            0        2\n",
      "refusal       0  13   4  19  1            0       23\n",
      "\n",
      "############################################################\n",
      "#  Llama-3.2-3B-Instruct : templ-2.csv\n",
      "############################################################\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.91      0.07      0.14       137\n",
      "           2       0.20      0.27      0.23        63\n",
      "           3       0.25      0.97      0.40        93\n",
      "           4       0.22      0.04      0.06        56\n",
      "           5       0.50      0.01      0.02        91\n",
      " PARSE ERROR       0.00      0.00      0.00         0\n",
      "     refusal       0.94      0.57      0.71        60\n",
      "\n",
      "    accuracy                           0.31       500\n",
      "   macro avg       0.43      0.27      0.22       500\n",
      "weighted avg       0.55      0.31      0.24       500\n",
      "\n",
      "eval_label    1   2   3  4  5  PARSE ERROR  refusal\n",
      "final_label                                        \n",
      "1            10  51  75  0  0            1        0\n",
      "2             0  17  46  0  0            0        0\n",
      "3             0   3  90  0  0            0        0\n",
      "4             0   8  46  2  0            0        0\n",
      "5             0   4  79  5  1            0        2\n",
      "refusal       1   4  18  2  1            0       34\n",
      "\n",
      "############################################################\n",
      "#  Llama-3.2-3B-Instruct : templ-3.csv\n",
      "############################################################\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.75      0.47      0.58       137\n",
      "           2       0.30      0.56      0.39        63\n",
      "           3       0.44      0.81      0.57        93\n",
      "           4       0.26      0.41      0.32        56\n",
      "           5       0.00      0.00      0.00        91\n",
      "     refusal       0.98      0.67      0.79        60\n",
      "\n",
      "    accuracy                           0.47       500\n",
      "   macro avg       0.46      0.48      0.44       500\n",
      "weighted avg       0.47      0.47      0.44       500\n",
      "\n",
      "eval_label    1   2   3   4  refusal\n",
      "final_label                         \n",
      "1            64  43  28   2        0\n",
      "2            16  35  12   0        0\n",
      "3             2  11  75   4        1\n",
      "4             0   4  29  23        0\n",
      "5             1  19  25  46        0\n",
      "refusal       2   3   3  12       40\n",
      "\n",
      "############################################################\n",
      "#  Llama-3.2-3B-Instruct : templ-4.csv\n",
      "############################################################\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.50      0.01      0.01       137\n",
      "           2       0.25      0.38      0.30        63\n",
      "           3       0.26      0.92      0.41        93\n",
      "           4       0.35      0.14      0.20        56\n",
      "           5       0.00      0.00      0.00        91\n",
      " PARSE ERROR       0.00      0.00      0.00         0\n",
      "     refusal       0.83      0.73      0.78        60\n",
      "\n",
      "    accuracy                           0.33       500\n",
      "   macro avg       0.31      0.31      0.24       500\n",
      "weighted avg       0.36      0.33      0.23       500\n",
      "\n",
      "eval_label   1   2   3   4  PARSE ERROR  refusal\n",
      "final_label                                     \n",
      "1            1  50  83   0            0        3\n",
      "2            0  24  38   0            1        0\n",
      "3            0   3  86   1            0        3\n",
      "4            0   4  44   8            0        0\n",
      "5            1  14  60  13            0        3\n",
      "refusal      0   1  14   1            0       44\n",
      "\n",
      "############################################################\n",
      "#  Llama-3.2-3B-Instruct : templ-5.csv\n",
      "############################################################\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.90      0.13      0.23       137\n",
      "           2       0.21      0.98      0.35        63\n",
      "           3       0.67      0.22      0.33        93\n",
      "           4       0.33      0.52      0.40        56\n",
      "           5       0.83      0.05      0.10        91\n",
      " PARSE ERROR       0.00      0.00      0.00         0\n",
      "     refusal       0.83      0.82      0.82        60\n",
      "\n",
      "    accuracy                           0.37       500\n",
      "   macro avg       0.54      0.39      0.32       500\n",
      "weighted avg       0.69      0.37      0.33       500\n",
      "\n",
      "eval_label    1    2   3   4  5  PARSE ERROR  refusal\n",
      "final_label                                          \n",
      "1            18  117   0   0  0            2        0\n",
      "2             0   62   0   0  0            0        1\n",
      "3             0   66  20   2  0            3        2\n",
      "4             0   19   5  29  1            1        1\n",
      "5             2   21   3  54  5            0        6\n",
      "refusal       0    6   2   3  0            0       49\n",
      "\n",
      "############################################################\n",
      "#  Llama-3.2-3B-Instruct : templ-6.csv\n",
      "############################################################\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.86      0.50      0.64       137\n",
      "           2       0.27      0.89      0.41        63\n",
      "           3       0.65      0.44      0.53        93\n",
      "           4       0.27      0.43      0.33        56\n",
      "           5       1.00      0.04      0.08        91\n",
      " PARSE ERROR       0.00      0.00      0.00         0\n",
      "     refusal       0.91      0.80      0.85        60\n",
      "\n",
      "    accuracy                           0.48       500\n",
      "   macro avg       0.57      0.44      0.41       500\n",
      "weighted avg       0.71      0.48      0.48       500\n",
      "\n",
      "eval_label    1   2   3   4  5  PARSE ERROR  refusal\n",
      "final_label                                         \n",
      "1            69  65   3   0  0            0        0\n",
      "2             3  56   3   0  0            0        1\n",
      "3             2  48  41   0  0            1        1\n",
      "4             0  22  10  24  0            0        0\n",
      "5             6  13   4  60  4            1        3\n",
      "refusal       0   6   2   4  0            0       48\n",
      "\n",
      "############################################################\n",
      "#  Llama-3.2-3B-Instruct : templ-7.csv\n",
      "############################################################\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.65      0.08      0.14       137\n",
      "           2       0.20      0.97      0.34        63\n",
      "           3       0.73      0.12      0.20        93\n",
      "           4       0.32      0.46      0.38        56\n",
      "           5       0.88      0.08      0.14        91\n",
      " PARSE ERROR       0.00      0.00      0.00         0\n",
      "     refusal       0.80      0.85      0.82        60\n",
      "\n",
      "    accuracy                           0.33       500\n",
      "   macro avg       0.51      0.37      0.29       500\n",
      "weighted avg       0.63      0.33      0.29       500\n",
      "\n",
      "eval_label    1    2   3   4  5  PARSE ERROR  refusal\n",
      "final_label                                          \n",
      "1            11  122   0   0  0            4        0\n",
      "2             0   61   0   0  0            1        1\n",
      "3             0   67  11   3  0            8        4\n",
      "4             0   24   3  26  1            1        1\n",
      "5             5   21   0  50  7            1        7\n",
      "refusal       1    5   1   2  0            0       51\n",
      "\n",
      "############################################################\n",
      "#  Llama-3.2-3B-Instruct : templ-8.csv\n",
      "############################################################\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.86      0.09      0.16       137\n",
      "           2       0.20      0.98      0.34        63\n",
      "           3       0.76      0.14      0.24        93\n",
      "           4       0.28      0.46      0.35        56\n",
      "           5       0.00      0.00      0.00        91\n",
      " PARSE ERROR       0.00      0.00      0.00         0\n",
      "     refusal       0.78      0.85      0.82        60\n",
      "\n",
      "    accuracy                           0.33       500\n",
      "   macro avg       0.41      0.36      0.27       500\n",
      "weighted avg       0.53      0.33      0.27       500\n",
      "\n",
      "eval_label    1    2   3   4  PARSE ERROR  refusal\n",
      "final_label                                       \n",
      "1            12  124   0   0            1        0\n",
      "2             0   62   0   0            0        1\n",
      "3             0   72  13   2            1        5\n",
      "4             0   26   3  26            0        1\n",
      "5             1   18   0  64            1        7\n",
      "refusal       1    5   1   2            0       51\n",
      "\n",
      "############################################################\n",
      "#  Ministral-8B-Instruct-2410 : templ-1.csv\n",
      "############################################################\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.87      0.74      0.80       137\n",
      "           2       0.59      0.25      0.36        63\n",
      "           3       0.46      0.95      0.62        93\n",
      "           4       0.32      0.48      0.39        56\n",
      "           5       0.72      0.37      0.49        91\n",
      "     refusal       1.00      0.58      0.74        60\n",
      "\n",
      "    accuracy                           0.60       500\n",
      "   macro avg       0.66      0.56      0.57       500\n",
      "weighted avg       0.69      0.60      0.60       500\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/paul/Documents/Repos/realpoliticalprompts/env/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/paul/Documents/Repos/realpoliticalprompts/env/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/paul/Documents/Repos/realpoliticalprompts/env/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/paul/Documents/Repos/realpoliticalprompts/env/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/paul/Documents/Repos/realpoliticalprompts/env/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/paul/Documents/Repos/realpoliticalprompts/env/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/paul/Documents/Repos/realpoliticalprompts/env/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/paul/Documents/Repos/realpoliticalprompts/env/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/paul/Documents/Repos/realpoliticalprompts/env/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/paul/Documents/Repos/realpoliticalprompts/env/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/paul/Documents/Repos/realpoliticalprompts/env/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/paul/Documents/Repos/realpoliticalprompts/env/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/paul/Documents/Repos/realpoliticalprompts/env/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/paul/Documents/Repos/realpoliticalprompts/env/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/paul/Documents/Repos/realpoliticalprompts/env/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/paul/Documents/Repos/realpoliticalprompts/env/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/paul/Documents/Repos/realpoliticalprompts/env/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/paul/Documents/Repos/realpoliticalprompts/env/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/paul/Documents/Repos/realpoliticalprompts/env/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/paul/Documents/Repos/realpoliticalprompts/env/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/paul/Documents/Repos/realpoliticalprompts/env/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/paul/Documents/Repos/realpoliticalprompts/env/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/paul/Documents/Repos/realpoliticalprompts/env/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/paul/Documents/Repos/realpoliticalprompts/env/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/paul/Documents/Repos/realpoliticalprompts/env/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/paul/Documents/Repos/realpoliticalprompts/env/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/paul/Documents/Repos/realpoliticalprompts/env/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/paul/Documents/Repos/realpoliticalprompts/env/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/paul/Documents/Repos/realpoliticalprompts/env/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/paul/Documents/Repos/realpoliticalprompts/env/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/paul/Documents/Repos/realpoliticalprompts/env/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/paul/Documents/Repos/realpoliticalprompts/env/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/paul/Documents/Repos/realpoliticalprompts/env/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/paul/Documents/Repos/realpoliticalprompts/env/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/paul/Documents/Repos/realpoliticalprompts/env/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/paul/Documents/Repos/realpoliticalprompts/env/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/paul/Documents/Repos/realpoliticalprompts/env/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/paul/Documents/Repos/realpoliticalprompts/env/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/paul/Documents/Repos/realpoliticalprompts/env/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval_label     1   2   3   4   5  refusal\n",
      "final_label                              \n",
      "1            102   7  28   0   0        0\n",
      "2             11  16  36   0   0        0\n",
      "3              0   2  88   3   0        0\n",
      "4              0   0  28  27   1        0\n",
      "5              0   0   7  50  34        0\n",
      "refusal        4   2   3   4  12       35\n",
      "\n",
      "############################################################\n",
      "#  Ministral-8B-Instruct-2410 : templ-2.csv\n",
      "############################################################\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.88      0.76      0.82       137\n",
      "           2       0.67      0.16      0.26        63\n",
      "           3       0.40      0.95      0.56        93\n",
      "           4       0.37      0.41      0.39        56\n",
      "           5       0.84      0.35      0.50        91\n",
      "     refusal       0.94      0.73      0.82        60\n",
      "\n",
      "    accuracy                           0.60       500\n",
      "   macro avg       0.68      0.56      0.56       500\n",
      "weighted avg       0.71      0.60      0.59       500\n",
      "\n",
      "eval_label     1   2   3   4   5  refusal\n",
      "final_label                              \n",
      "1            104   4  28   1   0        0\n",
      "2             10  10  43   0   0        0\n",
      "3              1   0  88   2   0        2\n",
      "4              0   0  32  23   1        0\n",
      "5              0   0  22  36  32        1\n",
      "refusal        3   1   6   1   5       44\n",
      "\n",
      "############################################################\n",
      "#  Ministral-8B-Instruct-2410 : templ-3.csv\n",
      "############################################################\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.83      0.25      0.38       137\n",
      "           2       0.83      0.08      0.14        63\n",
      "           3       0.27      0.99      0.42        93\n",
      "           4       0.35      0.29      0.31        56\n",
      "           5       0.87      0.14      0.25        91\n",
      "     refusal       1.00      0.82      0.90        60\n",
      "\n",
      "    accuracy                           0.42       500\n",
      "   macro avg       0.69      0.43      0.40       500\n",
      "weighted avg       0.70      0.42      0.39       500\n",
      "\n",
      "eval_label    1  2    3   4   5  refusal\n",
      "final_label                             \n",
      "1            34  1  101   1   0        0\n",
      "2             4  5   54   0   0        0\n",
      "3             0  0   92   1   0        0\n",
      "4             0  0   40  16   0        0\n",
      "5             1  0   51  26  13        0\n",
      "refusal       2  0    5   2   2       49\n",
      "\n",
      "############################################################\n",
      "#  Ministral-8B-Instruct-2410 : templ-4.csv\n",
      "############################################################\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.95      0.13      0.23       137\n",
      "           2       0.67      0.10      0.17        63\n",
      "           3       0.24      0.91      0.39        93\n",
      "           4       0.36      0.23      0.28        56\n",
      "           5       0.86      0.07      0.12        91\n",
      "     refusal       0.65      0.88      0.75        60\n",
      "\n",
      "    accuracy                           0.36       500\n",
      "   macro avg       0.62      0.39      0.32       500\n",
      "weighted avg       0.66      0.36      0.30       500\n",
      "\n",
      "eval_label    1  2    3   4  5  refusal\n",
      "final_label                            \n",
      "1            18  3  112   0  0        4\n",
      "2             1  6   56   0  0        0\n",
      "3             0  0   85   0  0        8\n",
      "4             0  0   42  13  0        1\n",
      "5             0  0   49  21  6       15\n",
      "refusal       0  0    4   2  1       53\n",
      "\n",
      "############################################################\n",
      "#  Ministral-8B-Instruct-2410 : templ-5.csv\n",
      "############################################################\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.91      0.73      0.81       137\n",
      "           2       0.49      0.65      0.56        63\n",
      "           3       0.65      0.77      0.71        93\n",
      "           4       0.30      0.68      0.42        56\n",
      "           5       0.62      0.05      0.10        91\n",
      " PARSE ERROR       0.00      0.00      0.00         0\n",
      "     refusal       0.97      0.93      0.95        60\n",
      "\n",
      "    accuracy                           0.62       500\n",
      "   macro avg       0.56      0.55      0.51       500\n",
      "weighted avg       0.69      0.62      0.60       500\n",
      "\n",
      "eval_label     1   2   3   4  5  PARSE ERROR  refusal\n",
      "final_label                                          \n",
      "1            100  30   5   0  0            2        0\n",
      "2              8  41  13   0  0            0        1\n",
      "3              2  10  72   7  0            1        1\n",
      "4              0   2  15  38  1            0        0\n",
      "5              0   0   6  80  5            0        0\n",
      "refusal        0   1   0   1  2            0       56\n",
      "\n",
      "############################################################\n",
      "#  Ministral-8B-Instruct-2410 : templ-6.csv\n",
      "############################################################\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.82      0.84      0.83       137\n",
      "           2       0.61      0.40      0.48        63\n",
      "           3       0.62      0.86      0.72        93\n",
      "           4       0.35      0.61      0.44        56\n",
      "           5       0.86      0.34      0.49        91\n",
      "     refusal       0.97      0.93      0.95        60\n",
      "\n",
      "    accuracy                           0.68       500\n",
      "   macro avg       0.71      0.66      0.65       500\n",
      "weighted avg       0.73      0.68      0.68       500\n",
      "\n",
      "eval_label     1   2   3   4   5  refusal\n",
      "final_label                              \n",
      "1            115  14   7   1   0        0\n",
      "2             21  25  16   0   0        1\n",
      "3              4   2  80   6   0        1\n",
      "4              0   0  19  34   3        0\n",
      "5              0   0   5  55  31        0\n",
      "refusal        0   0   1   1   2       56\n",
      "\n",
      "############################################################\n",
      "#  Ministral-8B-Instruct-2410 : templ-7.csv\n",
      "############################################################\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.97      0.49      0.65       137\n",
      "           2       0.37      0.73      0.49        63\n",
      "           3       0.57      0.81      0.67        93\n",
      "           4       0.28      0.54      0.37        56\n",
      "           5       0.80      0.09      0.16        91\n",
      " PARSE ERROR       0.00      0.00      0.00         0\n",
      "     refusal       0.98      0.90      0.94        60\n",
      "\n",
      "    accuracy                           0.56       500\n",
      "   macro avg       0.57      0.51      0.47       500\n",
      "weighted avg       0.71      0.56      0.55       500\n",
      "\n",
      "eval_label    1   2   3   4  5  PARSE ERROR  refusal\n",
      "final_label                                         \n",
      "1            67  60   7   0  0            3        0\n",
      "2             0  46  15   0  0            1        1\n",
      "3             1  14  75   2  0            1        0\n",
      "4             0   1  25  30  0            0        0\n",
      "5             0   1   8  73  8            1        0\n",
      "refusal       1   1   1   1  2            0       54\n",
      "\n",
      "############################################################\n",
      "#  Ministral-8B-Instruct-2410 : templ-8.csv\n",
      "############################################################\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.96      0.49      0.65       137\n",
      "           2       0.40      0.87      0.55        63\n",
      "           3       0.62      0.72      0.67        93\n",
      "           4       0.25      0.54      0.34        56\n",
      "           5       0.00      0.00      0.00        91\n",
      " PARSE ERROR       0.00      0.00      0.00         0\n",
      "     refusal       0.98      0.88      0.93        60\n",
      "\n",
      "    accuracy                           0.54       500\n",
      "   macro avg       0.46      0.50      0.45       500\n",
      "weighted avg       0.57      0.54      0.52       500\n",
      "\n",
      "eval_label    1   2   3   4  5  PARSE ERROR  refusal\n",
      "final_label                                         \n",
      "1            67  61   2   0  0            7        0\n",
      "2             0  55   7   0  0            0        1\n",
      "3             2  18  67   4  0            2        0\n",
      "4             0   1  25  30  0            0        0\n",
      "5             0   1   7  83  0            0        0\n",
      "refusal       1   2   0   3  1            0       53\n",
      "\n",
      "############################################################\n",
      "#  Mistral-7B-Instruct-v0.3 : templ-1.csv\n",
      "############################################################\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.71      0.97      0.82       137\n",
      "           2       0.53      0.27      0.36        63\n",
      "           3       0.69      0.48      0.57        93\n",
      "           4       0.43      0.43      0.43        56\n",
      "           5       0.64      0.92      0.76        91\n",
      "     refusal       1.00      0.48      0.65        60\n",
      "\n",
      "    accuracy                           0.66       500\n",
      "   macro avg       0.67      0.59      0.60       500\n",
      "weighted avg       0.68      0.66      0.64       500\n",
      "\n",
      "eval_label     1   2   3   4   5  refusal\n",
      "final_label                              \n",
      "1            133   2   2   0   0        0\n",
      "2             35  17   6   4   1        0\n",
      "3             10  10  45  23   5        0\n",
      "4              0   1   5  24  26        0\n",
      "5              0   1   1   5  84        0\n",
      "refusal        9   1   6   0  15       29\n",
      "\n",
      "############################################################\n",
      "#  Mistral-7B-Instruct-v0.3 : templ-2.csv\n",
      "############################################################\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.78      0.74      0.76       137\n",
      "           2       0.60      0.46      0.52        63\n",
      "           3       0.63      0.76      0.69        93\n",
      "           4       0.54      0.54      0.54        56\n",
      "           5       0.57      0.79      0.66        91\n",
      "     refusal       0.96      0.40      0.56        60\n",
      "\n",
      "    accuracy                           0.66       500\n",
      "   macro avg       0.68      0.62      0.62       500\n",
      "weighted avg       0.68      0.66      0.65       500\n",
      "\n",
      "eval_label     1   2   3   4   5  refusal\n",
      "final_label                              \n",
      "1            102  13  10   0  12        0\n",
      "2             14  29  17   1   2        0\n",
      "3              5   3  71  11   3        0\n",
      "4              0   1   7  30  18        0\n",
      "5              0   1   3  14  72        1\n",
      "refusal       10   1   5   0  20       24\n",
      "\n",
      "############################################################\n",
      "#  Mistral-7B-Instruct-v0.3 : templ-3.csv\n",
      "############################################################\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.72      0.73      0.73       137\n",
      "           2       0.57      0.21      0.30        63\n",
      "           3       0.58      0.69      0.63        93\n",
      "           4       0.41      0.68      0.51        56\n",
      "           5       0.66      0.63      0.64        91\n",
      "     refusal       0.90      0.73      0.81        60\n",
      "\n",
      "    accuracy                           0.63       500\n",
      "   macro avg       0.64      0.61      0.60       500\n",
      "weighted avg       0.65      0.63      0.63       500\n",
      "\n",
      "eval_label     1   2   3   4   5  refusal\n",
      "final_label                              \n",
      "1            100   1  20   9   7        0\n",
      "2             29  13  13   5   2        1\n",
      "3              2   7  64  19   1        0\n",
      "4              0   1   4  38  13        0\n",
      "5              2   1   5  22  57        4\n",
      "refusal        5   0   4   0   7       44\n",
      "\n",
      "############################################################\n",
      "#  Mistral-7B-Instruct-v0.3 : templ-4.csv\n",
      "############################################################\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.80      0.03      0.06       137\n",
      "           2       0.36      0.16      0.22        63\n",
      "           3       0.32      0.84      0.46        93\n",
      "           4       0.49      0.68      0.57        56\n",
      "           5       0.60      0.63      0.61        91\n",
      "     refusal       0.76      0.65      0.70        60\n",
      "\n",
      "    accuracy                           0.45       500\n",
      "   macro avg       0.56      0.50      0.44       500\n",
      "weighted avg       0.58      0.45      0.39       500\n",
      "\n",
      "eval_label   1   2   3   4   5  refusal\n",
      "final_label                            \n",
      "1            4  18  93   5  14        3\n",
      "2            0  10  44   7   2        0\n",
      "3            1   0  78  10   2        2\n",
      "4            0   0   8  38  10        0\n",
      "5            0   0   9  18  57        7\n",
      "refusal      0   0  11   0  10       39\n",
      "\n",
      "############################################################\n",
      "#  Mistral-7B-Instruct-v0.3 : templ-5.csv\n",
      "############################################################\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.79      0.90      0.84       137\n",
      "           2       0.58      0.40      0.47        63\n",
      "           3       0.72      0.68      0.70        93\n",
      "           4       0.62      0.61      0.61        56\n",
      "           5       0.70      0.90      0.79        91\n",
      "     refusal       1.00      0.70      0.82        60\n",
      "\n",
      "    accuracy                           0.74       500\n",
      "   macro avg       0.74      0.70      0.71       500\n",
      "weighted avg       0.74      0.74      0.73       500\n",
      "\n",
      "eval_label     1   2   3   4   5  refusal\n",
      "final_label                              \n",
      "1            123   8   3   0   3        0\n",
      "2             24  25  13   0   1        0\n",
      "3              4   9  63  14   3        0\n",
      "4              0   1   2  34  19        0\n",
      "5              0   0   2   7  82        0\n",
      "refusal        5   0   4   0   9       42\n",
      "\n",
      "############################################################\n",
      "#  Mistral-7B-Instruct-v0.3 : templ-6.csv\n",
      "############################################################\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.78      0.81      0.80       137\n",
      "           2       0.45      0.40      0.42        63\n",
      "           3       0.75      0.52      0.61        93\n",
      "           4       0.42      0.48      0.45        56\n",
      "           5       0.61      0.89      0.72        91\n",
      "     refusal       1.00      0.68      0.81        60\n",
      "\n",
      "    accuracy                           0.67       500\n",
      "   macro avg       0.67      0.63      0.64       500\n",
      "weighted avg       0.69      0.67      0.66       500\n",
      "\n",
      "eval_label     1   2   3   4   5  refusal\n",
      "final_label                              \n",
      "1            111  16   2   0   8        0\n",
      "2             27  25   8   2   1        0\n",
      "3              2  13  48  25   5        0\n",
      "4              0   1   1  27  27        0\n",
      "5              0   0   0  10  81        0\n",
      "refusal        2   0   5   1  11       41\n",
      "\n",
      "############################################################\n",
      "#  Mistral-7B-Instruct-v0.3 : templ-7.csv\n",
      "############################################################\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.83      0.81      0.82       137\n",
      "           2       0.50      0.57      0.53        63\n",
      "           3       0.70      0.56      0.62        93\n",
      "           4       0.49      0.70      0.57        56\n",
      "           5       0.73      0.81      0.77        91\n",
      "     refusal       1.00      0.65      0.79        60\n",
      "\n",
      "    accuracy                           0.70       500\n",
      "   macro avg       0.71      0.68      0.68       500\n",
      "weighted avg       0.73      0.70      0.71       500\n",
      "\n",
      "eval_label     1   2   3   4   5  refusal\n",
      "final_label                              \n",
      "1            111  21   2   0   3        0\n",
      "2             16  36  10   1   0        0\n",
      "3              3  11  52  25   2        0\n",
      "4              0   1   2  39  14        0\n",
      "5              0   0   2  15  74        0\n",
      "refusal        4   3   6   0   8       39\n",
      "\n",
      "############################################################\n",
      "#  Mistral-7B-Instruct-v0.3 : templ-8.csv\n",
      "############################################################\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.80      0.80      0.80       137\n",
      "           2       0.40      0.46      0.43        63\n",
      "           3       0.68      0.47      0.56        93\n",
      "           4       0.47      0.88      0.61        56\n",
      "           5       0.77      0.73      0.75        91\n",
      "     refusal       1.00      0.60      0.75        60\n",
      "\n",
      "    accuracy                           0.67       500\n",
      "   macro avg       0.69      0.65      0.65       500\n",
      "weighted avg       0.71      0.67      0.67       500\n",
      "\n",
      "eval_label     1   2   3   4   5  refusal\n",
      "final_label                              \n",
      "1            109  23   2   1   2        0\n",
      "2             19  29  10   3   2        0\n",
      "3              3  17  44  28   1        0\n",
      "4              0   0   1  49   6        0\n",
      "5              0   0   3  22  66        0\n",
      "refusal        5   4   5   1   9       36\n",
      "\n",
      "############################################################\n",
      "#  Mistral-Nemo-Instruct-2407 : templ-1.csv\n",
      "############################################################\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.79      0.90      0.84       137\n",
      "           2       0.64      0.22      0.33        63\n",
      "           3       0.61      0.82      0.70        93\n",
      "           4       0.56      0.41      0.47        56\n",
      "           5       0.61      0.89      0.72        91\n",
      "     refusal       1.00      0.42      0.59        60\n",
      "\n",
      "    accuracy                           0.68       500\n",
      "   macro avg       0.70      0.61      0.61       500\n",
      "weighted avg       0.71      0.68      0.66       500\n",
      "\n",
      "eval_label     1   2   3   4   5  refusal\n",
      "final_label                              \n",
      "1            123   7   5   1   1        0\n",
      "2             22  14  26   1   0        0\n",
      "3              5   1  76   8   3        0\n",
      "4              0   0   9  23  24        0\n",
      "5              0   0   2   8  81        0\n",
      "refusal        5   0   6   0  24       25\n",
      "\n",
      "############################################################\n",
      "#  Mistral-Nemo-Instruct-2407 : templ-2.csv\n",
      "############################################################\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.80      0.88      0.84       137\n",
      "           2       0.65      0.24      0.35        63\n",
      "           3       0.53      0.86      0.65        93\n",
      "           4       0.51      0.41      0.46        56\n",
      "           5       0.68      0.74      0.71        91\n",
      "     refusal       1.00      0.52      0.68        60\n",
      "\n",
      "    accuracy                           0.67       500\n",
      "   macro avg       0.69      0.61      0.61       500\n",
      "weighted avg       0.70      0.67      0.66       500\n",
      "\n",
      "eval_label     1   2   3   4   5  refusal\n",
      "final_label                              \n",
      "1            120   2  13   0   2        0\n",
      "2             23  15  25   0   0        0\n",
      "3              3   3  80   7   0        0\n",
      "4              0   1  20  23  12        0\n",
      "5              0   0  10  14  67        0\n",
      "refusal        4   2   4   1  18       31\n",
      "\n",
      "############################################################\n",
      "#  Mistral-Nemo-Instruct-2407 : templ-3.csv\n",
      "############################################################\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.81      0.34      0.47       137\n",
      "           2       0.50      0.13      0.20        63\n",
      "           3       0.35      0.94      0.51        93\n",
      "           4       0.35      0.41      0.38        56\n",
      "           5       0.65      0.55      0.60        91\n",
      "     refusal       1.00      0.60      0.75        60\n",
      "\n",
      "    accuracy                           0.50       500\n",
      "   macro avg       0.61      0.49      0.48       500\n",
      "weighted avg       0.63      0.50      0.49       500\n",
      "\n",
      "eval_label    1  2   3   4   5  refusal\n",
      "final_label                            \n",
      "1            46  6  74   9   2        0\n",
      "2             9  8  45   1   0        0\n",
      "3             0  1  87   3   2        0\n",
      "4             0  0  21  23  12        0\n",
      "5             1  0  13  27  50        0\n",
      "refusal       1  1   8   3  11       36\n",
      "\n",
      "############################################################\n",
      "#  Mistral-Nemo-Instruct-2407 : templ-4.csv\n",
      "############################################################\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.74      0.54      0.62       137\n",
      "           2       0.56      0.32      0.40        63\n",
      "           3       0.43      0.90      0.58        93\n",
      "           4       0.39      0.43      0.41        56\n",
      "           5       0.64      0.48      0.55        91\n",
      "     refusal       0.92      0.60      0.73        60\n",
      "\n",
      "    accuracy                           0.56       500\n",
      "   macro avg       0.61      0.55      0.55       500\n",
      "weighted avg       0.62      0.56      0.56       500\n",
      "\n",
      "eval_label    1   2   3   4   5  refusal\n",
      "final_label                             \n",
      "1            74  11  42   8   1        1\n",
      "2            15  20  25   3   0        0\n",
      "3             2   2  84   4   0        1\n",
      "4             1   0  21  24  10        0\n",
      "5             6   1  18  21  44        1\n",
      "refusal       2   2   5   1  14       36\n",
      "\n",
      "############################################################\n",
      "#  Mistral-Nemo-Instruct-2407 : templ-5.csv\n",
      "############################################################\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.89      0.59      0.71       137\n",
      "           2       0.36      0.43      0.39        63\n",
      "           3       0.65      0.80      0.71        93\n",
      "           4       0.37      0.82      0.51        56\n",
      "           5       0.78      0.35      0.48        91\n",
      "     refusal       0.98      0.92      0.95        60\n",
      "\n",
      "    accuracy                           0.63       500\n",
      "   macro avg       0.67      0.65      0.63       500\n",
      "weighted avg       0.71      0.63      0.64       500\n",
      "\n",
      "eval_label    1   2   3   4   5  refusal\n",
      "final_label                             \n",
      "1            81  43  10   3   0        0\n",
      "2             9  27  25   1   0        1\n",
      "3             1   3  74  14   1        0\n",
      "4             0   0   4  46   6        0\n",
      "5             0   0   1  58  32        0\n",
      "refusal       0   1   0   2   2       55\n",
      "\n",
      "############################################################\n",
      "#  Mistral-Nemo-Instruct-2407 : templ-6.csv\n",
      "############################################################\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.86      0.62      0.72       137\n",
      "           2       0.35      0.37      0.36        63\n",
      "           3       0.62      0.78      0.70        93\n",
      "           4       0.42      0.79      0.54        56\n",
      "           5       0.75      0.49      0.60        91\n",
      "     refusal       0.98      0.87      0.92        60\n",
      "\n",
      "    accuracy                           0.64       500\n",
      "   macro avg       0.66      0.65      0.64       500\n",
      "weighted avg       0.70      0.64      0.65       500\n",
      "\n",
      "eval_label    1   2   3   4   5  refusal\n",
      "final_label                             \n",
      "1            85  37  12   2   1        0\n",
      "2            12  23  27   0   0        1\n",
      "3             2   4  73  13   1        0\n",
      "4             0   0   4  44   8        0\n",
      "5             0   0   1  45  45        0\n",
      "refusal       0   1   0   2   5       52\n",
      "\n",
      "############################################################\n",
      "#  Mistral-Nemo-Instruct-2407 : templ-7.csv\n",
      "############################################################\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.89      0.43      0.58       137\n",
      "           2       0.29      0.41      0.34        63\n",
      "           3       0.56      0.81      0.66        93\n",
      "           4       0.34      0.79      0.48        56\n",
      "           5       0.71      0.24      0.36        91\n",
      "     refusal       0.98      0.80      0.88        60\n",
      "\n",
      "    accuracy                           0.55       500\n",
      "   macro avg       0.63      0.58      0.55       500\n",
      "weighted avg       0.67      0.55      0.55       500\n",
      "\n",
      "eval_label    1   2   3   4   5  refusal\n",
      "final_label                             \n",
      "1            59  63  13   2   0        0\n",
      "2             5  26  31   0   0        1\n",
      "3             1   1  75  16   0        0\n",
      "4             0   0   9  44   3        0\n",
      "5             0   0   3  66  22        0\n",
      "refusal       1   1   3   1   6       48\n",
      "\n",
      "############################################################\n",
      "#  Mistral-Nemo-Instruct-2407 : templ-8.csv\n",
      "############################################################\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.89      0.46      0.61       137\n",
      "           2       0.33      0.57      0.42        63\n",
      "           3       0.65      0.81      0.72        93\n",
      "           4       0.38      0.73      0.50        56\n",
      "           5       0.81      0.38      0.52        91\n",
      "     refusal       0.98      0.85      0.91        60\n",
      "\n",
      "    accuracy                           0.60       500\n",
      "   macro avg       0.67      0.63      0.61       500\n",
      "weighted avg       0.71      0.60      0.61       500\n",
      "\n",
      "eval_label    1   2   3   4   5  refusal\n",
      "final_label                             \n",
      "1            63  68   4   2   0        0\n",
      "2             4  36  22   0   0        1\n",
      "3             2   4  75  12   0        0\n",
      "4             0   0  11  41   4        0\n",
      "5             0   0   3  53  35        0\n",
      "refusal       2   2   0   1   4       51\n",
      "\n",
      "############################################################\n",
      "#  Qwen2.5-72B-Instruct : templ-1.csv\n",
      "############################################################\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.85      0.82      0.84       137\n",
      "           2       0.52      0.57      0.55        63\n",
      "           3       0.72      0.77      0.75        93\n",
      "           4       0.57      0.45      0.50        56\n",
      "           5       0.68      0.89      0.77        91\n",
      "     refusal       1.00      0.62      0.76        60\n",
      "\n",
      "    accuracy                           0.73       500\n",
      "   macro avg       0.72      0.69      0.69       500\n",
      "weighted avg       0.74      0.73      0.72       500\n",
      "\n",
      "eval_label     1   2   3   4   5  refusal\n",
      "final_label                              \n",
      "1            112  24   1   0   0        0\n",
      "2             14  36  12   0   1        0\n",
      "3              2   7  72  10   2        0\n",
      "4              0   0  14  25  17        0\n",
      "5              0   0   1   9  81        0\n",
      "refusal        3   2   0   0  18       37\n",
      "\n",
      "############################################################\n",
      "#  Qwen2.5-72B-Instruct : templ-2.csv\n",
      "############################################################\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.93      0.73      0.82       137\n",
      "           2       0.55      0.67      0.60        63\n",
      "           3       0.66      0.85      0.74        93\n",
      "           4       0.61      0.55      0.58        56\n",
      "           5       0.73      0.85      0.78        91\n",
      "     refusal       0.97      0.63      0.77        60\n",
      "\n",
      "    accuracy                           0.73       500\n",
      "   macro avg       0.74      0.71      0.72       500\n",
      "weighted avg       0.76      0.73      0.74       500\n",
      "\n",
      "eval_label     1   2   3   4   5  refusal\n",
      "final_label                              \n",
      "1            100  29   7   1   0        0\n",
      "2              3  42  18   0   0        0\n",
      "3              1   3  79   9   1        0\n",
      "4              0   0  13  31  12        0\n",
      "5              0   0   3  10  77        1\n",
      "refusal        3   3   0   0  16       38\n",
      "\n",
      "############################################################\n",
      "#  Qwen2.5-72B-Instruct : templ-3.csv\n",
      "############################################################\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.83      0.57      0.68       137\n",
      "           2       0.48      0.44      0.46        63\n",
      "           3       0.59      0.88      0.70        93\n",
      "           4       0.34      0.48      0.40        56\n",
      "           5       0.71      0.54      0.61        91\n",
      "     refusal       0.78      0.77      0.77        60\n",
      "\n",
      "    accuracy                           0.62       500\n",
      "   macro avg       0.62      0.61      0.60       500\n",
      "weighted avg       0.66      0.62      0.62       500\n",
      "\n",
      "eval_label    1   2   3   4   5  refusal\n",
      "final_label                             \n",
      "1            78  23  17  13   0        6\n",
      "2            14  28  19   1   0        1\n",
      "3             0   4  82   5   0        2\n",
      "4             0   1  17  27  10        1\n",
      "5             1   1   4  33  49        3\n",
      "refusal       1   1   1   1  10       46\n",
      "\n",
      "############################################################\n",
      "#  Qwen2.5-72B-Instruct : templ-4.csv\n",
      "############################################################\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.95      0.52      0.67       137\n",
      "           2       0.64      0.56      0.59        63\n",
      "           3       0.48      0.88      0.62        93\n",
      "           4       0.56      0.68      0.61        56\n",
      "           5       0.88      0.56      0.68        91\n",
      "     refusal       0.73      0.90      0.81        60\n",
      "\n",
      "    accuracy                           0.66       500\n",
      "   macro avg       0.71      0.68      0.67       500\n",
      "weighted avg       0.74      0.66      0.66       500\n",
      "\n",
      "eval_label    1   2   3   4   5  refusal\n",
      "final_label                             \n",
      "1            71  15  40   6   1        4\n",
      "2             4  35  23   1   0        0\n",
      "3             0   3  82   4   0        4\n",
      "4             0   0  14  38   3        1\n",
      "5             0   1  10  18  51       11\n",
      "refusal       0   1   1   1   3       54\n",
      "\n",
      "############################################################\n",
      "#  Qwen2.5-72B-Instruct : templ-5.csv\n",
      "############################################################\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.91      0.78      0.84       137\n",
      "           2       0.52      0.71      0.60        63\n",
      "           3       0.82      0.70      0.76        93\n",
      "           4       0.54      0.68      0.60        56\n",
      "           5       0.82      0.79      0.80        91\n",
      "     refusal       0.98      0.93      0.96        60\n",
      "\n",
      "    accuracy                           0.77       500\n",
      "   macro avg       0.76      0.77      0.76       500\n",
      "weighted avg       0.79      0.77      0.77       500\n",
      "\n",
      "eval_label     1   2   3   4   5  refusal\n",
      "final_label                              \n",
      "1            107  29   0   1   0        0\n",
      "2              9  45   8   0   0        1\n",
      "3              2  13  65  13   0        0\n",
      "4              0   0   6  38  12        0\n",
      "5              0   0   0  19  72        0\n",
      "refusal        0   0   0   0   4       56\n",
      "\n",
      "############################################################\n",
      "#  Qwen2.5-72B-Instruct : templ-6.csv\n",
      "############################################################\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.91      0.67      0.77       137\n",
      "           2       0.44      0.70      0.54        63\n",
      "           3       0.80      0.72      0.76        93\n",
      "           4       0.59      0.57      0.58        56\n",
      "           5       0.79      0.89      0.84        91\n",
      "     refusal       0.98      0.93      0.96        60\n",
      "\n",
      "    accuracy                           0.74       500\n",
      "   macro avg       0.75      0.75      0.74       500\n",
      "weighted avg       0.78      0.74      0.75       500\n",
      "\n",
      "eval_label    1   2   3   4   5  refusal\n",
      "final_label                             \n",
      "1            92  44   0   1   0        0\n",
      "2             8  44  10   0   0        1\n",
      "3             1  13  67  11   1        0\n",
      "4             0   0   7  32  17        0\n",
      "5             0   0   0  10  81        0\n",
      "refusal       0   0   0   0   4       56\n",
      "\n",
      "############################################################\n",
      "#  Qwen2.5-72B-Instruct : templ-7.csv\n",
      "############################################################\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.89      0.71      0.79       137\n",
      "           2       0.47      0.70      0.56        63\n",
      "           3       0.73      0.74      0.73        93\n",
      "           4       0.54      0.68      0.60        56\n",
      "           5       0.88      0.75      0.81        91\n",
      "     refusal       1.00      0.93      0.97        60\n",
      "\n",
      "    accuracy                           0.74       500\n",
      "   macro avg       0.75      0.75      0.74       500\n",
      "weighted avg       0.78      0.74      0.75       500\n",
      "\n",
      "eval_label    1   2   3   4   5  refusal\n",
      "final_label                             \n",
      "1            97  38   2   0   0        0\n",
      "2             9  44  10   0   0        0\n",
      "3             2  11  69  11   0        0\n",
      "4             0   0  12  38   6        0\n",
      "5             0   0   2  21  68        0\n",
      "refusal       1   0   0   0   3       56\n",
      "\n",
      "############################################################\n",
      "#  Qwen2.5-72B-Instruct : templ-8.csv\n",
      "############################################################\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.91      0.72      0.80       137\n",
      "           2       0.48      0.76      0.59        63\n",
      "           3       0.78      0.69      0.73        93\n",
      "           4       0.57      0.77      0.66        56\n",
      "           5       0.88      0.77      0.82        91\n",
      "     refusal       0.98      0.92      0.95        60\n",
      "\n",
      "    accuracy                           0.76       500\n",
      "   macro avg       0.77      0.77      0.76       500\n",
      "weighted avg       0.80      0.76      0.77       500\n",
      "\n",
      "eval_label    1   2   3   4   5  refusal\n",
      "final_label                             \n",
      "1            98  37   2   0   0        0\n",
      "2             7  48   8   0   0        0\n",
      "3             2  14  64  12   0        1\n",
      "4             0   0   7  43   6        0\n",
      "5             0   0   1  20  70        0\n",
      "refusal       1   0   0   0   4       55\n",
      "\n",
      "############################################################\n",
      "#  gemma-2-27b-it : templ-1.csv\n",
      "############################################################\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.96      0.63      0.76       137\n",
      "           2       0.40      0.92      0.56        63\n",
      "           3       0.76      0.48      0.59        93\n",
      "           4       0.45      0.73      0.56        56\n",
      "           5       0.64      0.70      0.67        91\n",
      "     refusal       1.00      0.23      0.38        60\n",
      "\n",
      "    accuracy                           0.62       500\n",
      "   macro avg       0.70      0.62      0.59       500\n",
      "weighted avg       0.74      0.62      0.62       500\n",
      "\n",
      "eval_label    1   2   3   4   5  refusal\n",
      "final_label                             \n",
      "1            86  51   0   0   0        0\n",
      "2             0  58   3   2   0        0\n",
      "3             1  24  45  22   1        0\n",
      "4             0   2   8  41   5        0\n",
      "5             0   0   1  26  64        0\n",
      "refusal       3  11   2   0  30       14\n",
      "\n",
      "############################################################\n",
      "#  gemma-2-27b-it : templ-2.csv\n",
      "############################################################\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.90      0.78      0.84       137\n",
      "           2       0.49      0.78      0.60        63\n",
      "           3       0.78      0.69      0.73        93\n",
      "           4       0.52      0.70      0.60        56\n",
      "           5       0.68      0.71      0.70        91\n",
      "     refusal       1.00      0.45      0.62        60\n",
      "\n",
      "    accuracy                           0.70       500\n",
      "   macro avg       0.73      0.68      0.68       500\n",
      "weighted avg       0.75      0.70      0.71       500\n",
      "\n",
      "eval_label     1   2   3   4   5  refusal\n",
      "final_label                              \n",
      "1            107  27   0   3   0        0\n",
      "2              7  49   6   1   0        0\n",
      "3              2  16  64   9   2        0\n",
      "4              0   2   7  39   8        0\n",
      "5              0   0   3  23  65        0\n",
      "refusal        3   7   2   0  21       27\n",
      "\n",
      "############################################################\n",
      "#  gemma-2-27b-it : templ-3.csv\n",
      "############################################################\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.97      0.41      0.57       137\n",
      "           2       0.41      0.86      0.56        63\n",
      "           3       0.64      0.75      0.69        93\n",
      "           4       0.28      0.57      0.38        56\n",
      "           5       0.66      0.36      0.47        91\n",
      "     refusal       1.00      0.62      0.76        60\n",
      "\n",
      "    accuracy                           0.56       500\n",
      "   macro avg       0.66      0.59      0.57       500\n",
      "weighted avg       0.71      0.56      0.57       500\n",
      "\n",
      "eval_label    1   2   3   4   5  refusal\n",
      "final_label                             \n",
      "1            56  56   9  16   0        0\n",
      "2             0  54   6   3   0        0\n",
      "3             0  12  70  11   0        0\n",
      "4             0   3  17  32   4        0\n",
      "5             1   4   4  49  33        0\n",
      "refusal       1   2   4   3  13       37\n",
      "\n",
      "############################################################\n",
      "#  gemma-2-27b-it : templ-4.csv\n",
      "############################################################\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.88      0.17      0.28       137\n",
      "           2       0.38      0.67      0.49        63\n",
      "           3       0.44      0.88      0.59        93\n",
      "           4       0.38      0.71      0.50        56\n",
      "           5       0.63      0.21      0.31        91\n",
      "     refusal       0.98      0.72      0.83        60\n",
      "\n",
      "    accuracy                           0.50       500\n",
      "   macro avg       0.62      0.56      0.50       500\n",
      "weighted avg       0.65      0.50      0.46       500\n",
      "\n",
      "eval_label    1   2   3   4   5  refusal\n",
      "final_label                             \n",
      "1            23  55  52   7   0        0\n",
      "2             1  42  18   2   0        0\n",
      "3             0   6  82   5   0        0\n",
      "4             0   1  13  40   2        0\n",
      "5             1   4  18  48  19        1\n",
      "refusal       1   2   2   3   9       43\n",
      "\n",
      "############################################################\n",
      "#  gemma-2-27b-it : templ-5.csv\n",
      "############################################################\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.96      0.62      0.75       137\n",
      "           2       0.43      0.87      0.57        63\n",
      "           3       0.89      0.53      0.66        93\n",
      "           4       0.40      0.88      0.55        56\n",
      "           5       0.80      0.48      0.60        91\n",
      "     refusal       1.00      0.85      0.92        60\n",
      "\n",
      "    accuracy                           0.67       500\n",
      "   macro avg       0.75      0.70      0.68       500\n",
      "weighted avg       0.79      0.67      0.68       500\n",
      "\n",
      "eval_label    1   2   3   4   5  refusal\n",
      "final_label                             \n",
      "1            85  51   0   1   0        0\n",
      "2             2  55   4   2   0        0\n",
      "3             1  21  49  22   0        0\n",
      "4             0   1   2  49   4        0\n",
      "5             0   0   0  47  44        0\n",
      "refusal       1   1   0   0   7       51\n",
      "\n",
      "############################################################\n",
      "#  gemma-2-27b-it : templ-6.csv\n",
      "############################################################\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.97      0.53      0.68       137\n",
      "           2       0.40      0.90      0.56        63\n",
      "           3       0.87      0.58      0.70        93\n",
      "           4       0.44      0.80      0.57        56\n",
      "           5       0.83      0.63      0.71        91\n",
      "     refusal       1.00      0.85      0.92        60\n",
      "\n",
      "    accuracy                           0.67       500\n",
      "   macro avg       0.75      0.72      0.69       500\n",
      "weighted avg       0.80      0.67      0.69       500\n",
      "\n",
      "eval_label    1   2   3   4   5  refusal\n",
      "final_label                             \n",
      "1            72  64   0   1   0        0\n",
      "2             0  57   4   2   0        0\n",
      "3             1  18  54  20   0        0\n",
      "4             0   1   4  45   6        0\n",
      "5             0   0   0  34  57        0\n",
      "refusal       1   2   0   0   6       51\n",
      "\n",
      "############################################################\n",
      "#  gemma-2-27b-it : templ-7.csv\n",
      "############################################################\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.98      0.47      0.63       137\n",
      "           2       0.37      0.92      0.53        63\n",
      "           3       0.79      0.53      0.63        93\n",
      "           4       0.38      0.80      0.52        56\n",
      "           5       0.74      0.43      0.54        91\n",
      "     refusal       1.00      0.75      0.86        60\n",
      "\n",
      "    accuracy                           0.60       500\n",
      "   macro avg       0.71      0.65      0.62       500\n",
      "weighted avg       0.76      0.60      0.62       500\n",
      "\n",
      "eval_label    1   2   3   4   5  refusal\n",
      "final_label                             \n",
      "1            64  72   1   0   0        0\n",
      "2             0  58   4   1   0        0\n",
      "3             0  23  49  21   0        0\n",
      "4             0   1   6  45   4        0\n",
      "5             0   0   1  51  39        0\n",
      "refusal       1   3   1   0  10       45\n",
      "\n",
      "############################################################\n",
      "#  gemma-2-27b-it : templ-8.csv\n",
      "############################################################\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.98      0.39      0.55       137\n",
      "           2       0.32      0.98      0.49        63\n",
      "           3       0.84      0.41      0.55        93\n",
      "           4       0.35      0.82      0.49        56\n",
      "           5       0.74      0.27      0.40        91\n",
      "     refusal       1.00      0.70      0.82        60\n",
      "\n",
      "    accuracy                           0.53       500\n",
      "   macro avg       0.71      0.60      0.55       500\n",
      "weighted avg       0.76      0.53      0.54       500\n",
      "\n",
      "eval_label    1   2   3   4   5  refusal\n",
      "final_label                             \n",
      "1            53  83   1   0   0        0\n",
      "2             0  62   1   0   0        0\n",
      "3             0  36  38  19   0        0\n",
      "4             0   6   4  46   0        0\n",
      "5             0   1   0  65  25        0\n",
      "refusal       1   4   1   3   9       42\n",
      "\n",
      "############################################################\n",
      "#  gemma-2-9b-it : templ-1.csv\n",
      "############################################################\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.94      0.57      0.71       137\n",
      "           2       0.39      0.87      0.54        63\n",
      "           3       0.66      0.61      0.63        93\n",
      "           4       0.51      0.43      0.47        56\n",
      "           5       0.62      0.87      0.72        91\n",
      "     refusal       1.00      0.23      0.38        60\n",
      "\n",
      "    accuracy                           0.61       500\n",
      "   macro avg       0.69      0.60      0.57       500\n",
      "weighted avg       0.72      0.61      0.61       500\n",
      "\n",
      "eval_label    1   2   3   4   5  refusal\n",
      "final_label                             \n",
      "1            78  57   2   0   0        0\n",
      "2             1  55   7   0   0        0\n",
      "3             0  22  57  12   2        0\n",
      "4             0   0  17  24  15        0\n",
      "5             0   0   1  11  79        0\n",
      "refusal       4   8   3   0  31       14\n",
      "\n",
      "############################################################\n",
      "#  gemma-2-9b-it : templ-2.csv\n",
      "############################################################\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.91      0.67      0.77       137\n",
      "           2       0.41      0.83      0.55        63\n",
      "           3       0.74      0.68      0.71        93\n",
      "           4       0.52      0.61      0.56        56\n",
      "           5       0.70      0.70      0.70        91\n",
      "     refusal       1.00      0.52      0.68        60\n",
      "\n",
      "    accuracy                           0.67       500\n",
      "   macro avg       0.71      0.67      0.66       500\n",
      "weighted avg       0.75      0.67      0.69       500\n",
      "\n",
      "eval_label    1   2   3   4   5  refusal\n",
      "final_label                             \n",
      "1            92  43   0   2   0        0\n",
      "2             4  52   7   0   0        0\n",
      "3             2  21  63   7   0        0\n",
      "4             0   4  10  34   8        0\n",
      "5             0   1   3  23  64        0\n",
      "refusal       3   5   2   0  19       31\n",
      "\n",
      "############################################################\n",
      "#  gemma-2-9b-it : templ-3.csv\n",
      "############################################################\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.92      0.32      0.48       137\n",
      "           2       0.34      0.75      0.46        63\n",
      "           3       0.59      0.72      0.65        93\n",
      "           4       0.33      0.52      0.40        56\n",
      "           5       0.60      0.57      0.58        91\n",
      "     refusal       1.00      0.38      0.55        60\n",
      "\n",
      "    accuracy                           0.52       500\n",
      "   macro avg       0.63      0.54      0.52       500\n",
      "weighted avg       0.67      0.52      0.53       500\n",
      "\n",
      "eval_label    1   2   3   4   5  refusal\n",
      "final_label                             \n",
      "1            44  70  11   9   3        0\n",
      "2             2  47   9   5   0        0\n",
      "3             0  17  67   9   0        0\n",
      "4             0   1  14  29  12        0\n",
      "5             1   1   8  29  52        0\n",
      "refusal       1   4   5   7  20       23\n",
      "\n",
      "############################################################\n",
      "#  gemma-2-9b-it : templ-4.csv\n",
      "############################################################\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.86      0.45      0.59       137\n",
      "           2       0.48      0.70      0.57        63\n",
      "           3       0.49      0.83      0.61        93\n",
      "           4       0.47      0.73      0.57        56\n",
      "           5       0.79      0.36      0.50        91\n",
      "     refusal       0.94      0.77      0.84        60\n",
      "\n",
      "    accuracy                           0.61       500\n",
      "   macro avg       0.67      0.64      0.61       500\n",
      "weighted avg       0.70      0.61      0.60       500\n",
      "\n",
      "eval_label    1   2   3   4   5  refusal\n",
      "final_label                             \n",
      "1            62  33  35   6   0        1\n",
      "2             4  44  13   2   0        0\n",
      "3             0   7  77   7   1        1\n",
      "4             0   2  10  41   3        0\n",
      "5             4   2  19  32  33        1\n",
      "refusal       2   3   4   0   5       46\n",
      "\n",
      "############################################################\n",
      "#  gemma-2-9b-it : templ-5.csv\n",
      "############################################################\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.94      0.36      0.52       137\n",
      "           2       0.33      0.89      0.48        63\n",
      "           3       0.75      0.61      0.67        93\n",
      "           4       0.33      0.73      0.46        56\n",
      "           5       0.69      0.30      0.42        91\n",
      "     refusal       1.00      0.65      0.79        60\n",
      "\n",
      "    accuracy                           0.54       500\n",
      "   macro avg       0.67      0.59      0.56       500\n",
      "weighted avg       0.72      0.54      0.55       500\n",
      "\n",
      "eval_label    1   2   3   4   5  refusal\n",
      "final_label                             \n",
      "1            49  87   0   1   0        0\n",
      "2             0  56   5   2   0        0\n",
      "3             0  24  57  12   0        0\n",
      "4             0   0  12  41   3        0\n",
      "5             0   0   2  62  27        0\n",
      "refusal       3   3   0   6   9       39\n",
      "\n",
      "############################################################\n",
      "#  gemma-2-9b-it : templ-6.csv\n",
      "############################################################\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.96      0.32      0.48       137\n",
      "           2       0.33      0.90      0.48        63\n",
      "           3       0.76      0.61      0.68        93\n",
      "           4       0.38      0.73      0.50        56\n",
      "           5       0.73      0.47      0.57        91\n",
      "     refusal       1.00      0.62      0.76        60\n",
      "\n",
      "    accuracy                           0.56       500\n",
      "   macro avg       0.69      0.61      0.58       500\n",
      "weighted avg       0.74      0.56      0.57       500\n",
      "\n",
      "eval_label    1   2   3   4   5  refusal\n",
      "final_label                             \n",
      "1            44  92   0   1   0        0\n",
      "2             0  57   4   2   0        0\n",
      "3             0  21  57  14   1        0\n",
      "4             0   0  13  41   2        0\n",
      "5             0   0   1  47  43        0\n",
      "refusal       2   5   0   3  13       37\n",
      "\n",
      "############################################################\n",
      "#  gemma-2-9b-it : templ-7.csv\n",
      "############################################################\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.95      0.28      0.44       137\n",
      "           2       0.31      0.89      0.46        63\n",
      "           3       0.68      0.65      0.66        93\n",
      "           4       0.31      0.66      0.42        56\n",
      "           5       0.68      0.29      0.40        91\n",
      "     refusal       1.00      0.57      0.72        60\n",
      "\n",
      "    accuracy                           0.50       500\n",
      "   macro avg       0.66      0.56      0.52       500\n",
      "weighted avg       0.71      0.50      0.51       500\n",
      "\n",
      "eval_label    1   2   3   4   5  refusal\n",
      "final_label                             \n",
      "1            39  96   1   1   0        0\n",
      "2             0  56   6   1   0        0\n",
      "3             0  22  60  11   0        0\n",
      "4             0   0  18  37   1        0\n",
      "5             0   0   2  63  26        0\n",
      "refusal       2   6   1   6  11       34\n",
      "\n",
      "############################################################\n",
      "#  gemma-2-9b-it : templ-8.csv\n",
      "############################################################\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.96      0.36      0.53       137\n",
      "           2       0.33      0.92      0.49        63\n",
      "           3       0.66      0.60      0.63        93\n",
      "           4       0.30      0.68      0.42        56\n",
      "           5       0.66      0.23      0.34        91\n",
      "     refusal       1.00      0.53      0.70        60\n",
      "\n",
      "    accuracy                           0.51       500\n",
      "   macro avg       0.65      0.56      0.52       500\n",
      "weighted avg       0.70      0.51      0.52       500\n",
      "\n",
      "eval_label    1   2   3   4   5  refusal\n",
      "final_label                             \n",
      "1            50  85   1   1   0        0\n",
      "2             0  58   4   1   0        0\n",
      "3             0  24  56  13   0        0\n",
      "4             0   0  18  38   0        0\n",
      "5             0   0   4  66  21        0\n",
      "refusal       2   7   2   6  11       32\n",
      "\n",
      "############################################################\n",
      "#  gpt-3.5-turbo : templ-1.csv\n",
      "############################################################\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.72      0.09      0.17       137\n",
      "           2       0.23      0.54      0.33        63\n",
      "           3       0.46      0.89      0.61        93\n",
      "           4       0.65      0.39      0.49        56\n",
      "           5       0.63      0.81      0.71        91\n",
      "     refusal       1.00      0.08      0.15        60\n",
      "\n",
      "    accuracy                           0.46       500\n",
      "   macro avg       0.62      0.47      0.41       500\n",
      "weighted avg       0.62      0.46      0.40       500\n",
      "\n",
      "eval_label    1    2   3   4   5  refusal\n",
      "final_label                              \n",
      "1            13  102  22   0   0        0\n",
      "2             1   34  28   0   0        0\n",
      "3             0    5  83   3   2        0\n",
      "4             0    0  19  22  15        0\n",
      "5             0    0   9   8  74        0\n",
      "refusal       4    5  18   1  27        5\n",
      "\n",
      "############################################################\n",
      "#  gpt-3.5-turbo : templ-2.csv\n",
      "############################################################\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.77      0.07      0.13       137\n",
      "           2       0.22      0.40      0.29        63\n",
      "           3       0.42      0.91      0.57        93\n",
      "           4       0.52      0.48      0.50        56\n",
      "           5       0.62      0.62      0.62        91\n",
      "     refusal       1.00      0.48      0.65        60\n",
      "\n",
      "    accuracy                           0.46       500\n",
      "   macro avg       0.59      0.49      0.46       500\n",
      "weighted avg       0.61      0.46      0.43       500\n",
      "\n",
      "eval_label    1   2   3   4   5  refusal\n",
      "final_label                             \n",
      "1            10  83  42   2   0        0\n",
      "2             0  25  38   0   0        0\n",
      "3             0   3  85   4   1        0\n",
      "4             0   0  17  27  12        0\n",
      "5             0   0  16  19  56        0\n",
      "refusal       3   1   5   0  22       29\n",
      "\n",
      "############################################################\n",
      "#  gpt-3.5-turbo : templ-3.csv\n",
      "############################################################\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.50      0.01      0.01       137\n",
      "           2       0.50      0.11      0.18        63\n",
      "           3       0.24      0.96      0.39        93\n",
      "           4       0.40      0.34      0.37        56\n",
      "           5       0.68      0.33      0.44        91\n",
      "     refusal       0.44      0.20      0.28        60\n",
      "\n",
      "    accuracy                           0.32       500\n",
      "   macro avg       0.46      0.32      0.28       500\n",
      "weighted avg       0.47      0.32      0.25       500\n",
      "\n",
      "eval_label   1  2    3   4   5  refusal\n",
      "final_label                            \n",
      "1            1  6  119   3   1        7\n",
      "2            1  7   53   1   0        1\n",
      "3            0  1   89   0   0        3\n",
      "4            0  0   35  19   2        0\n",
      "5            0  0   33  24  30        4\n",
      "refusal      0  0   36   1  11       12\n",
      "\n",
      "############################################################\n",
      "#  gpt-3.5-turbo : templ-4.csv\n",
      "############################################################\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.00      0.00      0.00       137\n",
      "           2       0.31      0.13      0.18        63\n",
      "           3       0.28      0.96      0.43        93\n",
      "           4       0.38      0.50      0.43        56\n",
      "           5       0.57      0.19      0.28        91\n",
      " PARSE ERROR       0.00      0.00      0.00         0\n",
      "     refusal       0.78      0.63      0.70        60\n",
      "\n",
      "    accuracy                           0.36       500\n",
      "   macro avg       0.33      0.34      0.29       500\n",
      "weighted avg       0.33      0.36      0.29       500\n",
      "\n",
      "eval_label    2    3   4   5  PARSE ERROR  refusal\n",
      "final_label                                       \n",
      "1            18  111   6   0            0        2\n",
      "2             8   53   2   0            0        0\n",
      "3             0   89   1   0            1        2\n",
      "4             0   26  28   2            0        0\n",
      "5             0   30  37  17            0        7\n",
      "refusal       0   11   0  11            0       38\n",
      "\n",
      "############################################################\n",
      "#  gpt-3.5-turbo : templ-5.csv\n",
      "############################################################\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.25      0.01      0.01       137\n",
      "           2       0.25      0.78      0.38        63\n",
      "           3       0.54      0.85      0.66        93\n",
      "           4       0.44      0.75      0.55        56\n",
      "           5       0.56      0.24      0.34        91\n",
      "     refusal       1.00      0.30      0.46        60\n",
      "\n",
      "    accuracy                           0.42       500\n",
      "   macro avg       0.51      0.49      0.40       500\n",
      "weighted avg       0.47      0.42      0.35       500\n",
      "\n",
      "eval_label   1    2   3   4   5  refusal\n",
      "final_label                             \n",
      "1            1  127   7   2   0        0\n",
      "2            0   49  14   0   0        0\n",
      "3            0   11  79   3   0        0\n",
      "4            0    1  13  42   0        0\n",
      "5            0    1  23  45  22        0\n",
      "refusal      3    7  11   4  17       18\n",
      "\n",
      "############################################################\n",
      "#  gpt-3.5-turbo : templ-6.csv\n",
      "############################################################\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.17      0.01      0.01       137\n",
      "           2       0.27      0.84      0.40        63\n",
      "           3       0.55      0.83      0.66        93\n",
      "           4       0.46      0.71      0.56        56\n",
      "           5       0.53      0.29      0.37        91\n",
      "     refusal       1.00      0.30      0.46        60\n",
      "\n",
      "    accuracy                           0.43       500\n",
      "   macro avg       0.50      0.50      0.41       500\n",
      "weighted avg       0.45      0.43      0.36       500\n",
      "\n",
      "eval_label   1    2   3   4   5  refusal\n",
      "final_label                             \n",
      "1            1  129   6   1   0        0\n",
      "2            0   53  10   0   0        0\n",
      "3            0   12  77   4   0        0\n",
      "4            0    2  13  40   1        0\n",
      "5            0    1  25  39  26        0\n",
      "refusal      5    3   9   3  22       18\n",
      "\n",
      "############################################################\n",
      "#  gpt-3.5-turbo : templ-7.csv\n",
      "############################################################\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.50      0.01      0.03       137\n",
      "           2       0.25      0.83      0.39        63\n",
      "           3       0.45      0.85      0.59        93\n",
      "           4       0.31      0.43      0.36        56\n",
      "           5       0.35      0.13      0.19        91\n",
      "     refusal       1.00      0.10      0.18        60\n",
      "\n",
      "    accuracy                           0.35       500\n",
      "   macro avg       0.48      0.39      0.29       500\n",
      "weighted avg       0.47      0.35      0.26       500\n",
      "\n",
      "eval_label   1    2   3   4   5  refusal\n",
      "final_label                             \n",
      "1            2  129   6   0   0        0\n",
      "2            0   52  11   0   0        0\n",
      "3            0   13  79   1   0        0\n",
      "4            0    1  31  24   0        0\n",
      "5            0    1  29  49  12        0\n",
      "refusal      2    8  18   4  22        6\n",
      "\n",
      "############################################################\n",
      "#  gpt-3.5-turbo : templ-8.csv\n",
      "############################################################\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.60      0.02      0.04       137\n",
      "           2       0.25      0.83      0.39        63\n",
      "           3       0.48      0.82      0.61        93\n",
      "           4       0.37      0.57      0.45        56\n",
      "           5       0.39      0.15      0.22        91\n",
      "     refusal       1.00      0.17      0.29        60\n",
      "\n",
      "    accuracy                           0.37       500\n",
      "   macro avg       0.52      0.43      0.33       500\n",
      "weighted avg       0.52      0.37      0.30       500\n",
      "\n",
      "eval_label   1    2   3   4   5  refusal\n",
      "final_label                             \n",
      "1            3  129   5   0   0        0\n",
      "2            0   52  11   0   0        0\n",
      "3            0   15  76   2   0        0\n",
      "4            0    1  23  32   0        0\n",
      "5            0    1  27  49  14        0\n",
      "refusal      2    7  15   4  22       10\n",
      "\n",
      "############################################################\n",
      "#  gpt-4o-2024-05-13 : templ-1.csv\n",
      "############################################################\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.78      0.96      0.86       137\n",
      "           2       0.74      0.41      0.53        63\n",
      "           3       0.78      0.74      0.76        93\n",
      "           4       0.62      0.41      0.49        56\n",
      "           5       0.72      0.95      0.82        91\n",
      "     refusal       0.98      0.85      0.91        60\n",
      "\n",
      "    accuracy                           0.77       500\n",
      "   macro avg       0.77      0.72      0.73       500\n",
      "weighted avg       0.77      0.77      0.76       500\n",
      "\n",
      "eval_label     1   2   3   4   5  refusal\n",
      "final_label                              \n",
      "1            132   4   0   0   0        1\n",
      "2             27  26   8   1   1        0\n",
      "3              7   5  69   9   3        0\n",
      "4              0   0  10  23  23        0\n",
      "5              0   0   1   4  86        0\n",
      "refusal        3   0   0   0   6       51\n",
      "\n",
      "############################################################\n",
      "#  gpt-4o-2024-05-13 : templ-2.csv\n",
      "############################################################\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.83      0.93      0.88       137\n",
      "           2       0.77      0.43      0.55        63\n",
      "           3       0.70      0.87      0.78        93\n",
      "           4       0.77      0.30      0.44        56\n",
      "           5       0.70      0.96      0.81        91\n",
      "     refusal       0.98      0.80      0.88        60\n",
      "\n",
      "    accuracy                           0.78       500\n",
      "   macro avg       0.79      0.72      0.72       500\n",
      "weighted avg       0.79      0.78      0.76       500\n",
      "\n",
      "eval_label     1   2   3   4   5  refusal\n",
      "final_label                              \n",
      "1            128   3   5   1   0        0\n",
      "2             21  27  15   0   0        0\n",
      "3              1   4  81   3   3        1\n",
      "4              0   0  11  17  28        0\n",
      "5              0   0   3   1  87        0\n",
      "refusal        4   1   1   0   6       48\n",
      "\n",
      "############################################################\n",
      "#  gpt-4o-2024-05-13 : templ-3.csv\n",
      "############################################################\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.75      0.72      0.73       137\n",
      "           2       0.73      0.30      0.43        63\n",
      "           3       0.59      0.83      0.69        93\n",
      "           4       0.70      0.34      0.46        56\n",
      "           5       0.71      0.67      0.69        91\n",
      "     refusal       0.57      0.95      0.71        60\n",
      "\n",
      "    accuracy                           0.66       500\n",
      "   macro avg       0.68      0.63      0.62       500\n",
      "weighted avg       0.68      0.66      0.64       500\n",
      "\n",
      "eval_label    1   2   3   4   5  refusal\n",
      "final_label                             \n",
      "1            98   4   9   1   7       18\n",
      "2            25  19  16   1   0        2\n",
      "3             3   2  77   1   1        9\n",
      "4             1   0  20  19  15        1\n",
      "5             4   0   8   5  61       13\n",
      "refusal       0   1   0   0   2       57\n",
      "\n",
      "############################################################\n",
      "#  gpt-4o-2024-05-13 : templ-4.csv\n",
      "############################################################\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.84      0.52      0.64       137\n",
      "           2       0.66      0.40      0.50        63\n",
      "           3       0.47      0.91      0.62        93\n",
      "           4       0.64      0.52      0.57        56\n",
      "           5       0.71      0.63      0.67        91\n",
      "     refusal       0.68      0.82      0.74        60\n",
      "\n",
      "    accuracy                           0.63       500\n",
      "   macro avg       0.67      0.63      0.62       500\n",
      "weighted avg       0.68      0.63      0.63       500\n",
      "\n",
      "eval_label    1   2   3   4   5  refusal\n",
      "final_label                             \n",
      "1            71   9  44   4   2        7\n",
      "2            13  25  23   1   1        0\n",
      "3             0   2  85   2   1        3\n",
      "4             0   0  14  29  11        2\n",
      "5             1   1  12   9  57       11\n",
      "refusal       0   1   2   0   8       49\n",
      "\n",
      "############################################################\n",
      "#  gpt-4o-2024-05-13 : templ-5.csv\n",
      "############################################################\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.81      0.98      0.89       137\n",
      "           2       0.78      0.44      0.57        63\n",
      "           3       0.83      0.69      0.75        93\n",
      "           4       0.51      0.32      0.40        56\n",
      "           5       0.72      0.98      0.83        91\n",
      "     refusal       0.95      1.00      0.98        60\n",
      "\n",
      "    accuracy                           0.79       500\n",
      "   macro avg       0.77      0.74      0.73       500\n",
      "weighted avg       0.78      0.79      0.77       500\n",
      "\n",
      "eval_label     1   2   3   4   5  refusal\n",
      "final_label                              \n",
      "1            134   2   1   0   0        0\n",
      "2             26  28   7   1   0        1\n",
      "3              5   6  64  15   1        2\n",
      "4              0   0   4  18  34        0\n",
      "5              0   0   1   1  89        0\n",
      "refusal        0   0   0   0   0       60\n",
      "\n",
      "############################################################\n",
      "#  gpt-4o-2024-05-13 : templ-6.csv\n",
      "############################################################\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.78      0.98      0.87       137\n",
      "           2       0.78      0.33      0.47        63\n",
      "           3       0.80      0.70      0.75        93\n",
      "           4       0.53      0.32      0.40        56\n",
      "           5       0.71      0.97      0.82        91\n",
      "     refusal       0.95      1.00      0.98        60\n",
      "\n",
      "    accuracy                           0.77       500\n",
      "   macro avg       0.76      0.72      0.71       500\n",
      "weighted avg       0.76      0.77      0.75       500\n",
      "\n",
      "eval_label     1   2   3   4   5  refusal\n",
      "final_label                              \n",
      "1            134   1   1   0   0        1\n",
      "2             31  21  10   1   0        0\n",
      "3              6   5  65  13   2        2\n",
      "4              0   0   4  18  34        0\n",
      "5              0   0   1   2  88        0\n",
      "refusal        0   0   0   0   0       60\n",
      "\n",
      "############################################################\n",
      "#  gpt-4o-2024-05-13 : templ-7.csv\n",
      "############################################################\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.83      0.94      0.88       137\n",
      "           2       0.71      0.46      0.56        63\n",
      "           3       0.79      0.74      0.77        93\n",
      "           4       0.54      0.36      0.43        56\n",
      "           5       0.73      0.97      0.83        91\n",
      "     refusal       1.00      0.98      0.99        60\n",
      "\n",
      "    accuracy                           0.79       500\n",
      "   macro avg       0.77      0.74      0.74       500\n",
      "weighted avg       0.78      0.79      0.77       500\n",
      "\n",
      "eval_label     1   2   3   4   5  refusal\n",
      "final_label                              \n",
      "1            129   6   2   0   0        0\n",
      "2             24  29   8   1   1        0\n",
      "3              3   6  69  14   1        0\n",
      "4              0   0   7  20  29        0\n",
      "5              0   0   1   2  88        0\n",
      "refusal        0   0   0   0   1       59\n",
      "\n",
      "############################################################\n",
      "#  gpt-4o-2024-05-13 : templ-8.csv\n",
      "############################################################\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.82      0.95      0.88       137\n",
      "           2       0.72      0.49      0.58        63\n",
      "           3       0.82      0.70      0.76        93\n",
      "           4       0.56      0.43      0.48        56\n",
      "           5       0.74      0.96      0.84        91\n",
      "     refusal       0.98      0.98      0.98        60\n",
      "\n",
      "    accuracy                           0.79       500\n",
      "   macro avg       0.78      0.75      0.75       500\n",
      "weighted avg       0.79      0.79      0.78       500\n",
      "\n",
      "eval_label     1   2   3   4   5  refusal\n",
      "final_label                              \n",
      "1            130   5   1   0   0        1\n",
      "2             23  31   7   1   1        0\n",
      "3              5   7  65  15   1        0\n",
      "4              0   0   5  24  27        0\n",
      "5              0   0   1   3  87        0\n",
      "refusal        0   0   0   0   1       59\n",
      "\n",
      "############################################################\n",
      "#  gpt-4o-2024-08-06 : templ-1.csv\n",
      "############################################################\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.79      0.98      0.88       137\n",
      "           2       0.76      0.41      0.54        63\n",
      "           3       0.78      0.75      0.77        93\n",
      "           4       0.55      0.21      0.31        56\n",
      "           5       0.66      0.98      0.79        91\n",
      "     refusal       1.00      0.85      0.92        60\n",
      "\n",
      "    accuracy                           0.76       500\n",
      "   macro avg       0.76      0.70      0.70       500\n",
      "weighted avg       0.76      0.76      0.74       500\n",
      "\n",
      "eval_label     1   2   3   4   5  refusal\n",
      "final_label                              \n",
      "1            134   2   1   0   0        0\n",
      "2             28  26   8   1   0        0\n",
      "3              4   6  70   8   5        0\n",
      "4              0   0  10  12  34        0\n",
      "5              0   0   1   1  89        0\n",
      "refusal        3   0   0   0   6       51\n",
      "\n",
      "############################################################\n",
      "#  gpt-4o-2024-08-06 : templ-2.csv\n",
      "############################################################\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.81      0.95      0.88       137\n",
      "           2       0.67      0.41      0.51        63\n",
      "           3       0.74      0.83      0.78        93\n",
      "           4       0.72      0.23      0.35        56\n",
      "           5       0.64      0.95      0.76        91\n",
      "     refusal       0.98      0.73      0.84        60\n",
      "\n",
      "    accuracy                           0.75       500\n",
      "   macro avg       0.76      0.68      0.69       500\n",
      "weighted avg       0.76      0.75      0.73       500\n",
      "\n",
      "eval_label     1   2   3   4   5  refusal\n",
      "final_label                              \n",
      "1            130   5   2   0   0        0\n",
      "2             24  26  12   1   0        0\n",
      "3              2   6  77   3   4        1\n",
      "4              0   0   9  13  34        0\n",
      "5              0   0   4   1  86        0\n",
      "refusal        4   2   0   0  10       44\n",
      "\n",
      "############################################################\n",
      "#  gpt-4o-2024-08-06 : templ-3.csv\n",
      "############################################################\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.73      0.77      0.75       137\n",
      "           2       0.75      0.24      0.36        63\n",
      "           3       0.56      0.85      0.68        93\n",
      "           4       0.50      0.29      0.36        56\n",
      "           5       0.68      0.65      0.66        91\n",
      "     refusal       0.71      0.90      0.79        60\n",
      "\n",
      "    accuracy                           0.66       500\n",
      "   macro avg       0.66      0.62      0.60       500\n",
      "weighted avg       0.66      0.66      0.63       500\n",
      "\n",
      "eval_label     1   2   3   4   5  refusal\n",
      "final_label                              \n",
      "1            106   1  14   5   4        7\n",
      "2             31  15  16   1   0        0\n",
      "3              2   3  79   3   1        5\n",
      "4              1   0  19  16  20        0\n",
      "5              3   1  11   7  59       10\n",
      "refusal        2   0   1   0   3       54\n",
      "\n",
      "############################################################\n",
      "#  gpt-4o-2024-08-06 : templ-4.csv\n",
      "############################################################\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.81      0.65      0.72       137\n",
      "           2       0.75      0.38      0.51        63\n",
      "           3       0.51      0.89      0.65        93\n",
      "           4       0.69      0.32      0.44        56\n",
      "           5       0.64      0.77      0.70        91\n",
      "     refusal       0.86      0.85      0.86        60\n",
      "\n",
      "    accuracy                           0.67       500\n",
      "   macro avg       0.71      0.64      0.64       500\n",
      "weighted avg       0.71      0.67      0.66       500\n",
      "\n",
      "eval_label    1   2   3   4   5  refusal\n",
      "final_label                             \n",
      "1            89   3  32   2   8        3\n",
      "2            19  24  19   0   1        0\n",
      "3             1   1  83   4   2        2\n",
      "4             0   1  14  18  22        1\n",
      "5             1   1  15   2  70        2\n",
      "refusal       0   2   1   0   6       51\n",
      "\n",
      "############################################################\n",
      "#  gpt-4o-2024-08-06 : templ-5.csv\n",
      "############################################################\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.82      0.97      0.89       137\n",
      "           2       0.66      0.43      0.52        63\n",
      "           3       0.82      0.71      0.76        93\n",
      "           4       0.53      0.30      0.39        56\n",
      "           5       0.69      0.97      0.81        91\n",
      "     refusal       0.98      0.95      0.97        60\n",
      "\n",
      "    accuracy                           0.78       500\n",
      "   macro avg       0.75      0.72      0.72       500\n",
      "weighted avg       0.76      0.78      0.76       500\n",
      "\n",
      "eval_label     1   2   3   4   5  refusal\n",
      "final_label                              \n",
      "1            133   3   1   0   0        0\n",
      "2             26  27   9   1   0        0\n",
      "3              3  11  66  11   1        1\n",
      "4              0   0   4  17  35        0\n",
      "5              0   0   0   3  88        0\n",
      "refusal        0   0   0   0   3       57\n",
      "\n",
      "############################################################\n",
      "#  gpt-4o-2024-08-06 : templ-6.csv\n",
      "############################################################\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.81      0.97      0.88       137\n",
      "           2       0.72      0.33      0.46        63\n",
      "           3       0.78      0.77      0.78        93\n",
      "           4       0.54      0.27      0.36        56\n",
      "           5       0.69      0.97      0.81        91\n",
      "     refusal       0.98      0.97      0.97        60\n",
      "\n",
      "    accuracy                           0.77       500\n",
      "   macro avg       0.75      0.71      0.71       500\n",
      "weighted avg       0.76      0.77      0.75       500\n",
      "\n",
      "eval_label     1   2   3   4   5  refusal\n",
      "final_label                              \n",
      "1            133   3   1   0   0        0\n",
      "2             29  21  13   0   0        0\n",
      "3              3   5  72  10   2        1\n",
      "4              0   0   6  15  35        0\n",
      "5              0   0   0   3  88        0\n",
      "refusal        0   0   0   0   2       58\n",
      "\n",
      "############################################################\n",
      "#  gpt-4o-2024-08-06 : templ-7.csv\n",
      "############################################################\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.82      0.94      0.87       137\n",
      "           2       0.66      0.46      0.54        63\n",
      "           3       0.76      0.74      0.75        93\n",
      "           4       0.60      0.32      0.42        56\n",
      "           5       0.72      0.96      0.82        91\n",
      "     refusal       1.00      0.95      0.97        60\n",
      "\n",
      "    accuracy                           0.78       500\n",
      "   macro avg       0.76      0.73      0.73       500\n",
      "weighted avg       0.77      0.78      0.76       500\n",
      "\n",
      "eval_label     1   2   3   4   5  refusal\n",
      "final_label                              \n",
      "1            129   6   2   0   0        0\n",
      "2             24  29  10   0   0        0\n",
      "3              4   9  69   9   2        0\n",
      "4              0   0   9  18  29        0\n",
      "5              0   0   1   3  87        0\n",
      "refusal        1   0   0   0   2       57\n",
      "\n",
      "############################################################\n",
      "#  gpt-4o-2024-08-06 : templ-8.csv\n",
      "############################################################\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.83      0.96      0.89       137\n",
      "           2       0.65      0.49      0.56        63\n",
      "           3       0.77      0.69      0.73        93\n",
      "           4       0.58      0.32      0.41        56\n",
      "           5       0.70      0.96      0.81        91\n",
      "     refusal       1.00      0.93      0.97        60\n",
      "\n",
      "    accuracy                           0.77       500\n",
      "   macro avg       0.75      0.72      0.73       500\n",
      "weighted avg       0.76      0.77      0.76       500\n",
      "\n",
      "eval_label     1   2   3   4   5  refusal\n",
      "final_label                              \n",
      "1            131   5   1   0   0        0\n",
      "2             22  31  10   0   0        0\n",
      "3              4  12  64  10   3        0\n",
      "4              0   0   7  18  31        0\n",
      "5              0   0   1   3  87        0\n",
      "refusal        1   0   0   0   3       56\n",
      "\n",
      "############################################################\n",
      "#  gpt-4o-mini-2024-07-18 : templ-1.csv\n",
      "############################################################\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.78      0.97      0.87       137\n",
      "           2       0.62      0.46      0.53        63\n",
      "           3       0.88      0.56      0.68        93\n",
      "           4       0.49      0.32      0.39        56\n",
      "           5       0.59      0.97      0.73        91\n",
      "     refusal       0.97      0.62      0.76        60\n",
      "\n",
      "    accuracy                           0.71       500\n",
      "   macro avg       0.72      0.65      0.66       500\n",
      "weighted avg       0.73      0.71      0.70       500\n",
      "\n",
      "eval_label     1   2   3   4   5  refusal\n",
      "final_label                              \n",
      "1            133   3   0   0   0        1\n",
      "2             26  29   6   1   1        0\n",
      "3              7  13  52  15   6        0\n",
      "4              0   0   1  18  37        0\n",
      "5              0   0   0   3  88        0\n",
      "refusal        4   2   0   0  17       37\n",
      "\n",
      "############################################################\n",
      "#  gpt-4o-mini-2024-07-18 : templ-2.csv\n",
      "############################################################\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.83      0.95      0.88       137\n",
      "           2       0.69      0.56      0.61        63\n",
      "           3       0.78      0.76      0.77        93\n",
      "           4       0.67      0.36      0.47        56\n",
      "           5       0.66      0.95      0.78        91\n",
      "     refusal       0.95      0.65      0.77        60\n",
      "\n",
      "    accuracy                           0.76       500\n",
      "   macro avg       0.76      0.70      0.71       500\n",
      "weighted avg       0.77      0.76      0.75       500\n",
      "\n",
      "eval_label     1   2   3   4   5  refusal\n",
      "final_label                              \n",
      "1            130   7   0   0   0        0\n",
      "2             16  35  11   0   1        0\n",
      "3              4   7  71   7   2        2\n",
      "4              0   1   7  20  28        0\n",
      "5              0   0   2   3  86        0\n",
      "refusal        7   1   0   0  13       39\n",
      "\n",
      "############################################################\n",
      "#  gpt-4o-mini-2024-07-18 : templ-3.csv\n",
      "############################################################\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.73      0.86      0.79       137\n",
      "           2       0.53      0.41      0.46        63\n",
      "           3       0.78      0.66      0.71        93\n",
      "           4       0.50      0.59      0.54        56\n",
      "           5       0.71      0.69      0.70        91\n",
      "     refusal       0.93      0.88      0.91        60\n",
      "\n",
      "    accuracy                           0.71       500\n",
      "   macro avg       0.70      0.68      0.69       500\n",
      "weighted avg       0.71      0.71      0.70       500\n",
      "\n",
      "eval_label     1   2   3   4   5  refusal\n",
      "final_label                              \n",
      "1            118   6   5   6   1        1\n",
      "2             28  26   7   2   0        0\n",
      "3              6  12  61  11   2        1\n",
      "4              1   0   2  33  19        1\n",
      "5              6   4   3  14  63        1\n",
      "refusal        2   1   0   0   4       53\n",
      "\n",
      "############################################################\n",
      "#  gpt-4o-mini-2024-07-18 : templ-4.csv\n",
      "############################################################\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.78      0.77      0.78       137\n",
      "           2       0.72      0.44      0.55        63\n",
      "           3       0.56      0.75      0.65        93\n",
      "           4       0.66      0.48      0.56        56\n",
      "           5       0.68      0.65      0.66        91\n",
      "     refusal       0.63      0.77      0.69        60\n",
      "\n",
      "    accuracy                           0.67       500\n",
      "   macro avg       0.67      0.64      0.65       500\n",
      "weighted avg       0.68      0.67      0.67       500\n",
      "\n",
      "eval_label     1   2   3   4   5  refusal\n",
      "final_label                              \n",
      "1            106   3  16   3   2        7\n",
      "2             18  28  17   0   0        0\n",
      "3              2   4  70   4   1       12\n",
      "4              1   0  10  27  17        1\n",
      "5              5   3  10   7  59        7\n",
      "refusal        4   1   1   0   8       46\n",
      "\n",
      "############################################################\n",
      "#  gpt-4o-mini-2024-07-18 : templ-5.csv\n",
      "############################################################\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/paul/Documents/Repos/realpoliticalprompts/env/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/paul/Documents/Repos/realpoliticalprompts/env/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/paul/Documents/Repos/realpoliticalprompts/env/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/paul/Documents/Repos/realpoliticalprompts/env/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/paul/Documents/Repos/realpoliticalprompts/env/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/paul/Documents/Repos/realpoliticalprompts/env/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.85      0.88      0.86       137\n",
      "           2       0.53      0.67      0.59        63\n",
      "           3       0.91      0.46      0.61        93\n",
      "           4       0.48      0.62      0.54        56\n",
      "           5       0.74      0.85      0.79        91\n",
      "     refusal       0.98      0.92      0.95        60\n",
      "\n",
      "    accuracy                           0.74       500\n",
      "   macro avg       0.75      0.73      0.72       500\n",
      "weighted avg       0.78      0.74      0.74       500\n",
      "\n",
      "eval_label     1   2   3   4   5  refusal\n",
      "final_label                              \n",
      "1            120  16   0   1   0        0\n",
      "2             15  42   4   1   1        0\n",
      "3              5  21  43  21   2        1\n",
      "4              0   0   0  35  21        0\n",
      "5              0   0   0  14  77        0\n",
      "refusal        1   0   0   1   3       55\n",
      "\n",
      "############################################################\n",
      "#  gpt-4o-mini-2024-07-18 : templ-6.csv\n",
      "############################################################\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.83      0.91      0.86       137\n",
      "           2       0.51      0.52      0.52        63\n",
      "           3       0.87      0.49      0.63        93\n",
      "           4       0.43      0.36      0.39        56\n",
      "           5       0.66      0.93      0.77        91\n",
      "     refusal       0.98      0.92      0.95        60\n",
      "\n",
      "    accuracy                           0.73       500\n",
      "   macro avg       0.71      0.69      0.69       500\n",
      "weighted avg       0.74      0.73      0.72       500\n",
      "\n",
      "eval_label     1   2   3   4   5  refusal\n",
      "final_label                              \n",
      "1            124  12   0   1   0        0\n",
      "2             21  33   7   1   1        0\n",
      "3              3  20  46  19   4        1\n",
      "4              0   0   0  20  36        0\n",
      "5              0   0   0   6  85        0\n",
      "refusal        2   0   0   0   3       55\n",
      "\n",
      "############################################################\n",
      "#  gpt-4o-mini-2024-07-18 : templ-7.csv\n",
      "############################################################\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.87      0.86      0.87       137\n",
      "           2       0.49      0.68      0.57        63\n",
      "           3       0.83      0.42      0.56        93\n",
      "           4       0.49      0.70      0.57        56\n",
      "           5       0.80      0.86      0.83        91\n",
      "     refusal       0.98      0.87      0.92        60\n",
      "\n",
      "    accuracy                           0.74       500\n",
      "   macro avg       0.74      0.73      0.72       500\n",
      "weighted avg       0.77      0.74      0.74       500\n",
      "\n",
      "eval_label     1   2   3   4   5  refusal\n",
      "final_label                              \n",
      "1            118  19   0   0   0        0\n",
      "2             12  43   6   2   0        0\n",
      "3              4  24  39  24   1        1\n",
      "4              0   0   2  39  15        0\n",
      "5              0   0   0  13  78        0\n",
      "refusal        1   2   0   2   3       52\n",
      "\n",
      "############################################################\n",
      "#  gpt-4o-mini-2024-07-18 : templ-8.csv\n",
      "############################################################\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.89      0.87      0.88       137\n",
      "           2       0.48      0.71      0.57        63\n",
      "           3       0.85      0.38      0.52        93\n",
      "           4       0.47      0.70      0.56        56\n",
      "           5       0.80      0.82      0.81        91\n",
      "     refusal       0.98      0.88      0.93        60\n",
      "\n",
      "    accuracy                           0.73       500\n",
      "   macro avg       0.74      0.73      0.71       500\n",
      "weighted avg       0.78      0.73      0.73       500\n",
      "\n",
      "eval_label     1   2   3   4   5  refusal\n",
      "final_label                              \n",
      "1            119  18   0   0   0        0\n",
      "2             11  45   5   2   0        0\n",
      "3              3  29  35  25   0        1\n",
      "4              0   0   1  39  16        0\n",
      "5              0   0   0  16  75        0\n",
      "refusal        1   2   0   1   3       53\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create classification report and confusion matrix\n",
    "\n",
    "for model in df_dict:\n",
    "\n",
    "    for templ in sorted(df_dict[model]):\n",
    "\n",
    "        print(\"###\"*20)\n",
    "        print(f'#  {model} : {templ}')\n",
    "        print(\"###\"*20, end='\\n\\n')\n",
    "        print(classification_report(df_dict[model][templ]['final_label'], df_dict[model][templ]['eval_label']))\n",
    "        print(pd.crosstab(df_dict[model][templ]['final_label'], df_dict[model][templ]['eval_label']))\n",
    "        print()\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############################################################\n",
      "#  Llama-3.1-70B-Instruct : OVERVIEW\n",
      "############################################################\n",
      "\n",
      "ORIGINAL LABELS: 1, 2, 3, 4, 5, refusal\n",
      "\n",
      "             accuracy  macro_f1  weighted_f1\n",
      "templ-1.csv     0.756    0.7351       0.7604\n",
      "templ-2.csv     0.754    0.7390       0.7625\n",
      "templ-3.csv     0.654    0.6636       0.6704\n",
      "templ-4.csv     0.604    0.6180       0.6046\n",
      "templ-5.csv     0.774    0.7707       0.7821\n",
      "templ-6.csv     0.776    0.7617       0.7786\n",
      "templ-7.csv     0.756    0.7565       0.7673\n",
      "templ-8.csv     0.766    0.7660       0.7762\n",
      "\n",
      "############################################################\n",
      "#  Llama-3.1-8B-Instruct : OVERVIEW\n",
      "############################################################\n",
      "\n",
      "ORIGINAL LABELS: 1, 2, 3, 4, 5, refusal\n",
      "\n",
      "             accuracy  macro_f1  weighted_f1\n",
      "templ-1.csv     0.442    0.3926       0.4051\n",
      "templ-2.csv     0.478    0.4837       0.4668\n",
      "templ-3.csv     0.354    0.3038       0.2775\n",
      "templ-4.csv     0.476    0.4922       0.4567\n",
      "templ-5.csv     0.516    0.5510       0.5174\n",
      "templ-6.csv     0.514    0.5462       0.5101\n",
      "templ-7.csv     0.448    0.4815       0.4401\n",
      "templ-8.csv     0.410    0.4341       0.3867\n",
      "\n",
      "############################################################\n",
      "#  Llama-3.2-3B-Instruct : OVERVIEW\n",
      "############################################################\n",
      "\n",
      "ORIGINAL LABELS: 1, 2, 3, 4, 5, refusal\n",
      "\n",
      "             accuracy  macro_f1  weighted_f1\n",
      "templ-1.csv     0.464    0.3566       0.4486\n",
      "templ-2.csv     0.308    0.2223       0.2363\n",
      "templ-3.csv     0.474    0.4416       0.4439\n",
      "templ-4.csv     0.326    0.2442       0.2347\n",
      "templ-5.csv     0.366    0.3192       0.3301\n",
      "templ-6.csv     0.484    0.4056       0.4783\n",
      "templ-7.csv     0.334    0.2895       0.2863\n",
      "templ-8.csv     0.328    0.2704       0.2665\n",
      "\n",
      "############################################################\n",
      "#  Ministral-8B-Instruct-2410 : OVERVIEW\n",
      "############################################################\n",
      "\n",
      "ORIGINAL LABELS: 1, 2, 3, 4, 5, refusal\n",
      "\n",
      "             accuracy  macro_f1  weighted_f1\n",
      "templ-1.csv     0.604    0.5660       0.6018\n",
      "templ-2.csv     0.602    0.5569       0.5930\n",
      "templ-3.csv     0.418    0.4012       0.3891\n",
      "templ-4.csv     0.362    0.3233       0.3001\n",
      "templ-5.csv     0.624    0.5059       0.6025\n",
      "templ-6.csv     0.682    0.6528       0.6753\n",
      "templ-7.csv     0.560    0.4690       0.5481\n",
      "templ-8.csv     0.544    0.4474       0.5201\n",
      "\n",
      "############################################################\n",
      "#  Mistral-7B-Instruct-v0.3 : OVERVIEW\n",
      "############################################################\n",
      "\n",
      "ORIGINAL LABELS: 1, 2, 3, 4, 5, refusal\n",
      "\n",
      "             accuracy  macro_f1  weighted_f1\n",
      "templ-1.csv     0.664    0.5976       0.6399\n",
      "templ-2.csv     0.656    0.6223       0.6506\n",
      "templ-3.csv     0.632    0.6030       0.6252\n",
      "templ-4.csv     0.452    0.4372       0.3889\n",
      "templ-5.csv     0.738    0.7060       0.7306\n",
      "templ-6.csv     0.666    0.6354       0.6642\n",
      "templ-7.csv     0.702    0.6846       0.7066\n",
      "templ-8.csv     0.666    0.6484       0.6705\n",
      "\n",
      "############################################################\n",
      "#  Mistral-Nemo-Instruct-2407 : OVERVIEW\n",
      "############################################################\n",
      "\n",
      "ORIGINAL LABELS: 1, 2, 3, 4, 5, refusal\n",
      "\n",
      "             accuracy  macro_f1  weighted_f1\n",
      "templ-1.csv     0.684    0.6097       0.6580\n",
      "templ-2.csv     0.672    0.6134       0.6557\n",
      "templ-3.csv     0.500    0.4849       0.4909\n",
      "templ-4.csv     0.564    0.5499       0.5638\n",
      "templ-5.csv     0.630    0.6273       0.6366\n",
      "templ-6.csv     0.644    0.6391       0.6517\n",
      "templ-7.csv     0.548    0.5495       0.5493\n",
      "templ-8.csv     0.602    0.6122       0.6126\n",
      "\n",
      "############################################################\n",
      "#  gemma-2-27b-it : OVERVIEW\n",
      "############################################################\n",
      "\n",
      "ORIGINAL LABELS: 1, 2, 3, 4, 5, refusal\n",
      "\n",
      "             accuracy  macro_f1  weighted_f1\n",
      "templ-1.csv     0.616    0.5852       0.6175\n",
      "templ-2.csv     0.702    0.6794       0.7081\n",
      "templ-3.csv     0.564    0.5714       0.5747\n",
      "templ-4.csv     0.498    0.4993       0.4603\n",
      "templ-5.csv     0.666    0.6771       0.6834\n",
      "templ-6.csv     0.672    0.6894       0.6904\n",
      "templ-7.csv     0.600    0.6182       0.6170\n",
      "templ-8.csv     0.532    0.5504       0.5419\n",
      "\n",
      "############################################################\n",
      "#  gemma-2-9b-it : OVERVIEW\n",
      "############################################################\n",
      "\n",
      "ORIGINAL LABELS: 1, 2, 3, 4, 5, refusal\n",
      "\n",
      "             accuracy  macro_f1  weighted_f1\n",
      "templ-1.csv     0.614    0.5747       0.6092\n",
      "templ-2.csv     0.672    0.6622       0.6850\n",
      "templ-3.csv     0.524    0.5212       0.5270\n",
      "templ-4.csv     0.606    0.6147       0.6041\n",
      "templ-5.csv     0.538    0.5554       0.5493\n",
      "templ-6.csv     0.558    0.5791       0.5702\n",
      "templ-7.csv     0.504    0.5186       0.5090\n",
      "templ-8.csv     0.510    0.5175       0.5163\n",
      "\n",
      "############################################################\n",
      "#  gpt-3.5-turbo : OVERVIEW\n",
      "############################################################\n",
      "\n",
      "ORIGINAL LABELS: 1, 2, 3, 4, 5, refusal\n",
      "\n",
      "             accuracy  macro_f1  weighted_f1\n",
      "templ-1.csv     0.462    0.4090       0.4026\n",
      "templ-2.csv     0.464    0.4601       0.4256\n",
      "templ-3.csv     0.314    0.2385       0.2540\n",
      "templ-4.csv     0.360    0.2914       0.2882\n",
      "templ-5.csv     0.418    0.3376       0.3480\n",
      "templ-6.csv     0.426    0.3471       0.3583\n",
      "templ-7.csv     0.350    0.2903       0.2638\n",
      "templ-8.csv     0.374    0.3320       0.2981\n",
      "\n",
      "############################################################\n",
      "#  gpt-4o-2024-05-13 : OVERVIEW\n",
      "############################################################\n",
      "\n",
      "ORIGINAL LABELS: 1, 2, 3, 4, 5, refusal\n",
      "\n",
      "             accuracy  macro_f1  weighted_f1\n",
      "templ-1.csv     0.774    0.7300       0.7588\n",
      "templ-2.csv     0.776    0.7220       0.7564\n",
      "templ-3.csv     0.662    0.6181       0.6449\n",
      "templ-4.csv     0.632    0.6235       0.6282\n",
      "templ-5.csv     0.786    0.7342       0.7665\n",
      "templ-6.csv     0.772    0.7130       0.7470\n",
      "templ-7.csv     0.788    0.7435       0.7731\n",
      "templ-8.csv     0.792    0.7545       0.7803\n",
      "\n",
      "############################################################\n",
      "#  gpt-4o-2024-08-06 : OVERVIEW\n",
      "############################################################\n",
      "\n",
      "ORIGINAL LABELS: 1, 2, 3, 4, 5, refusal\n",
      "\n",
      "             accuracy  macro_f1  weighted_f1\n",
      "templ-1.csv     0.764    0.6991       0.7385\n",
      "templ-2.csv     0.752    0.6868       0.7286\n",
      "templ-3.csv     0.658    0.6020       0.6343\n",
      "templ-4.csv     0.670    0.6447       0.6607\n",
      "templ-5.csv     0.776    0.7219       0.7572\n",
      "templ-6.csv     0.774    0.7092       0.7475\n",
      "templ-7.csv     0.778    0.7307       0.7613\n",
      "templ-8.csv     0.774    0.7271       0.7585\n",
      "\n",
      "############################################################\n",
      "#  gpt-4o-mini-2024-07-18 : OVERVIEW\n",
      "############################################################\n",
      "\n",
      "ORIGINAL LABELS: 1, 2, 3, 4, 5, refusal\n",
      "\n",
      "             accuracy  macro_f1  weighted_f1\n",
      "templ-1.csv     0.714    0.6589       0.6985\n",
      "templ-2.csv     0.762    0.7143       0.7496\n",
      "templ-3.csv     0.708    0.6861       0.7049\n",
      "templ-4.csv     0.672    0.6470       0.6680\n",
      "templ-5.csv     0.744    0.7250       0.7436\n",
      "templ-6.csv     0.726    0.6865       0.7169\n",
      "templ-7.csv     0.738    0.7197       0.7388\n",
      "templ-8.csv     0.732    0.7126       0.7320\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for model in df_dict:\n",
    "    \n",
    "    # create overview df for each model: each row is a template, columns are accuracy and macro f1 score and weighted f1 score\n",
    "\n",
    "    overview_df = pd.DataFrame(columns=['accuracy', 'macro_f1', 'weighted_f1'])\n",
    "\n",
    "    for templ in sorted(df_dict[model]):\n",
    "        accuracy = accuracy_score(df_dict[model][templ]['final_label'], df_dict[model][templ]['eval_label'])\n",
    "        macro_f1 = round(f1_score(df_dict[model][templ]['final_label'], df_dict[model][templ]['eval_label'], average='macro'),4)\n",
    "        weighted_f1 = round(f1_score(df_dict[model][templ]['final_label'], df_dict[model][templ]['eval_label'], average='weighted'),4)\n",
    "\n",
    "        overview_df.loc[templ] = [accuracy, macro_f1, weighted_f1]\n",
    "\n",
    "    print(\"###\"*20)\n",
    "    print(f'#  {model} : OVERVIEW')\n",
    "    print(\"###\"*20, end='\\n\\n')\n",
    "    print(\"ORIGINAL LABELS: 1, 2, 3, 4, 5, refusal\\n\")\n",
    "    print(overview_df, end='\\n\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############################################################\n",
      "#  FULL OVERVIEW\n",
      "############################################################\n",
      "\n",
      "ORIGINAL LABELS: 1, 2, 3, 4, 5, refusal\n",
      "\n",
      "METRIC: MACRO F1 SCORE\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_7ef36\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_7ef36_level0_col0\" class=\"col_heading level0 col0\" >templ-1.csv</th>\n",
       "      <th id=\"T_7ef36_level0_col1\" class=\"col_heading level0 col1\" >templ-2.csv</th>\n",
       "      <th id=\"T_7ef36_level0_col2\" class=\"col_heading level0 col2\" >templ-3.csv</th>\n",
       "      <th id=\"T_7ef36_level0_col3\" class=\"col_heading level0 col3\" >templ-4.csv</th>\n",
       "      <th id=\"T_7ef36_level0_col4\" class=\"col_heading level0 col4\" >templ-5.csv</th>\n",
       "      <th id=\"T_7ef36_level0_col5\" class=\"col_heading level0 col5\" >templ-6.csv</th>\n",
       "      <th id=\"T_7ef36_level0_col6\" class=\"col_heading level0 col6\" >templ-7.csv</th>\n",
       "      <th id=\"T_7ef36_level0_col7\" class=\"col_heading level0 col7\" >templ-8.csv</th>\n",
       "      <th id=\"T_7ef36_level0_col8\" class=\"col_heading level0 col8\" >AVERAGE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_7ef36_level0_row0\" class=\"row_heading level0 row0\" >Llama-3.1-70B-Instruct</th>\n",
       "      <td id=\"T_7ef36_row0_col0\" class=\"data row0 col0\" >0.74</td>\n",
       "      <td id=\"T_7ef36_row0_col1\" class=\"data row0 col1\" >0.74</td>\n",
       "      <td id=\"T_7ef36_row0_col2\" class=\"data row0 col2\" >0.66</td>\n",
       "      <td id=\"T_7ef36_row0_col3\" class=\"data row0 col3\" >0.62</td>\n",
       "      <td id=\"T_7ef36_row0_col4\" class=\"data row0 col4\" >0.77</td>\n",
       "      <td id=\"T_7ef36_row0_col5\" class=\"data row0 col5\" >0.76</td>\n",
       "      <td id=\"T_7ef36_row0_col6\" class=\"data row0 col6\" >0.76</td>\n",
       "      <td id=\"T_7ef36_row0_col7\" class=\"data row0 col7\" >0.77</td>\n",
       "      <td id=\"T_7ef36_row0_col8\" class=\"data row0 col8\" >0.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7ef36_level0_row1\" class=\"row_heading level0 row1\" >Qwen2.5-72B-Instruct</th>\n",
       "      <td id=\"T_7ef36_row1_col0\" class=\"data row1 col0\" >0.69</td>\n",
       "      <td id=\"T_7ef36_row1_col1\" class=\"data row1 col1\" >0.71</td>\n",
       "      <td id=\"T_7ef36_row1_col2\" class=\"data row1 col2\" >0.6</td>\n",
       "      <td id=\"T_7ef36_row1_col3\" class=\"data row1 col3\" >0.67</td>\n",
       "      <td id=\"T_7ef36_row1_col4\" class=\"data row1 col4\" >0.76</td>\n",
       "      <td id=\"T_7ef36_row1_col5\" class=\"data row1 col5\" >0.74</td>\n",
       "      <td id=\"T_7ef36_row1_col6\" class=\"data row1 col6\" >0.74</td>\n",
       "      <td id=\"T_7ef36_row1_col7\" class=\"data row1 col7\" >0.76</td>\n",
       "      <td id=\"T_7ef36_row1_col8\" class=\"data row1 col8\" >0.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7ef36_level0_row2\" class=\"row_heading level0 row2\" >gpt-4o-2024-05-13</th>\n",
       "      <td id=\"T_7ef36_row2_col0\" class=\"data row2 col0\" >0.73</td>\n",
       "      <td id=\"T_7ef36_row2_col1\" class=\"data row2 col1\" >0.72</td>\n",
       "      <td id=\"T_7ef36_row2_col2\" class=\"data row2 col2\" >0.62</td>\n",
       "      <td id=\"T_7ef36_row2_col3\" class=\"data row2 col3\" >0.62</td>\n",
       "      <td id=\"T_7ef36_row2_col4\" class=\"data row2 col4\" >0.73</td>\n",
       "      <td id=\"T_7ef36_row2_col5\" class=\"data row2 col5\" >0.71</td>\n",
       "      <td id=\"T_7ef36_row2_col6\" class=\"data row2 col6\" >0.74</td>\n",
       "      <td id=\"T_7ef36_row2_col7\" class=\"data row2 col7\" >0.75</td>\n",
       "      <td id=\"T_7ef36_row2_col8\" class=\"data row2 col8\" >0.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7ef36_level0_row3\" class=\"row_heading level0 row3\" >gpt-4o-mini-2024-07-18</th>\n",
       "      <td id=\"T_7ef36_row3_col0\" class=\"data row3 col0\" >0.66</td>\n",
       "      <td id=\"T_7ef36_row3_col1\" class=\"data row3 col1\" >0.71</td>\n",
       "      <td id=\"T_7ef36_row3_col2\" class=\"data row3 col2\" >0.69</td>\n",
       "      <td id=\"T_7ef36_row3_col3\" class=\"data row3 col3\" >0.65</td>\n",
       "      <td id=\"T_7ef36_row3_col4\" class=\"data row3 col4\" >0.72</td>\n",
       "      <td id=\"T_7ef36_row3_col5\" class=\"data row3 col5\" >0.69</td>\n",
       "      <td id=\"T_7ef36_row3_col6\" class=\"data row3 col6\" >0.72</td>\n",
       "      <td id=\"T_7ef36_row3_col7\" class=\"data row3 col7\" >0.71</td>\n",
       "      <td id=\"T_7ef36_row3_col8\" class=\"data row3 col8\" >0.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7ef36_level0_row4\" class=\"row_heading level0 row4\" >gpt-4o-2024-08-06</th>\n",
       "      <td id=\"T_7ef36_row4_col0\" class=\"data row4 col0\" >0.7</td>\n",
       "      <td id=\"T_7ef36_row4_col1\" class=\"data row4 col1\" >0.69</td>\n",
       "      <td id=\"T_7ef36_row4_col2\" class=\"data row4 col2\" >0.6</td>\n",
       "      <td id=\"T_7ef36_row4_col3\" class=\"data row4 col3\" >0.64</td>\n",
       "      <td id=\"T_7ef36_row4_col4\" class=\"data row4 col4\" >0.72</td>\n",
       "      <td id=\"T_7ef36_row4_col5\" class=\"data row4 col5\" >0.71</td>\n",
       "      <td id=\"T_7ef36_row4_col6\" class=\"data row4 col6\" >0.73</td>\n",
       "      <td id=\"T_7ef36_row4_col7\" class=\"data row4 col7\" >0.73</td>\n",
       "      <td id=\"T_7ef36_row4_col8\" class=\"data row4 col8\" >0.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7ef36_level0_row5\" class=\"row_heading level0 row5\" >Mistral-7B-Instruct-v0.3</th>\n",
       "      <td id=\"T_7ef36_row5_col0\" class=\"data row5 col0\" >0.6</td>\n",
       "      <td id=\"T_7ef36_row5_col1\" class=\"data row5 col1\" >0.62</td>\n",
       "      <td id=\"T_7ef36_row5_col2\" class=\"data row5 col2\" >0.6</td>\n",
       "      <td id=\"T_7ef36_row5_col3\" class=\"data row5 col3\" >0.44</td>\n",
       "      <td id=\"T_7ef36_row5_col4\" class=\"data row5 col4\" >0.71</td>\n",
       "      <td id=\"T_7ef36_row5_col5\" class=\"data row5 col5\" >0.64</td>\n",
       "      <td id=\"T_7ef36_row5_col6\" class=\"data row5 col6\" >0.68</td>\n",
       "      <td id=\"T_7ef36_row5_col7\" class=\"data row5 col7\" >0.65</td>\n",
       "      <td id=\"T_7ef36_row5_col8\" class=\"data row5 col8\" >0.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7ef36_level0_row6\" class=\"row_heading level0 row6\" >gemma-2-27b-it</th>\n",
       "      <td id=\"T_7ef36_row6_col0\" class=\"data row6 col0\" >0.59</td>\n",
       "      <td id=\"T_7ef36_row6_col1\" class=\"data row6 col1\" >0.68</td>\n",
       "      <td id=\"T_7ef36_row6_col2\" class=\"data row6 col2\" >0.57</td>\n",
       "      <td id=\"T_7ef36_row6_col3\" class=\"data row6 col3\" >0.5</td>\n",
       "      <td id=\"T_7ef36_row6_col4\" class=\"data row6 col4\" >0.68</td>\n",
       "      <td id=\"T_7ef36_row6_col5\" class=\"data row6 col5\" >0.69</td>\n",
       "      <td id=\"T_7ef36_row6_col6\" class=\"data row6 col6\" >0.62</td>\n",
       "      <td id=\"T_7ef36_row6_col7\" class=\"data row6 col7\" >0.55</td>\n",
       "      <td id=\"T_7ef36_row6_col8\" class=\"data row6 col8\" >0.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7ef36_level0_row7\" class=\"row_heading level0 row7\" >Mistral-Nemo-Instruct-2407</th>\n",
       "      <td id=\"T_7ef36_row7_col0\" class=\"data row7 col0\" >0.61</td>\n",
       "      <td id=\"T_7ef36_row7_col1\" class=\"data row7 col1\" >0.61</td>\n",
       "      <td id=\"T_7ef36_row7_col2\" class=\"data row7 col2\" >0.48</td>\n",
       "      <td id=\"T_7ef36_row7_col3\" class=\"data row7 col3\" >0.55</td>\n",
       "      <td id=\"T_7ef36_row7_col4\" class=\"data row7 col4\" >0.63</td>\n",
       "      <td id=\"T_7ef36_row7_col5\" class=\"data row7 col5\" >0.64</td>\n",
       "      <td id=\"T_7ef36_row7_col6\" class=\"data row7 col6\" >0.55</td>\n",
       "      <td id=\"T_7ef36_row7_col7\" class=\"data row7 col7\" >0.61</td>\n",
       "      <td id=\"T_7ef36_row7_col8\" class=\"data row7 col8\" >0.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7ef36_level0_row8\" class=\"row_heading level0 row8\" >gemma-2-9b-it</th>\n",
       "      <td id=\"T_7ef36_row8_col0\" class=\"data row8 col0\" >0.57</td>\n",
       "      <td id=\"T_7ef36_row8_col1\" class=\"data row8 col1\" >0.66</td>\n",
       "      <td id=\"T_7ef36_row8_col2\" class=\"data row8 col2\" >0.52</td>\n",
       "      <td id=\"T_7ef36_row8_col3\" class=\"data row8 col3\" >0.61</td>\n",
       "      <td id=\"T_7ef36_row8_col4\" class=\"data row8 col4\" >0.56</td>\n",
       "      <td id=\"T_7ef36_row8_col5\" class=\"data row8 col5\" >0.58</td>\n",
       "      <td id=\"T_7ef36_row8_col6\" class=\"data row8 col6\" >0.52</td>\n",
       "      <td id=\"T_7ef36_row8_col7\" class=\"data row8 col7\" >0.52</td>\n",
       "      <td id=\"T_7ef36_row8_col8\" class=\"data row8 col8\" >0.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7ef36_level0_row9\" class=\"row_heading level0 row9\" >Ministral-8B-Instruct-2410</th>\n",
       "      <td id=\"T_7ef36_row9_col0\" class=\"data row9 col0\" >0.57</td>\n",
       "      <td id=\"T_7ef36_row9_col1\" class=\"data row9 col1\" >0.56</td>\n",
       "      <td id=\"T_7ef36_row9_col2\" class=\"data row9 col2\" >0.4</td>\n",
       "      <td id=\"T_7ef36_row9_col3\" class=\"data row9 col3\" >0.32</td>\n",
       "      <td id=\"T_7ef36_row9_col4\" class=\"data row9 col4\" >0.51</td>\n",
       "      <td id=\"T_7ef36_row9_col5\" class=\"data row9 col5\" >0.65</td>\n",
       "      <td id=\"T_7ef36_row9_col6\" class=\"data row9 col6\" >0.47</td>\n",
       "      <td id=\"T_7ef36_row9_col7\" class=\"data row9 col7\" >0.45</td>\n",
       "      <td id=\"T_7ef36_row9_col8\" class=\"data row9 col8\" >0.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7ef36_level0_row10\" class=\"row_heading level0 row10\" >Llama-3.1-8B-Instruct</th>\n",
       "      <td id=\"T_7ef36_row10_col0\" class=\"data row10 col0\" >0.39</td>\n",
       "      <td id=\"T_7ef36_row10_col1\" class=\"data row10 col1\" >0.48</td>\n",
       "      <td id=\"T_7ef36_row10_col2\" class=\"data row10 col2\" >0.3</td>\n",
       "      <td id=\"T_7ef36_row10_col3\" class=\"data row10 col3\" >0.49</td>\n",
       "      <td id=\"T_7ef36_row10_col4\" class=\"data row10 col4\" >0.55</td>\n",
       "      <td id=\"T_7ef36_row10_col5\" class=\"data row10 col5\" >0.55</td>\n",
       "      <td id=\"T_7ef36_row10_col6\" class=\"data row10 col6\" >0.48</td>\n",
       "      <td id=\"T_7ef36_row10_col7\" class=\"data row10 col7\" >0.43</td>\n",
       "      <td id=\"T_7ef36_row10_col8\" class=\"data row10 col8\" >0.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7ef36_level0_row11\" class=\"row_heading level0 row11\" >gpt-3.5-turbo</th>\n",
       "      <td id=\"T_7ef36_row11_col0\" class=\"data row11 col0\" >0.41</td>\n",
       "      <td id=\"T_7ef36_row11_col1\" class=\"data row11 col1\" >0.46</td>\n",
       "      <td id=\"T_7ef36_row11_col2\" class=\"data row11 col2\" >0.28</td>\n",
       "      <td id=\"T_7ef36_row11_col3\" class=\"data row11 col3\" >0.29</td>\n",
       "      <td id=\"T_7ef36_row11_col4\" class=\"data row11 col4\" >0.4</td>\n",
       "      <td id=\"T_7ef36_row11_col5\" class=\"data row11 col5\" >0.41</td>\n",
       "      <td id=\"T_7ef36_row11_col6\" class=\"data row11 col6\" >0.29</td>\n",
       "      <td id=\"T_7ef36_row11_col7\" class=\"data row11 col7\" >0.33</td>\n",
       "      <td id=\"T_7ef36_row11_col8\" class=\"data row11 col8\" >0.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7ef36_level0_row12\" class=\"row_heading level0 row12\" >Llama-3.2-3B-Instruct</th>\n",
       "      <td id=\"T_7ef36_row12_col0\" class=\"data row12 col0\" >0.36</td>\n",
       "      <td id=\"T_7ef36_row12_col1\" class=\"data row12 col1\" >0.22</td>\n",
       "      <td id=\"T_7ef36_row12_col2\" class=\"data row12 col2\" >0.44</td>\n",
       "      <td id=\"T_7ef36_row12_col3\" class=\"data row12 col3\" >0.24</td>\n",
       "      <td id=\"T_7ef36_row12_col4\" class=\"data row12 col4\" >0.32</td>\n",
       "      <td id=\"T_7ef36_row12_col5\" class=\"data row12 col5\" >0.41</td>\n",
       "      <td id=\"T_7ef36_row12_col6\" class=\"data row12 col6\" >0.29</td>\n",
       "      <td id=\"T_7ef36_row12_col7\" class=\"data row12 col7\" >0.27</td>\n",
       "      <td id=\"T_7ef36_row12_col8\" class=\"data row12 col8\" >0.32</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x30c332b10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Llama-3.1-70B-Instruct & 0.74 & 0.74 & 0.66 & 0.62 & 0.77 & 0.76 & 0.76 & 0.77 & 0.73 \\\\ \n",
      "Qwen2.5-72B-Instruct & 0.69 & 0.71 & 0.60 & 0.67 & 0.76 & 0.74 & 0.74 & 0.76 & 0.71 \\\\ \n",
      "gpt-4o-2024-05-13 & 0.73 & 0.72 & 0.62 & 0.62 & 0.73 & 0.71 & 0.74 & 0.75 & 0.70 \\\\ \n",
      "gpt-4o-mini-2024-07-18 & 0.66 & 0.71 & 0.69 & 0.65 & 0.72 & 0.69 & 0.72 & 0.71 & 0.69 \\\\ \n",
      "gpt-4o-2024-08-06 & 0.70 & 0.69 & 0.60 & 0.64 & 0.72 & 0.71 & 0.73 & 0.73 & 0.69 \\\\ \n",
      "Mistral-7B-Instruct-v0.3 & 0.60 & 0.62 & 0.60 & 0.44 & 0.71 & 0.64 & 0.68 & 0.65 & 0.62 \\\\ \n",
      "gemma-2-27b-it & 0.59 & 0.68 & 0.57 & 0.50 & 0.68 & 0.69 & 0.62 & 0.55 & 0.61 \\\\ \n",
      "Mistral-Nemo-Instruct-2407 & 0.61 & 0.61 & 0.48 & 0.55 & 0.63 & 0.64 & 0.55 & 0.61 & 0.59 \\\\ \n",
      "gemma-2-9b-it & 0.57 & 0.66 & 0.52 & 0.61 & 0.56 & 0.58 & 0.52 & 0.52 & 0.57 \\\\ \n",
      "Ministral-8B-Instruct-2410 & 0.57 & 0.56 & 0.40 & 0.32 & 0.51 & 0.65 & 0.47 & 0.45 & 0.49 \\\\ \n",
      "Llama-3.1-8B-Instruct & 0.39 & 0.48 & 0.30 & 0.49 & 0.55 & 0.55 & 0.48 & 0.43 & 0.46 \\\\ \n",
      "gpt-3.5-turbo & 0.41 & 0.46 & 0.28 & 0.29 & 0.40 & 0.41 & 0.29 & 0.33 & 0.36 \\\\ \n",
      "Llama-3.2-3B-Instruct & 0.36 & 0.22 & 0.44 & 0.24 & 0.32 & 0.41 & 0.29 & 0.27 & 0.32 \\\\ \n"
     ]
    }
   ],
   "source": [
    "\n",
    "# create full overview: each row is a template, each column is a model, values are macro f1 score\n",
    "overview_full_df = pd.DataFrame(columns=sorted(df_dict.keys()))\n",
    "\n",
    "for model in df_dict:\n",
    "    for templ in sorted(df_dict[model]):\n",
    "        macro_f1 = round(f1_score(df_dict[model][templ]['final_label'], df_dict[model][templ]['eval_label'], average='macro'),4)\n",
    "        overview_full_df.loc[templ, model] = macro_f1\n",
    "\n",
    "# transpose\n",
    "overview_full_df = overview_full_df.T\n",
    "\n",
    "# add column that is the average across all templates\n",
    "overview_full_df['AVERAGE'] = overview_full_df.mean(axis=1)\n",
    "overview_full_df = overview_full_df.sort_values(by='AVERAGE', ascending=False)\n",
    "\n",
    "print(\"###\"*20)\n",
    "print(f'#  FULL OVERVIEW')\n",
    "print(\"###\"*20, end='\\n\\n')\n",
    "print(\"ORIGINAL LABELS: 1, 2, 3, 4, 5, refusal\\n\")\n",
    "print(\"METRIC: MACRO F1 SCORE\\n\")\n",
    "display(overview_full_df.style.format(\"{:.2}\"))\n",
    "\n",
    "# iterate through rows of df, and print each row for the latex table. the row should end with \"\\\\\" and not \"&\"\n",
    "# for every second row, start with \"\\rowcolor[HTML]{F0F0F0} \"\n",
    "for model in overview_full_df.index:\n",
    "    print(f'{model} & ', end='')\n",
    "    for val in overview_full_df.loc[model]:\n",
    "        if val == overview_full_df.loc[model, 'AVERAGE']:\n",
    "            print(f'{val:.2f}', end=' \\\\\\\\ \\n')\n",
    "        else:\n",
    "            print(f'{val:.2f} & ', end='')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############################################################\n",
      "#  Llama-3.1-70B-Instruct : templ-1.csv\n",
      "############################################################\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         con       0.84      0.87      0.86       147\n",
      "     neutral       0.70      0.67      0.68        93\n",
      "         pro       0.92      0.94      0.93       200\n",
      "     refusal       1.00      0.88      0.94        60\n",
      "\n",
      "    accuracy                           0.86       500\n",
      "   macro avg       0.86      0.84      0.85       500\n",
      "weighted avg       0.86      0.86      0.86       500\n",
      "\n",
      "eval_label_collapsed   con  neutral  pro  refusal\n",
      "final_label_collapsed                            \n",
      "con                    128       19    0        0\n",
      "neutral                 18       62   13        0\n",
      "pro                      3        8  189        0\n",
      "refusal                  3        0    4       53\n",
      "\n",
      "############################################################\n",
      "#  Llama-3.1-70B-Instruct : templ-2.csv\n",
      "############################################################\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         con       0.93      0.80      0.86       147\n",
      "     neutral       0.68      0.78      0.73        93\n",
      "         pro       0.88      0.94      0.91       200\n",
      "     refusal       1.00      0.83      0.91        60\n",
      "\n",
      "    accuracy                           0.86       500\n",
      "   macro avg       0.87      0.84      0.85       500\n",
      "weighted avg       0.87      0.86      0.86       500\n",
      "\n",
      "eval_label_collapsed   con  neutral  pro  refusal\n",
      "final_label_collapsed                            \n",
      "con                    117       22    8        0\n",
      "neutral                  4       73   16        0\n",
      "pro                      0       11  189        0\n",
      "refusal                  5        2    3       50\n",
      "\n",
      "############################################################\n",
      "#  Llama-3.1-70B-Instruct : templ-3.csv\n",
      "############################################################\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         con       0.78      0.82      0.80       147\n",
      "     neutral       0.66      0.76      0.71        93\n",
      "         pro       0.89      0.81      0.85       200\n",
      "     refusal       1.00      0.88      0.94        60\n",
      "\n",
      "    accuracy                           0.81       500\n",
      "   macro avg       0.83      0.82      0.82       500\n",
      "weighted avg       0.82      0.81      0.82       500\n",
      "\n",
      "eval_label_collapsed   con  neutral  pro  refusal\n",
      "final_label_collapsed                            \n",
      "con                    121       20    6        0\n",
      "neutral                 10       71   12        0\n",
      "pro                     22       16  162        0\n",
      "refusal                  3        1    3       53\n",
      "\n",
      "############################################################\n",
      "#  Llama-3.1-70B-Instruct : templ-4.csv\n",
      "############################################################\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         con       0.85      0.69      0.76       147\n",
      "     neutral       0.51      0.87      0.64        93\n",
      "         pro       0.89      0.78      0.83       200\n",
      "     refusal       1.00      0.75      0.86        60\n",
      "\n",
      "    accuracy                           0.77       500\n",
      "   macro avg       0.81      0.77      0.77       500\n",
      "weighted avg       0.82      0.77      0.78       500\n",
      "\n",
      "eval_label_collapsed   con  neutral  pro  refusal\n",
      "final_label_collapsed                            \n",
      "con                    101       37    9        0\n",
      "neutral                  3       81    9        0\n",
      "pro                      5       39  156        0\n",
      "refusal                 10        3    2       45\n",
      "\n",
      "############################################################\n",
      "#  Llama-3.1-70B-Instruct : templ-5.csv\n",
      "############################################################\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         con       0.85      0.97      0.90       147\n",
      "     neutral       0.83      0.58      0.68        93\n",
      "         pro       0.91      0.95      0.93       200\n",
      "     refusal       1.00      0.97      0.98        60\n",
      "\n",
      "    accuracy                           0.89       500\n",
      "   macro avg       0.90      0.87      0.88       500\n",
      "weighted avg       0.89      0.89      0.88       500\n",
      "\n",
      "eval_label_collapsed   con  neutral  pro  refusal\n",
      "final_label_collapsed                            \n",
      "con                    142        5    0        0\n",
      "neutral                 22       54   17        0\n",
      "pro                      3        6  191        0\n",
      "refusal                  1        0    1       58\n",
      "\n",
      "############################################################\n",
      "#  Llama-3.1-70B-Instruct : templ-6.csv\n",
      "############################################################\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         con       0.88      0.95      0.92       147\n",
      "     neutral       0.82      0.68      0.74        93\n",
      "         pro       0.94      0.95      0.94       200\n",
      "     refusal       0.98      1.00      0.99        60\n",
      "\n",
      "    accuracy                           0.91       500\n",
      "   macro avg       0.90      0.89      0.90       500\n",
      "weighted avg       0.90      0.91      0.90       500\n",
      "\n",
      "eval_label_collapsed   con  neutral  pro  refusal\n",
      "final_label_collapsed                            \n",
      "con                    140        7    0        0\n",
      "neutral                 17       63   13        0\n",
      "pro                      2        7  190        1\n",
      "refusal                  0        0    0       60\n",
      "\n",
      "############################################################\n",
      "#  Llama-3.1-70B-Instruct : templ-7.csv\n",
      "############################################################\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         con       0.84      0.93      0.88       147\n",
      "     neutral       0.77      0.61      0.68        93\n",
      "         pro       0.91      0.95      0.93       200\n",
      "     refusal       1.00      0.88      0.94        60\n",
      "\n",
      "    accuracy                           0.88       500\n",
      "   macro avg       0.88      0.85      0.86       500\n",
      "weighted avg       0.87      0.88      0.87       500\n",
      "\n",
      "eval_label_collapsed   con  neutral  pro  refusal\n",
      "final_label_collapsed                            \n",
      "con                    137       10    0        0\n",
      "neutral                 20       57   16        0\n",
      "pro                      2        7  191        0\n",
      "refusal                  4        0    3       53\n",
      "\n",
      "############################################################\n",
      "#  Llama-3.1-70B-Instruct : templ-8.csv\n",
      "############################################################\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         con       0.83      0.95      0.89       147\n",
      "     neutral       0.81      0.56      0.66        93\n",
      "         pro       0.89      0.97      0.93       200\n",
      "     refusal       1.00      0.87      0.93        60\n",
      "\n",
      "    accuracy                           0.87       500\n",
      "   macro avg       0.88      0.84      0.85       500\n",
      "weighted avg       0.87      0.87      0.87       500\n",
      "\n",
      "eval_label_collapsed   con  neutral  pro  refusal\n",
      "final_label_collapsed                            \n",
      "con                    139        8    0        0\n",
      "neutral                 22       52   19        0\n",
      "pro                      2        4  194        0\n",
      "refusal                  4        0    4       52\n",
      "\n",
      "############################################################\n",
      "#  Llama-3.1-8B-Instruct : templ-1.csv\n",
      "############################################################\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         con       0.73      0.84      0.78       147\n",
      "     neutral       0.58      0.70      0.63        93\n",
      "         pro       0.86      0.93      0.89       200\n",
      "     refusal       1.00      0.02      0.03        60\n",
      "\n",
      "    accuracy                           0.75       500\n",
      "   macro avg       0.79      0.62      0.59       500\n",
      "weighted avg       0.79      0.75      0.71       500\n",
      "\n",
      "eval_label_collapsed   con  neutral  pro  refusal\n",
      "final_label_collapsed                            \n",
      "con                    124       23    0        0\n",
      "neutral                 10       65   18        0\n",
      "pro                      1       13  186        0\n",
      "refusal                 36       11   12        1\n",
      "\n",
      "############################################################\n",
      "#  Llama-3.1-8B-Instruct : templ-2.csv\n",
      "############################################################\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         con       0.78      0.68      0.73       147\n",
      "     neutral       0.55      0.73      0.63        93\n",
      "         pro       0.84      0.94      0.89       200\n",
      "     refusal       1.00      0.42      0.59        60\n",
      "\n",
      "    accuracy                           0.76       500\n",
      "   macro avg       0.79      0.69      0.71       500\n",
      "weighted avg       0.79      0.76      0.76       500\n",
      "\n",
      "eval_label_collapsed   con  neutral  pro  refusal\n",
      "final_label_collapsed                            \n",
      "con                    100       42    5        0\n",
      "neutral                  3       68   22        0\n",
      "pro                      1       11  188        0\n",
      "refusal                 24        3    8       25\n",
      "\n",
      "############################################################\n",
      "#  Llama-3.1-8B-Instruct : templ-3.csv\n",
      "############################################################\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         con       0.69      0.78      0.73       147\n",
      "     neutral       0.48      0.76      0.59        93\n",
      "         pro       0.86      0.79      0.83       200\n",
      "     refusal       1.00      0.08      0.15        60\n",
      "\n",
      "    accuracy                           0.70       500\n",
      "   macro avg       0.76      0.60      0.58       500\n",
      "weighted avg       0.76      0.70      0.67       500\n",
      "\n",
      "eval_label_collapsed   con  neutral  pro  refusal\n",
      "final_label_collapsed                            \n",
      "con                    114       28    5        0\n",
      "neutral                  9       71   13        0\n",
      "pro                     12       30  158        0\n",
      "refusal                 30       18    7        5\n",
      "\n",
      "############################################################\n",
      "#  Llama-3.1-8B-Instruct : templ-4.csv\n",
      "############################################################\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         con       0.84      0.52      0.64       147\n",
      "     neutral       0.38      0.88      0.53        93\n",
      "         pro       0.84      0.65      0.73       200\n",
      "     refusal       0.97      0.63      0.77        60\n",
      "\n",
      "    accuracy                           0.65       500\n",
      "   macro avg       0.76      0.67      0.67       500\n",
      "weighted avg       0.77      0.65      0.67       500\n",
      "\n",
      "eval_label_collapsed   con  neutral  pro  refusal\n",
      "final_label_collapsed                            \n",
      "con                     76       57   14        0\n",
      "neutral                  5       82    6        0\n",
      "pro                      1       68  130        1\n",
      "refusal                  8       10    4       38\n",
      "\n",
      "############################################################\n",
      "#  Llama-3.1-8B-Instruct : templ-5.csv\n",
      "############################################################\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         con       0.91      0.88      0.89       147\n",
      "     neutral       0.67      0.42      0.52        93\n",
      "         pro       0.78      0.98      0.87       200\n",
      "     refusal       1.00      0.82      0.90        60\n",
      "\n",
      "    accuracy                           0.83       500\n",
      "   macro avg       0.84      0.77      0.80       500\n",
      "weighted avg       0.83      0.83      0.82       500\n",
      "\n",
      "eval_label_collapsed   con  neutral  pro  refusal\n",
      "final_label_collapsed                            \n",
      "con                    129       15    3        0\n",
      "neutral                  8       39   46        0\n",
      "pro                      0        3  197        0\n",
      "refusal                  5        1    5       49\n",
      "\n",
      "############################################################\n",
      "#  Llama-3.1-8B-Instruct : templ-6.csv\n",
      "############################################################\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         con       0.92      0.88      0.90       147\n",
      "     neutral       0.70      0.52      0.59        93\n",
      "         pro       0.81      0.97      0.89       200\n",
      "     refusal       1.00      0.82      0.90        60\n",
      "\n",
      "    accuracy                           0.84       500\n",
      "   macro avg       0.86      0.80      0.82       500\n",
      "weighted avg       0.84      0.84      0.84       500\n",
      "\n",
      "eval_label_collapsed   con  neutral  pro  refusal\n",
      "final_label_collapsed                            \n",
      "con                    130       14    3        0\n",
      "neutral                  7       48   38        0\n",
      "pro                      0        5  195        0\n",
      "refusal                  5        2    4       49\n",
      "\n",
      "############################################################\n",
      "#  Llama-3.1-8B-Instruct : templ-7.csv\n",
      "############################################################\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         con       0.91      0.82      0.86       147\n",
      "     neutral       0.59      0.38      0.46        93\n",
      "         pro       0.75      0.98      0.85       200\n",
      "     refusal       1.00      0.80      0.89        60\n",
      "\n",
      "    accuracy                           0.80       500\n",
      "   macro avg       0.81      0.74      0.77       500\n",
      "weighted avg       0.80      0.80      0.79       500\n",
      "\n",
      "eval_label_collapsed   con  neutral  pro  refusal\n",
      "final_label_collapsed                            \n",
      "con                    120       21    6        0\n",
      "neutral                  6       35   52        0\n",
      "pro                      0        3  197        0\n",
      "refusal                  6        0    6       48\n",
      "\n",
      "############################################################\n",
      "#  Llama-3.1-8B-Instruct : templ-8.csv\n",
      "############################################################\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         con       0.92      0.85      0.88       147\n",
      "     neutral       0.65      0.33      0.44        93\n",
      "         pro       0.73      0.99      0.84       200\n",
      "     refusal       1.00      0.73      0.85        60\n",
      "\n",
      "    accuracy                           0.80       500\n",
      "   macro avg       0.82      0.73      0.75       500\n",
      "weighted avg       0.80      0.80      0.78       500\n",
      "\n",
      "eval_label_collapsed   con  neutral  pro  refusal\n",
      "final_label_collapsed                            \n",
      "con                    125       14    8        0\n",
      "neutral                  5       31   57        0\n",
      "pro                      0        1  199        0\n",
      "refusal                  6        2    8       44\n",
      "\n",
      "############################################################\n",
      "#  Llama-3.2-3B-Instruct : templ-1.csv\n",
      "############################################################\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       ERROR       0.00      0.00      0.00         0\n",
      "         con       0.77      0.76      0.76       147\n",
      "     neutral       0.48      0.72      0.57        93\n",
      "         pro       0.83      0.77      0.80       200\n",
      "     refusal       0.85      0.38      0.53        60\n",
      "\n",
      "    accuracy                           0.71       500\n",
      "   macro avg       0.59      0.53      0.53       500\n",
      "weighted avg       0.75      0.71      0.71       500\n",
      "\n",
      "eval_label_collapsed   ERROR  con  neutral  pro  refusal\n",
      "final_label_collapsed                                   \n",
      "con                        0  112       30    3        2\n",
      "neutral                    0   10       67   15        1\n",
      "pro                        1    4       40  154        1\n",
      "refusal                    0   20        4   13       23\n",
      "\n",
      "############################################################\n",
      "#  Llama-3.2-3B-Instruct : templ-2.csv\n",
      "############################################################\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       ERROR       0.00      0.00      0.00         0\n",
      "         con       0.73      0.05      0.10       147\n",
      "     neutral       0.25      0.97      0.40        93\n",
      "         pro       0.80      0.39      0.52       200\n",
      "     refusal       0.94      0.57      0.71        60\n",
      "\n",
      "    accuracy                           0.42       500\n",
      "   macro avg       0.54      0.40      0.35       500\n",
      "weighted avg       0.69      0.42      0.40       500\n",
      "\n",
      "eval_label_collapsed   ERROR  con  neutral  pro  refusal\n",
      "final_label_collapsed                                   \n",
      "con                        0    8      125   12        2\n",
      "neutral                    0    0       90    3        0\n",
      "pro                        1    0      121   78        0\n",
      "refusal                    0    3       18    5       34\n",
      "\n",
      "############################################################\n",
      "#  Llama-3.2-3B-Instruct : templ-3.csv\n",
      "############################################################\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         con       0.79      0.47      0.59       147\n",
      "     neutral       0.44      0.81      0.57        93\n",
      "         pro       0.79      0.79      0.79       200\n",
      "     refusal       0.98      0.67      0.79        60\n",
      "\n",
      "    accuracy                           0.68       500\n",
      "   macro avg       0.75      0.68      0.68       500\n",
      "weighted avg       0.75      0.68      0.69       500\n",
      "\n",
      "eval_label_collapsed   con  neutral  pro  refusal\n",
      "final_label_collapsed                            \n",
      "con                     69       54   24        0\n",
      "neutral                  4       75   13        1\n",
      "pro                      2       40  158        0\n",
      "refusal                 12        3    5       40\n",
      "\n",
      "############################################################\n",
      "#  Llama-3.2-3B-Instruct : templ-4.csv\n",
      "############################################################\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       ERROR       0.00      0.00      0.00         0\n",
      "         con       0.91      0.14      0.25       147\n",
      "     neutral       0.26      0.92      0.41        93\n",
      "         pro       0.77      0.38      0.50       200\n",
      "     refusal       0.83      0.73      0.78        60\n",
      "\n",
      "    accuracy                           0.45       500\n",
      "   macro avg       0.55      0.44      0.39       500\n",
      "weighted avg       0.72      0.45      0.44       500\n",
      "\n",
      "eval_label_collapsed   ERROR  con  neutral  pro  refusal\n",
      "final_label_collapsed                                   \n",
      "con                        0   21      104   19        3\n",
      "neutral                    0    1       86    3        3\n",
      "pro                        1    0      121   75        3\n",
      "refusal                    0    1       14    1       44\n",
      "\n",
      "############################################################\n",
      "#  Llama-3.2-3B-Instruct : templ-5.csv\n",
      "############################################################\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       ERROR       0.00      0.00      0.00         0\n",
      "         con       0.95      0.61      0.74       147\n",
      "     neutral       0.67      0.22      0.33        93\n",
      "         pro       0.63      0.98      0.77       200\n",
      "     refusal       0.83      0.82      0.82        60\n",
      "\n",
      "    accuracy                           0.71       500\n",
      "   macro avg       0.62      0.52      0.53       500\n",
      "weighted avg       0.76      0.71      0.68       500\n",
      "\n",
      "eval_label_collapsed   ERROR  con  neutral  pro  refusal\n",
      "final_label_collapsed                                   \n",
      "con                        1   89        8   42        7\n",
      "neutral                    3    2       20   66        2\n",
      "pro                        2    0        0  197        1\n",
      "refusal                    0    3        2    6       49\n",
      "\n",
      "############################################################\n",
      "#  Llama-3.2-3B-Instruct : templ-6.csv\n",
      "############################################################\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       ERROR       0.00      0.00      0.00         0\n",
      "         con       0.96      0.60      0.74       147\n",
      "     neutral       0.65      0.44      0.53        93\n",
      "         pro       0.67      0.96      0.79       200\n",
      "     refusal       0.91      0.80      0.85        60\n",
      "\n",
      "    accuracy                           0.74       500\n",
      "   macro avg       0.64      0.56      0.58       500\n",
      "weighted avg       0.78      0.74      0.73       500\n",
      "\n",
      "eval_label_collapsed   ERROR  con  neutral  pro  refusal\n",
      "final_label_collapsed                                   \n",
      "con                        1   88       14   41        3\n",
      "neutral                    1    0       41   50        1\n",
      "pro                        0    0        6  193        1\n",
      "refusal                    0    4        2    6       48\n",
      "\n",
      "############################################################\n",
      "#  Llama-3.2-3B-Instruct : templ-7.csv\n",
      "############################################################\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       ERROR       0.00      0.00      0.00         0\n",
      "         con       0.94      0.57      0.71       147\n",
      "     neutral       0.73      0.12      0.20        93\n",
      "         pro       0.61      0.97      0.75       200\n",
      "     refusal       0.80      0.85      0.82        60\n",
      "\n",
      "    accuracy                           0.68       500\n",
      "   macro avg       0.62      0.50      0.50       500\n",
      "weighted avg       0.75      0.68      0.65       500\n",
      "\n",
      "eval_label_collapsed   ERROR  con  neutral  pro  refusal\n",
      "final_label_collapsed                                   \n",
      "con                        2   84        3   50        8\n",
      "neutral                    8    3       11   67        4\n",
      "pro                        5    0        0  194        1\n",
      "refusal                    0    2        1    6       51\n",
      "\n",
      "############################################################\n",
      "#  Llama-3.2-3B-Instruct : templ-8.csv\n",
      "############################################################\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       ERROR       0.00      0.00      0.00         0\n",
      "         con       0.96      0.61      0.75       147\n",
      "     neutral       0.76      0.14      0.24        93\n",
      "         pro       0.62      0.99      0.76       200\n",
      "     refusal       0.78      0.85      0.82        60\n",
      "\n",
      "    accuracy                           0.70       500\n",
      "   macro avg       0.62      0.52      0.51       500\n",
      "weighted avg       0.76      0.70      0.67       500\n",
      "\n",
      "eval_label_collapsed   ERROR  con  neutral  pro  refusal\n",
      "final_label_collapsed                                   \n",
      "con                        1   90        3   45        8\n",
      "neutral                    1    2       13   72        5\n",
      "pro                        1    0        0  198        1\n",
      "refusal                    0    2        1    6       51\n",
      "\n",
      "############################################################\n",
      "#  Ministral-8B-Instruct-2410 : templ-1.csv\n",
      "############################################################\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         con       0.85      0.76      0.81       147\n",
      "     neutral       0.46      0.95      0.62        93\n",
      "         pro       0.94      0.68      0.79       200\n",
      "     refusal       1.00      0.58      0.74        60\n",
      "\n",
      "    accuracy                           0.74       500\n",
      "   macro avg       0.82      0.74      0.74       500\n",
      "weighted avg       0.84      0.74      0.76       500\n",
      "\n",
      "eval_label_collapsed   con  neutral  pro  refusal\n",
      "final_label_collapsed                            \n",
      "con                    112       35    0        0\n",
      "neutral                  3       88    2        0\n",
      "pro                      0       64  136        0\n",
      "refusal                 16        3    6       35\n",
      "\n",
      "############################################################\n",
      "#  Ministral-8B-Instruct-2410 : templ-2.csv\n",
      "############################################################\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         con       0.91      0.63      0.74       147\n",
      "     neutral       0.40      0.95      0.56        93\n",
      "         pro       0.96      0.64      0.77       200\n",
      "     refusal       0.94      0.73      0.82        60\n",
      "\n",
      "    accuracy                           0.70       500\n",
      "   macro avg       0.80      0.74      0.72       500\n",
      "weighted avg       0.84      0.70      0.73       500\n",
      "\n",
      "eval_label_collapsed   con  neutral  pro  refusal\n",
      "final_label_collapsed                            \n",
      "con                     92       54    0        1\n",
      "neutral                  2       88    1        2\n",
      "pro                      1       71  128        0\n",
      "refusal                  6        6    4       44\n",
      "\n",
      "############################################################\n",
      "#  Ministral-8B-Instruct-2410 : templ-3.csv\n",
      "############################################################\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         con       0.90      0.37      0.53       147\n",
      "     neutral       0.27      0.99      0.42        93\n",
      "         pro       0.94      0.22      0.36       200\n",
      "     refusal       1.00      0.82      0.90        60\n",
      "\n",
      "    accuracy                           0.48       500\n",
      "   macro avg       0.78      0.60      0.55       500\n",
      "weighted avg       0.81      0.48      0.48       500\n",
      "\n",
      "eval_label_collapsed   con  neutral  pro  refusal\n",
      "final_label_collapsed                            \n",
      "con                     55       91    1        0\n",
      "neutral                  1       92    0        0\n",
      "pro                      1      155   44        0\n",
      "refusal                  4        5    2       49\n",
      "\n",
      "############################################################\n",
      "#  Ministral-8B-Instruct-2410 : templ-4.csv\n",
      "############################################################\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         con       0.93      0.27      0.42       147\n",
      "     neutral       0.24      0.91      0.39        93\n",
      "         pro       1.00      0.14      0.25       200\n",
      "     refusal       0.65      0.88      0.75        60\n",
      "\n",
      "    accuracy                           0.41       500\n",
      "   macro avg       0.71      0.55      0.45       500\n",
      "weighted avg       0.80      0.41      0.38       500\n",
      "\n",
      "eval_label_collapsed   con  neutral  pro  refusal\n",
      "final_label_collapsed                            \n",
      "con                     40       91    0       16\n",
      "neutral                  0       85    0        8\n",
      "pro                      0      168   28        4\n",
      "refusal                  3        4    0       53\n",
      "\n",
      "############################################################\n",
      "#  Ministral-8B-Instruct-2410 : templ-5.csv\n",
      "############################################################\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       ERROR       0.00      0.00      0.00         0\n",
      "         con       0.93      0.84      0.88       147\n",
      "     neutral       0.65      0.77      0.71        93\n",
      "         pro       0.92      0.90      0.91       200\n",
      "     refusal       0.97      0.93      0.95        60\n",
      "\n",
      "    accuracy                           0.86       500\n",
      "   macro avg       0.69      0.69      0.69       500\n",
      "weighted avg       0.88      0.86      0.87       500\n",
      "\n",
      "eval_label_collapsed   ERROR  con  neutral  pro  refusal\n",
      "final_label_collapsed                                   \n",
      "con                        0  124       21    2        0\n",
      "neutral                    1    7       72   12        1\n",
      "pro                        2    0       18  179        1\n",
      "refusal                    0    3        0    1       56\n",
      "\n",
      "############################################################\n",
      "#  Ministral-8B-Instruct-2410 : templ-6.csv\n",
      "############################################################\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         con       0.92      0.84      0.88       147\n",
      "     neutral       0.62      0.86      0.72        93\n",
      "         pro       0.97      0.88      0.92       200\n",
      "     refusal       0.97      0.93      0.95        60\n",
      "\n",
      "    accuracy                           0.87       500\n",
      "   macro avg       0.87      0.88      0.87       500\n",
      "weighted avg       0.89      0.87      0.87       500\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/paul/Documents/Repos/realpoliticalprompts/env/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/paul/Documents/Repos/realpoliticalprompts/env/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/paul/Documents/Repos/realpoliticalprompts/env/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/paul/Documents/Repos/realpoliticalprompts/env/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/paul/Documents/Repos/realpoliticalprompts/env/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/paul/Documents/Repos/realpoliticalprompts/env/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/paul/Documents/Repos/realpoliticalprompts/env/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/paul/Documents/Repos/realpoliticalprompts/env/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/paul/Documents/Repos/realpoliticalprompts/env/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/paul/Documents/Repos/realpoliticalprompts/env/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/paul/Documents/Repos/realpoliticalprompts/env/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/paul/Documents/Repos/realpoliticalprompts/env/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/paul/Documents/Repos/realpoliticalprompts/env/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/paul/Documents/Repos/realpoliticalprompts/env/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/paul/Documents/Repos/realpoliticalprompts/env/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/paul/Documents/Repos/realpoliticalprompts/env/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/paul/Documents/Repos/realpoliticalprompts/env/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/paul/Documents/Repos/realpoliticalprompts/env/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/paul/Documents/Repos/realpoliticalprompts/env/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/paul/Documents/Repos/realpoliticalprompts/env/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/paul/Documents/Repos/realpoliticalprompts/env/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/paul/Documents/Repos/realpoliticalprompts/env/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/paul/Documents/Repos/realpoliticalprompts/env/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/paul/Documents/Repos/realpoliticalprompts/env/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/paul/Documents/Repos/realpoliticalprompts/env/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/paul/Documents/Repos/realpoliticalprompts/env/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/paul/Documents/Repos/realpoliticalprompts/env/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/paul/Documents/Repos/realpoliticalprompts/env/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/paul/Documents/Repos/realpoliticalprompts/env/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/paul/Documents/Repos/realpoliticalprompts/env/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval_label_collapsed   con  neutral  pro  refusal\n",
      "final_label_collapsed                            \n",
      "con                    123       24    0        0\n",
      "neutral                  6       80    6        1\n",
      "pro                      1       23  175        1\n",
      "refusal                  3        1    0       56\n",
      "\n",
      "############################################################\n",
      "#  Ministral-8B-Instruct-2410 : templ-7.csv\n",
      "############################################################\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       ERROR       0.00      0.00      0.00         0\n",
      "         con       0.96      0.76      0.84       147\n",
      "     neutral       0.57      0.81      0.67        93\n",
      "         pro       0.90      0.86      0.88       200\n",
      "     refusal       0.98      0.90      0.94        60\n",
      "\n",
      "    accuracy                           0.83       500\n",
      "   macro avg       0.68      0.67      0.67       500\n",
      "weighted avg       0.87      0.83      0.84       500\n",
      "\n",
      "eval_label_collapsed   ERROR  con  neutral  pro  refusal\n",
      "final_label_collapsed                                   \n",
      "con                        1  111       33    2        0\n",
      "neutral                    1    2       75   15        0\n",
      "pro                        4    0       22  173        1\n",
      "refusal                    0    3        1    2       54\n",
      "\n",
      "############################################################\n",
      "#  Ministral-8B-Instruct-2410 : templ-8.csv\n",
      "############################################################\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       ERROR       0.00      0.00      0.00         0\n",
      "         con       0.93      0.77      0.84       147\n",
      "     neutral       0.62      0.72      0.67        93\n",
      "         pro       0.88      0.92      0.90       200\n",
      "     refusal       0.98      0.88      0.93        60\n",
      "\n",
      "    accuracy                           0.83       500\n",
      "   macro avg       0.68      0.66      0.67       500\n",
      "weighted avg       0.86      0.83      0.84       500\n",
      "\n",
      "eval_label_collapsed   ERROR  con  neutral  pro  refusal\n",
      "final_label_collapsed                                   \n",
      "con                        0  113       32    2        0\n",
      "neutral                    2    4       67   20        0\n",
      "pro                        7    0        9  183        1\n",
      "refusal                    0    4        0    3       53\n",
      "\n",
      "############################################################\n",
      "#  Mistral-7B-Instruct-v0.3 : templ-1.csv\n",
      "############################################################\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         con       0.74      0.95      0.83       147\n",
      "     neutral       0.69      0.48      0.57        93\n",
      "         pro       0.85      0.94      0.89       200\n",
      "     refusal       1.00      0.48      0.65        60\n",
      "\n",
      "    accuracy                           0.80       500\n",
      "   macro avg       0.82      0.71      0.74       500\n",
      "weighted avg       0.81      0.80      0.79       500\n",
      "\n",
      "eval_label_collapsed   con  neutral  pro  refusal\n",
      "final_label_collapsed                            \n",
      "con                    139        6    2        0\n",
      "neutral                 28       45   20        0\n",
      "pro                      5        8  187        0\n",
      "refusal                 15        6   10       29\n",
      "\n",
      "############################################################\n",
      "#  Mistral-7B-Instruct-v0.3 : templ-2.csv\n",
      "############################################################\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         con       0.73      0.91      0.81       147\n",
      "     neutral       0.63      0.76      0.69        93\n",
      "         pro       0.88      0.79      0.83       200\n",
      "     refusal       0.96      0.40      0.56        60\n",
      "\n",
      "    accuracy                           0.77       500\n",
      "   macro avg       0.80      0.72      0.72       500\n",
      "weighted avg       0.80      0.77      0.77       500\n",
      "\n",
      "eval_label_collapsed   con  neutral  pro  refusal\n",
      "final_label_collapsed                            \n",
      "con                    134       10    2        1\n",
      "neutral                 14       71    8        0\n",
      "pro                     15       27  158        0\n",
      "refusal                 20        5   11       24\n",
      "\n",
      "############################################################\n",
      "#  Mistral-7B-Instruct-v0.3 : templ-3.csv\n",
      "############################################################\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         con       0.72      0.88      0.80       147\n",
      "     neutral       0.58      0.69      0.63        93\n",
      "         pro       0.89      0.71      0.79       200\n",
      "     refusal       0.90      0.73      0.81        60\n",
      "\n",
      "    accuracy                           0.76       500\n",
      "   macro avg       0.77      0.76      0.76       500\n",
      "weighted avg       0.78      0.76      0.76       500\n",
      "\n",
      "eval_label_collapsed   con  neutral  pro  refusal\n",
      "final_label_collapsed                            \n",
      "con                    130        9    4        4\n",
      "neutral                 20       64    9        0\n",
      "pro                     23       33  143        1\n",
      "refusal                  7        4    5       44\n",
      "\n",
      "############################################################\n",
      "#  Mistral-7B-Instruct-v0.3 : templ-4.csv\n",
      "############################################################\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         con       0.71      0.84      0.77       147\n",
      "     neutral       0.32      0.84      0.46        93\n",
      "         pro       0.97      0.16      0.27       200\n",
      "     refusal       0.76      0.65      0.70        60\n",
      "\n",
      "    accuracy                           0.54       500\n",
      "   macro avg       0.69      0.62      0.55       500\n",
      "weighted avg       0.75      0.54      0.51       500\n",
      "\n",
      "eval_label_collapsed   con  neutral  pro  refusal\n",
      "final_label_collapsed                            \n",
      "con                    123       17    0        7\n",
      "neutral                 12       78    1        2\n",
      "pro                     28      137   32        3\n",
      "refusal                 10       11    0       39\n",
      "\n",
      "############################################################\n",
      "#  Mistral-7B-Instruct-v0.3 : templ-5.csv\n",
      "############################################################\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         con       0.83      0.97      0.89       147\n",
      "     neutral       0.72      0.68      0.70        93\n",
      "         pro       0.90      0.90      0.90       200\n",
      "     refusal       1.00      0.70      0.82        60\n",
      "\n",
      "    accuracy                           0.85       500\n",
      "   macro avg       0.86      0.81      0.83       500\n",
      "weighted avg       0.86      0.85      0.85       500\n",
      "\n",
      "eval_label_collapsed   con  neutral  pro  refusal\n",
      "final_label_collapsed                            \n",
      "con                    142        4    1        0\n",
      "neutral                 17       63   13        0\n",
      "pro                      4       16  180        0\n",
      "refusal                  9        4    5       42\n",
      "\n",
      "############################################################\n",
      "#  Mistral-7B-Instruct-v0.3 : templ-6.csv\n",
      "############################################################\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         con       0.73      0.99      0.84       147\n",
      "     neutral       0.75      0.52      0.61        93\n",
      "         pro       0.91      0.90      0.90       200\n",
      "     refusal       1.00      0.68      0.81        60\n",
      "\n",
      "    accuracy                           0.83       500\n",
      "   macro avg       0.85      0.77      0.79       500\n",
      "weighted avg       0.84      0.83      0.82       500\n",
      "\n",
      "eval_label_collapsed   con  neutral  pro  refusal\n",
      "final_label_collapsed                            \n",
      "con                    145        1    1        0\n",
      "neutral                 30       48   15        0\n",
      "pro                     11       10  179        0\n",
      "refusal                 12        5    2       41\n",
      "\n",
      "############################################################\n",
      "#  Mistral-7B-Instruct-v0.3 : templ-7.csv\n",
      "############################################################\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         con       0.78      0.97      0.87       147\n",
      "     neutral       0.70      0.56      0.62        93\n",
      "         pro       0.89      0.92      0.91       200\n",
      "     refusal       1.00      0.65      0.79        60\n",
      "\n",
      "    accuracy                           0.83       500\n",
      "   macro avg       0.85      0.77      0.80       500\n",
      "weighted avg       0.84      0.83      0.83       500\n",
      "\n",
      "eval_label_collapsed   con  neutral  pro  refusal\n",
      "final_label_collapsed                            \n",
      "con                    142        4    1        0\n",
      "neutral                 27       52   14        0\n",
      "pro                      4       12  184        0\n",
      "refusal                  8        6    7       39\n",
      "\n",
      "############################################################\n",
      "#  Mistral-7B-Instruct-v0.3 : templ-8.csv\n",
      "############################################################\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         con       0.75      0.97      0.85       147\n",
      "     neutral       0.68      0.47      0.56        93\n",
      "         pro       0.86      0.90      0.88       200\n",
      "     refusal       1.00      0.60      0.75        60\n",
      "\n",
      "    accuracy                           0.81       500\n",
      "   macro avg       0.82      0.74      0.76       500\n",
      "weighted avg       0.81      0.81      0.80       500\n",
      "\n",
      "eval_label_collapsed   con  neutral  pro  refusal\n",
      "final_label_collapsed                            \n",
      "con                    143        4    0        0\n",
      "neutral                 29       44   20        0\n",
      "pro                      8       12  180        0\n",
      "refusal                 10        5    9       36\n",
      "\n",
      "############################################################\n",
      "#  Mistral-Nemo-Instruct-2407 : templ-1.csv\n",
      "############################################################\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         con       0.78      0.93      0.85       147\n",
      "     neutral       0.61      0.82      0.70        93\n",
      "         pro       0.94      0.83      0.88       200\n",
      "     refusal       1.00      0.42      0.59        60\n",
      "\n",
      "    accuracy                           0.81       500\n",
      "   macro avg       0.83      0.75      0.75       500\n",
      "weighted avg       0.84      0.81      0.80       500\n",
      "\n",
      "eval_label_collapsed   con  neutral  pro  refusal\n",
      "final_label_collapsed                            \n",
      "con                    136       11    0        0\n",
      "neutral                 11       76    6        0\n",
      "pro                      3       31  166        0\n",
      "refusal                 24        6    5       25\n",
      "\n",
      "############################################################\n",
      "#  Mistral-Nemo-Instruct-2407 : templ-2.csv\n",
      "############################################################\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         con       0.81      0.79      0.80       147\n",
      "     neutral       0.53      0.86      0.65        93\n",
      "         pro       0.92      0.80      0.86       200\n",
      "     refusal       1.00      0.52      0.68        60\n",
      "\n",
      "    accuracy                           0.77       500\n",
      "   macro avg       0.81      0.74      0.75       500\n",
      "weighted avg       0.82      0.77      0.78       500\n",
      "\n",
      "eval_label_collapsed   con  neutral  pro  refusal\n",
      "final_label_collapsed                            \n",
      "con                    116       30    1        0\n",
      "neutral                  7       80    6        0\n",
      "pro                      2       38  160        0\n",
      "refusal                 19        4    6       31\n",
      "\n",
      "############################################################\n",
      "#  Mistral-Nemo-Instruct-2407 : templ-3.csv\n",
      "############################################################\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         con       0.78      0.76      0.77       147\n",
      "     neutral       0.35      0.94      0.51        93\n",
      "         pro       0.95      0.34      0.51       200\n",
      "     refusal       1.00      0.60      0.75        60\n",
      "\n",
      "    accuracy                           0.61       500\n",
      "   macro avg       0.77      0.66      0.63       500\n",
      "weighted avg       0.79      0.61      0.61       500\n",
      "\n",
      "eval_label_collapsed   con  neutral  pro  refusal\n",
      "final_label_collapsed                            \n",
      "con                    112       34    1        0\n",
      "neutral                  5       87    1        0\n",
      "pro                     12      119   69        0\n",
      "refusal                 14        8    2       36\n",
      "\n",
      "############################################################\n",
      "#  Mistral-Nemo-Instruct-2407 : templ-4.csv\n",
      "############################################################\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         con       0.76      0.67      0.71       147\n",
      "     neutral       0.43      0.90      0.58        93\n",
      "         pro       0.88      0.60      0.71       200\n",
      "     refusal       0.92      0.60      0.73        60\n",
      "\n",
      "    accuracy                           0.68       500\n",
      "   macro avg       0.75      0.69      0.68       500\n",
      "weighted avg       0.77      0.68      0.69       500\n",
      "\n",
      "eval_label_collapsed   con  neutral  pro  refusal\n",
      "final_label_collapsed                            \n",
      "con                     99       39    8        1\n",
      "neutral                  4       84    4        1\n",
      "pro                     12       67  120        1\n",
      "refusal                 15        5    4       36\n",
      "\n",
      "############################################################\n",
      "#  Mistral-Nemo-Instruct-2407 : templ-5.csv\n",
      "############################################################\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         con       0.86      0.97      0.91       147\n",
      "     neutral       0.65      0.80      0.71        93\n",
      "         pro       0.97      0.80      0.88       200\n",
      "     refusal       0.98      0.92      0.95        60\n",
      "\n",
      "    accuracy                           0.86       500\n",
      "   macro avg       0.87      0.87      0.86       500\n",
      "weighted avg       0.88      0.86      0.87       500\n",
      "\n",
      "eval_label_collapsed   con  neutral  pro  refusal\n",
      "final_label_collapsed                            \n",
      "con                    142        5    0        0\n",
      "neutral                 15       74    4        0\n",
      "pro                      4       35  160        1\n",
      "refusal                  4        0    1       55\n",
      "\n",
      "############################################################\n",
      "#  Mistral-Nemo-Instruct-2407 : templ-6.csv\n",
      "############################################################\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         con       0.86      0.97      0.91       147\n",
      "     neutral       0.62      0.78      0.70        93\n",
      "         pro       0.96      0.79      0.86       200\n",
      "     refusal       0.98      0.87      0.92        60\n",
      "\n",
      "    accuracy                           0.85       500\n",
      "   macro avg       0.85      0.85      0.85       500\n",
      "weighted avg       0.87      0.85      0.85       500\n",
      "\n",
      "eval_label_collapsed   con  neutral  pro  refusal\n",
      "final_label_collapsed                            \n",
      "con                    142        5    0        0\n",
      "neutral                 14       73    6        0\n",
      "pro                      3       39  157        1\n",
      "refusal                  7        0    1       52\n",
      "\n",
      "############################################################\n",
      "#  Mistral-Nemo-Instruct-2407 : templ-7.csv\n",
      "############################################################\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         con       0.84      0.92      0.88       147\n",
      "     neutral       0.56      0.81      0.66        93\n",
      "         pro       0.97      0.77      0.86       200\n",
      "     refusal       0.98      0.80      0.88        60\n",
      "\n",
      "    accuracy                           0.82       500\n",
      "   macro avg       0.84      0.82      0.82       500\n",
      "weighted avg       0.86      0.82      0.83       500\n",
      "\n",
      "eval_label_collapsed   con  neutral  pro  refusal\n",
      "final_label_collapsed                            \n",
      "con                    135       12    0        0\n",
      "neutral                 16       75    2        0\n",
      "pro                      2       44  153        1\n",
      "refusal                  7        3    2       48\n",
      "\n",
      "############################################################\n",
      "#  Mistral-Nemo-Instruct-2407 : templ-8.csv\n",
      "############################################################\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         con       0.88      0.90      0.89       147\n",
      "     neutral       0.65      0.81      0.72        93\n",
      "         pro       0.94      0.85      0.90       200\n",
      "     refusal       0.98      0.85      0.91        60\n",
      "\n",
      "    accuracy                           0.86       500\n",
      "   macro avg       0.86      0.85      0.85       500\n",
      "weighted avg       0.87      0.86      0.86       500\n",
      "\n",
      "eval_label_collapsed   con  neutral  pro  refusal\n",
      "final_label_collapsed                            \n",
      "con                    133       14    0        0\n",
      "neutral                 12       75    6        0\n",
      "pro                      2       26  171        1\n",
      "refusal                  5        0    4       51\n",
      "\n",
      "############################################################\n",
      "#  gemma-2-27b-it : templ-1.csv\n",
      "############################################################\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         con       0.71      0.93      0.80       147\n",
      "     neutral       0.76      0.48      0.59        93\n",
      "         pro       0.83      0.97      0.89       200\n",
      "     refusal       1.00      0.23      0.38        60\n",
      "\n",
      "    accuracy                           0.78       500\n",
      "   macro avg       0.83      0.65      0.67       500\n",
      "weighted avg       0.80      0.78      0.75       500\n",
      "\n",
      "eval_label_collapsed   con  neutral  pro  refusal\n",
      "final_label_collapsed                            \n",
      "con                    136        9    2        0\n",
      "neutral                 23       45   25        0\n",
      "pro                      2        3  195        0\n",
      "refusal                 30        2   14       14\n",
      "\n",
      "############################################################\n",
      "#  gemma-2-27b-it : templ-2.csv\n",
      "############################################################\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         con       0.79      0.92      0.85       147\n",
      "     neutral       0.78      0.69      0.73        93\n",
      "         pro       0.86      0.95      0.90       200\n",
      "     refusal       1.00      0.45      0.62        60\n",
      "\n",
      "    accuracy                           0.83       500\n",
      "   macro avg       0.86      0.75      0.78       500\n",
      "weighted avg       0.84      0.83      0.82       500\n",
      "\n",
      "eval_label_collapsed   con  neutral  pro  refusal\n",
      "final_label_collapsed                            \n",
      "con                    135       10    2        0\n",
      "neutral                 11       64   18        0\n",
      "pro                      4        6  190        0\n",
      "refusal                 21        2   10       27\n",
      "\n",
      "############################################################\n",
      "#  gemma-2-27b-it : templ-3.csv\n",
      "############################################################\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         con       0.72      0.80      0.76       147\n",
      "     neutral       0.64      0.75      0.69        93\n",
      "         pro       0.88      0.83      0.85       200\n",
      "     refusal       1.00      0.62      0.76        60\n",
      "\n",
      "    accuracy                           0.78       500\n",
      "   macro avg       0.81      0.75      0.77       500\n",
      "weighted avg       0.80      0.78      0.78       500\n",
      "\n",
      "eval_label_collapsed   con  neutral  pro  refusal\n",
      "final_label_collapsed                            \n",
      "con                    118       21    8        0\n",
      "neutral                 11       70   12        0\n",
      "pro                     19       15  166        0\n",
      "refusal                 16        4    3       37\n",
      "\n",
      "############################################################\n",
      "#  gemma-2-27b-it : templ-4.csv\n",
      "############################################################\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         con       0.81      0.74      0.77       147\n",
      "     neutral       0.44      0.88      0.59        93\n",
      "         pro       0.89      0.60      0.72       200\n",
      "     refusal       0.98      0.72      0.83        60\n",
      "\n",
      "    accuracy                           0.71       500\n",
      "   macro avg       0.78      0.74      0.73       500\n",
      "weighted avg       0.79      0.71      0.72       500\n",
      "\n",
      "eval_label_collapsed   con  neutral  pro  refusal\n",
      "final_label_collapsed                            \n",
      "con                    109       31    6        1\n",
      "neutral                  5       82    6        0\n",
      "pro                      9       70  121        0\n",
      "refusal                 12        2    3       43\n",
      "\n",
      "############################################################\n",
      "#  gemma-2-27b-it : templ-5.csv\n",
      "############################################################\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         con       0.82      0.98      0.89       147\n",
      "     neutral       0.89      0.53      0.66        93\n",
      "         pro       0.89      0.96      0.92       200\n",
      "     refusal       1.00      0.85      0.92        60\n",
      "\n",
      "    accuracy                           0.87       500\n",
      "   macro avg       0.90      0.83      0.85       500\n",
      "weighted avg       0.88      0.87      0.86       500\n",
      "\n",
      "eval_label_collapsed   con  neutral  pro  refusal\n",
      "final_label_collapsed                            \n",
      "con                    144        2    1        0\n",
      "neutral                 22       49   22        0\n",
      "pro                      3        4  193        0\n",
      "refusal                  7        0    2       51\n",
      "\n",
      "############################################################\n",
      "#  gemma-2-27b-it : templ-6.csv\n",
      "############################################################\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         con       0.83      0.97      0.89       147\n",
      "     neutral       0.87      0.58      0.70        93\n",
      "         pro       0.89      0.96      0.93       200\n",
      "     refusal       1.00      0.85      0.92        60\n",
      "\n",
      "    accuracy                           0.88       500\n",
      "   macro avg       0.90      0.84      0.86       500\n",
      "weighted avg       0.88      0.88      0.87       500\n",
      "\n",
      "eval_label_collapsed   con  neutral  pro  refusal\n",
      "final_label_collapsed                            \n",
      "con                    142        4    1        0\n",
      "neutral                 20       54   19        0\n",
      "pro                      3        4  193        0\n",
      "refusal                  6        0    3       51\n",
      "\n",
      "############################################################\n",
      "#  gemma-2-27b-it : templ-7.csv\n",
      "############################################################\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         con       0.81      0.95      0.87       147\n",
      "     neutral       0.79      0.53      0.63        93\n",
      "         pro       0.87      0.97      0.92       200\n",
      "     refusal       1.00      0.75      0.86        60\n",
      "\n",
      "    accuracy                           0.85       500\n",
      "   macro avg       0.87      0.80      0.82       500\n",
      "weighted avg       0.86      0.85      0.85       500\n",
      "\n",
      "eval_label_collapsed   con  neutral  pro  refusal\n",
      "final_label_collapsed                            \n",
      "con                    139        7    1        0\n",
      "neutral                 21       49   23        0\n",
      "pro                      1        5  194        0\n",
      "refusal                 10        1    4       45\n",
      "\n",
      "############################################################\n",
      "#  gemma-2-27b-it : templ-8.csv\n",
      "############################################################\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         con       0.81      0.93      0.87       147\n",
      "     neutral       0.84      0.41      0.55        93\n",
      "         pro       0.80      0.99      0.89       200\n",
      "     refusal       1.00      0.70      0.82        60\n",
      "\n",
      "    accuracy                           0.83       500\n",
      "   macro avg       0.87      0.76      0.78       500\n",
      "weighted avg       0.84      0.83      0.81       500\n",
      "\n",
      "eval_label_collapsed   con  neutral  pro  refusal\n",
      "final_label_collapsed                            \n",
      "con                    136        4    7        0\n",
      "neutral                 19       38   36        0\n",
      "pro                      0        2  198        0\n",
      "refusal                 12        1    5       42\n",
      "\n",
      "############################################################\n",
      "#  gemma-2-9b-it : templ-1.csv\n",
      "############################################################\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         con       0.74      0.88      0.80       147\n",
      "     neutral       0.66      0.61      0.63        93\n",
      "         pro       0.85      0.95      0.90       200\n",
      "     refusal       1.00      0.23      0.38        60\n",
      "\n",
      "    accuracy                           0.78       500\n",
      "   macro avg       0.81      0.67      0.68       500\n",
      "weighted avg       0.80      0.78      0.76       500\n",
      "\n",
      "eval_label_collapsed   con  neutral  pro  refusal\n",
      "final_label_collapsed                            \n",
      "con                    129       18    0        0\n",
      "neutral                 14       57   22        0\n",
      "pro                      0        9  191        0\n",
      "refusal                 31        3   12       14\n",
      "\n",
      "############################################################\n",
      "#  gemma-2-9b-it : templ-2.csv\n",
      "############################################################\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         con       0.82      0.88      0.85       147\n",
      "     neutral       0.74      0.68      0.71        93\n",
      "         pro       0.84      0.95      0.89       200\n",
      "     refusal       1.00      0.52      0.68        60\n",
      "\n",
      "    accuracy                           0.83       500\n",
      "   macro avg       0.85      0.76      0.78       500\n",
      "weighted avg       0.84      0.83      0.82       500\n",
      "\n",
      "eval_label_collapsed   con  neutral  pro  refusal\n",
      "final_label_collapsed                            \n",
      "con                    129       13    5        0\n",
      "neutral                  7       63   23        0\n",
      "pro                      2        7  191        0\n",
      "refusal                 19        2    8       31\n",
      "\n",
      "############################################################\n",
      "#  gemma-2-9b-it : templ-3.csv\n",
      "############################################################\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         con       0.70      0.83      0.76       147\n",
      "     neutral       0.59      0.72      0.65        93\n",
      "         pro       0.87      0.81      0.84       200\n",
      "     refusal       1.00      0.38      0.55        60\n",
      "\n",
      "    accuracy                           0.75       500\n",
      "   macro avg       0.79      0.69      0.70       500\n",
      "weighted avg       0.78      0.75      0.75       500\n",
      "\n",
      "eval_label_collapsed   con  neutral  pro  refusal\n",
      "final_label_collapsed                            \n",
      "con                    122       22    3        0\n",
      "neutral                  9       67   17        0\n",
      "pro                     17       20  163        0\n",
      "refusal                 27        5    5       23\n",
      "\n",
      "############################################################\n",
      "#  gemma-2-9b-it : templ-4.csv\n",
      "############################################################\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         con       0.84      0.74      0.79       147\n",
      "     neutral       0.49      0.83      0.61        93\n",
      "         pro       0.88      0.71      0.79       200\n",
      "     refusal       0.94      0.77      0.84        60\n",
      "\n",
      "    accuracy                           0.75       500\n",
      "   macro avg       0.79      0.76      0.76       500\n",
      "weighted avg       0.80      0.75      0.76       500\n",
      "\n",
      "eval_label_collapsed   con  neutral  pro  refusal\n",
      "final_label_collapsed                            \n",
      "con                    109       29    8        1\n",
      "neutral                  8       77    7        1\n",
      "pro                      8       48  143        1\n",
      "refusal                  5        4    5       46\n",
      "\n",
      "############################################################\n",
      "#  gemma-2-9b-it : templ-5.csv\n",
      "############################################################\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         con       0.82      0.90      0.86       147\n",
      "     neutral       0.75      0.61      0.67        93\n",
      "         pro       0.86      0.96      0.91       200\n",
      "     refusal       1.00      0.65      0.79        60\n",
      "\n",
      "    accuracy                           0.84       500\n",
      "   macro avg       0.86      0.78      0.81       500\n",
      "weighted avg       0.85      0.84      0.84       500\n",
      "\n",
      "eval_label_collapsed   con  neutral  pro  refusal\n",
      "final_label_collapsed                            \n",
      "con                    133       14    0        0\n",
      "neutral                 12       57   24        0\n",
      "pro                      3        5  192        0\n",
      "refusal                 15        0    6       39\n",
      "\n",
      "############################################################\n",
      "#  gemma-2-9b-it : templ-6.csv\n",
      "############################################################\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         con       0.80      0.90      0.85       147\n",
      "     neutral       0.76      0.61      0.68        93\n",
      "         pro       0.87      0.96      0.92       200\n",
      "     refusal       1.00      0.62      0.76        60\n",
      "\n",
      "    accuracy                           0.84       500\n",
      "   macro avg       0.86      0.77      0.80       500\n",
      "weighted avg       0.84      0.84      0.83       500\n",
      "\n",
      "eval_label_collapsed   con  neutral  pro  refusal\n",
      "final_label_collapsed                            \n",
      "con                    133       14    0        0\n",
      "neutral                 15       57   21        0\n",
      "pro                      3        4  193        0\n",
      "refusal                 16        0    7       37\n",
      "\n",
      "############################################################\n",
      "#  gemma-2-9b-it : templ-7.csv\n",
      "############################################################\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         con       0.81      0.86      0.84       147\n",
      "     neutral       0.68      0.65      0.66        93\n",
      "         pro       0.86      0.95      0.91       200\n",
      "     refusal       1.00      0.57      0.72        60\n",
      "\n",
      "    accuracy                           0.82       500\n",
      "   macro avg       0.84      0.76      0.78       500\n",
      "weighted avg       0.83      0.82      0.82       500\n",
      "\n",
      "eval_label_collapsed   con  neutral  pro  refusal\n",
      "final_label_collapsed                            \n",
      "con                    127       20    0        0\n",
      "neutral                 11       60   22        0\n",
      "pro                      2        7  191        0\n",
      "refusal                 17        1    8       34\n",
      "\n",
      "############################################################\n",
      "#  gemma-2-9b-it : templ-8.csv\n",
      "############################################################\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         con       0.80      0.85      0.82       147\n",
      "     neutral       0.66      0.60      0.63        93\n",
      "         pro       0.85      0.96      0.91       200\n",
      "     refusal       1.00      0.53      0.70        60\n",
      "\n",
      "    accuracy                           0.81       500\n",
      "   macro avg       0.83      0.74      0.76       500\n",
      "weighted avg       0.82      0.81      0.80       500\n",
      "\n",
      "eval_label_collapsed   con  neutral  pro  refusal\n",
      "final_label_collapsed                            \n",
      "con                    125       22    0        0\n",
      "neutral                 13       56   24        0\n",
      "pro                      2        5  193        0\n",
      "refusal                 17        2    9       32\n",
      "\n",
      "############################################################\n",
      "#  gpt-3.5-turbo : templ-1.csv\n",
      "############################################################\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         con       0.78      0.81      0.80       147\n",
      "     neutral       0.46      0.89      0.61        93\n",
      "         pro       0.91      0.75      0.82       200\n",
      "     refusal       1.00      0.08      0.15        60\n",
      "\n",
      "    accuracy                           0.71       500\n",
      "   macro avg       0.79      0.63      0.60       500\n",
      "weighted avg       0.80      0.71      0.70       500\n",
      "\n",
      "eval_label_collapsed   con  neutral  pro  refusal\n",
      "final_label_collapsed                            \n",
      "con                    119       28    0        0\n",
      "neutral                  5       83    5        0\n",
      "pro                      0       50  150        0\n",
      "refusal                 28       18    9        5\n",
      "\n",
      "############################################################\n",
      "#  gpt-3.5-turbo : templ-2.csv\n",
      "############################################################\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         con       0.80      0.78      0.79       147\n",
      "     neutral       0.42      0.91      0.57        93\n",
      "         pro       0.94      0.59      0.73       200\n",
      "     refusal       1.00      0.48      0.65        60\n",
      "\n",
      "    accuracy                           0.69       500\n",
      "   macro avg       0.79      0.69      0.68       500\n",
      "weighted avg       0.81      0.69      0.71       500\n",
      "\n",
      "eval_label_collapsed   con  neutral  pro  refusal\n",
      "final_label_collapsed                            \n",
      "con                    114       33    0        0\n",
      "neutral                  5       85    3        0\n",
      "pro                      2       80  118        0\n",
      "refusal                 22        5    4       29\n",
      "\n",
      "############################################################\n",
      "#  gpt-3.5-turbo : templ-3.csv\n",
      "############################################################\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       ERROR       0.00      0.00      0.00         0\n",
      "         con       0.82      0.51      0.63       147\n",
      "     neutral       0.24      0.96      0.39        93\n",
      "         pro       0.94      0.07      0.14       200\n",
      "     refusal       0.55      0.18      0.28        60\n",
      "\n",
      "    accuracy                           0.38       500\n",
      "   macro avg       0.51      0.35      0.29       500\n",
      "weighted avg       0.73      0.38      0.35       500\n",
      "\n",
      "eval_label_collapsed   ERROR  con  neutral  pro  refusal\n",
      "final_label_collapsed                                   \n",
      "con                        1   75       68    0        3\n",
      "neutral                    1    0       89    1        2\n",
      "pro                        4    5      172   15        4\n",
      "refusal                    1   12       36    0       11\n",
      "\n",
      "############################################################\n",
      "#  gpt-3.5-turbo : templ-4.csv\n",
      "############################################################\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       ERROR       0.00      0.00      0.00         0\n",
      "         con       0.81      0.57      0.67       147\n",
      "     neutral       0.28      0.96      0.43        93\n",
      "         pro       1.00      0.13      0.23       200\n",
      "     refusal       0.83      0.63      0.72        60\n",
      "\n",
      "    accuracy                           0.47       500\n",
      "   macro avg       0.58      0.46      0.41       500\n",
      "weighted avg       0.79      0.47      0.46       500\n",
      "\n",
      "eval_label_collapsed   ERROR  con  neutral  pro  refusal\n",
      "final_label_collapsed                                   \n",
      "con                        2   84       56    0        5\n",
      "neutral                    2    1       89    0        1\n",
      "pro                        0    8      164   26        2\n",
      "refusal                    0   11       11    0       38\n",
      "\n",
      "############################################################\n",
      "#  gpt-3.5-turbo : templ-5.csv\n",
      "############################################################\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       ERROR       0.00      0.00      0.00         0\n",
      "         con       0.81      0.74      0.77       147\n",
      "     neutral       0.54      0.85      0.66        93\n",
      "         pro       0.89      0.89      0.89       200\n",
      "     refusal       1.00      0.27      0.42        60\n",
      "\n",
      "    accuracy                           0.76       500\n",
      "   macro avg       0.65      0.55      0.55       500\n",
      "weighted avg       0.81      0.76      0.75       500\n",
      "\n",
      "eval_label_collapsed   ERROR  con  neutral  pro  refusal\n",
      "final_label_collapsed                                   \n",
      "con                        0  109       36    2        0\n",
      "neutral                    0    3       79   11        0\n",
      "pro                        0    2       21  177        0\n",
      "refusal                    2   21       11   10       16\n",
      "\n",
      "############################################################\n",
      "#  gpt-3.5-turbo : templ-6.csv\n",
      "############################################################\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       ERROR       0.00      0.00      0.00         0\n",
      "         con       0.78      0.72      0.75       147\n",
      "     neutral       0.55      0.83      0.66        93\n",
      "         pro       0.89      0.92      0.90       200\n",
      "     refusal       1.00      0.27      0.42        60\n",
      "\n",
      "    accuracy                           0.76       500\n",
      "   macro avg       0.64      0.55      0.55       500\n",
      "weighted avg       0.81      0.76      0.75       500\n",
      "\n",
      "eval_label_collapsed   ERROR  con  neutral  pro  refusal\n",
      "final_label_collapsed                                   \n",
      "con                        0  106       38    3        0\n",
      "neutral                    0    4       77   12        0\n",
      "pro                        0    1       16  183        0\n",
      "refusal                    2   25        9    8       16\n",
      "\n",
      "############################################################\n",
      "#  gpt-3.5-turbo : templ-7.csv\n",
      "############################################################\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         con       0.76      0.58      0.66       147\n",
      "     neutral       0.45      0.85      0.59        93\n",
      "         pro       0.88      0.92      0.90       200\n",
      "     refusal       1.00      0.10      0.18        60\n",
      "\n",
      "    accuracy                           0.71       500\n",
      "   macro avg       0.77      0.61      0.58       500\n",
      "weighted avg       0.78      0.71      0.68       500\n",
      "\n",
      "eval_label_collapsed   con  neutral  pro  refusal\n",
      "final_label_collapsed                            \n",
      "con                     85       60    2        0\n",
      "neutral                  1       79   13        0\n",
      "pro                      0       17  183        0\n",
      "refusal                 26       18   10        6\n",
      "\n",
      "############################################################\n",
      "#  gpt-3.5-turbo : templ-8.csv\n",
      "############################################################\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         con       0.77      0.65      0.70       147\n",
      "     neutral       0.48      0.82      0.61        93\n",
      "         pro       0.88      0.92      0.90       200\n",
      "     refusal       1.00      0.17      0.29        60\n",
      "\n",
      "    accuracy                           0.73       500\n",
      "   macro avg       0.78      0.64      0.62       500\n",
      "weighted avg       0.79      0.73      0.71       500\n",
      "\n",
      "eval_label_collapsed   con  neutral  pro  refusal\n",
      "final_label_collapsed                            \n",
      "con                     95       50    2        0\n",
      "neutral                  2       76   15        0\n",
      "pro                      0       16  184        0\n",
      "refusal                 26       15    9       10\n",
      "\n",
      "############################################################\n",
      "#  gpt-4o-2024-05-13 : templ-1.csv\n",
      "############################################################\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         con       0.87      0.93      0.90       147\n",
      "     neutral       0.78      0.74      0.76        93\n",
      "         pro       0.93      0.94      0.94       200\n",
      "     refusal       0.98      0.85      0.91        60\n",
      "\n",
      "    accuracy                           0.89       500\n",
      "   macro avg       0.89      0.87      0.88       500\n",
      "weighted avg       0.89      0.89      0.89       500\n",
      "\n",
      "eval_label_collapsed   con  neutral  pro  refusal\n",
      "final_label_collapsed                            \n",
      "con                    136       11    0        0\n",
      "neutral                 12       69   12        0\n",
      "pro                      2        8  189        1\n",
      "refusal                  6        0    3       51\n",
      "\n",
      "############################################################\n",
      "#  gpt-4o-2024-05-13 : templ-2.csv\n",
      "############################################################\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         con       0.91      0.90      0.91       147\n",
      "     neutral       0.70      0.87      0.78        93\n",
      "         pro       0.95      0.90      0.92       200\n",
      "     refusal       0.98      0.80      0.88        60\n",
      "\n",
      "    accuracy                           0.88       500\n",
      "   macro avg       0.88      0.87      0.87       500\n",
      "weighted avg       0.89      0.88      0.88       500\n",
      "\n",
      "eval_label_collapsed   con  neutral  pro  refusal\n",
      "final_label_collapsed                            \n",
      "con                    133       14    0        0\n",
      "neutral                  6       81    5        1\n",
      "pro                      1       20  179        0\n",
      "refusal                  6        1    5       48\n",
      "\n",
      "############################################################\n",
      "#  gpt-4o-2024-05-13 : templ-3.csv\n",
      "############################################################\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         con       0.88      0.68      0.77       147\n",
      "     neutral       0.59      0.83      0.69        93\n",
      "         pro       0.93      0.73      0.82       200\n",
      "     refusal       0.57      0.95      0.71        60\n",
      "\n",
      "    accuracy                           0.76       500\n",
      "   macro avg       0.74      0.80      0.75       500\n",
      "weighted avg       0.81      0.76      0.77       500\n",
      "\n",
      "eval_label_collapsed   con  neutral  pro  refusal\n",
      "final_label_collapsed                            \n",
      "con                    100       28    5       14\n",
      "neutral                  2       77    5        9\n",
      "pro                      9       25  146       20\n",
      "refusal                  2        0    1       57\n",
      "\n",
      "############################################################\n",
      "#  gpt-4o-2024-05-13 : templ-4.csv\n",
      "############################################################\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         con       0.85      0.72      0.78       147\n",
      "     neutral       0.47      0.91      0.62        93\n",
      "         pro       0.96      0.59      0.73       200\n",
      "     refusal       0.68      0.82      0.74        60\n",
      "\n",
      "    accuracy                           0.72       500\n",
      "   macro avg       0.74      0.76      0.72       500\n",
      "weighted avg       0.80      0.72      0.73       500\n",
      "\n",
      "eval_label_collapsed   con  neutral  pro  refusal\n",
      "final_label_collapsed                            \n",
      "con                    106       26    2       13\n",
      "neutral                  3       85    2        3\n",
      "pro                      8       67  118        7\n",
      "refusal                  8        2    1       49\n",
      "\n",
      "############################################################\n",
      "#  gpt-4o-2024-05-13 : templ-5.csv\n",
      "############################################################\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         con       0.89      0.97      0.93       147\n",
      "     neutral       0.83      0.69      0.75        93\n",
      "         pro       0.95      0.95      0.95       200\n",
      "     refusal       0.95      1.00      0.98        60\n",
      "\n",
      "    accuracy                           0.91       500\n",
      "   macro avg       0.91      0.90      0.90       500\n",
      "weighted avg       0.91      0.91      0.91       500\n",
      "\n",
      "eval_label_collapsed   con  neutral  pro  refusal\n",
      "final_label_collapsed                            \n",
      "con                    142        5    0        0\n",
      "neutral                 16       64   11        2\n",
      "pro                      1        8  190        1\n",
      "refusal                  0        0    0       60\n",
      "\n",
      "############################################################\n",
      "#  gpt-4o-2024-05-13 : templ-6.csv\n",
      "############################################################\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         con       0.90      0.97      0.93       147\n",
      "     neutral       0.80      0.70      0.75        93\n",
      "         pro       0.94      0.94      0.94       200\n",
      "     refusal       0.95      1.00      0.98        60\n",
      "\n",
      "    accuracy                           0.91       500\n",
      "   macro avg       0.90      0.90      0.90       500\n",
      "weighted avg       0.91      0.91      0.91       500\n",
      "\n",
      "eval_label_collapsed   con  neutral  pro  refusal\n",
      "final_label_collapsed                            \n",
      "con                    142        5    0        0\n",
      "neutral                 15       65   11        2\n",
      "pro                      1       11  187        1\n",
      "refusal                  0        0    0       60\n",
      "\n",
      "############################################################\n",
      "#  gpt-4o-2024-05-13 : templ-7.csv\n",
      "############################################################\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         con       0.89      0.95      0.91       147\n",
      "     neutral       0.79      0.74      0.77        93\n",
      "         pro       0.95      0.94      0.95       200\n",
      "     refusal       1.00      0.98      0.99        60\n",
      "\n",
      "    accuracy                           0.91       500\n",
      "   macro avg       0.91      0.90      0.90       500\n",
      "weighted avg       0.91      0.91      0.91       500\n",
      "\n",
      "eval_label_collapsed   con  neutral  pro  refusal\n",
      "final_label_collapsed                            \n",
      "con                    139        8    0        0\n",
      "neutral                 15       69    9        0\n",
      "pro                      2       10  188        0\n",
      "refusal                  1        0    0       59\n",
      "\n",
      "############################################################\n",
      "#  gpt-4o-2024-05-13 : templ-8.csv\n",
      "############################################################\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         con       0.88      0.96      0.92       147\n",
      "     neutral       0.82      0.70      0.76        93\n",
      "         pro       0.94      0.94      0.94       200\n",
      "     refusal       0.98      0.98      0.98        60\n",
      "\n",
      "    accuracy                           0.91       500\n",
      "   macro avg       0.91      0.90      0.90       500\n",
      "weighted avg       0.91      0.91      0.91       500\n",
      "\n",
      "eval_label_collapsed   con  neutral  pro  refusal\n",
      "final_label_collapsed                            \n",
      "con                    141        6    0        0\n",
      "neutral                 16       65   12        0\n",
      "pro                      2        8  189        1\n",
      "refusal                  1        0    0       59\n",
      "\n",
      "############################################################\n",
      "#  gpt-4o-2024-08-06 : templ-1.csv\n",
      "############################################################\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         con       0.87      0.93      0.90       147\n",
      "     neutral       0.78      0.75      0.77        93\n",
      "         pro       0.94      0.95      0.94       200\n",
      "     refusal       1.00      0.85      0.92        60\n",
      "\n",
      "    accuracy                           0.89       500\n",
      "   macro avg       0.90      0.87      0.88       500\n",
      "weighted avg       0.90      0.89      0.89       500\n",
      "\n",
      "eval_label_collapsed   con  neutral  pro  refusal\n",
      "final_label_collapsed                            \n",
      "con                    136       11    0        0\n",
      "neutral                 13       70   10        0\n",
      "pro                      1        9  190        0\n",
      "refusal                  6        0    3       51\n",
      "\n",
      "############################################################\n",
      "#  gpt-4o-2024-08-06 : templ-2.csv\n",
      "############################################################\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         con       0.88      0.91      0.90       147\n",
      "     neutral       0.74      0.83      0.78        93\n",
      "         pro       0.93      0.93      0.93       200\n",
      "     refusal       0.98      0.73      0.84        60\n",
      "\n",
      "    accuracy                           0.88       500\n",
      "   macro avg       0.88      0.85      0.86       500\n",
      "weighted avg       0.89      0.88      0.88       500\n",
      "\n",
      "eval_label_collapsed   con  neutral  pro  refusal\n",
      "final_label_collapsed                            \n",
      "con                    134       13    0        0\n",
      "neutral                  7       77    8        1\n",
      "pro                      1       14  185        0\n",
      "refusal                 10        0    6       44\n",
      "\n",
      "############################################################\n",
      "#  gpt-4o-2024-08-06 : templ-3.csv\n",
      "############################################################\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         con       0.86      0.69      0.77       147\n",
      "     neutral       0.56      0.85      0.68        93\n",
      "         pro       0.93      0.77      0.84       200\n",
      "     refusal       0.71      0.90      0.79        60\n",
      "\n",
      "    accuracy                           0.78       500\n",
      "   macro avg       0.76      0.80      0.77       500\n",
      "weighted avg       0.81      0.78      0.78       500\n",
      "\n",
      "eval_label_collapsed   con  neutral  pro  refusal\n",
      "final_label_collapsed                            \n",
      "con                    102       30    5       10\n",
      "neutral                  4       79    5        5\n",
      "pro                     10       30  153        7\n",
      "refusal                  3        1    2       54\n",
      "\n",
      "############################################################\n",
      "#  gpt-4o-2024-08-06 : templ-4.csv\n",
      "############################################################\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         con       0.83      0.76      0.79       147\n",
      "     neutral       0.51      0.89      0.65        93\n",
      "         pro       0.95      0.68      0.79       200\n",
      "     refusal       0.86      0.85      0.86        60\n",
      "\n",
      "    accuracy                           0.76       500\n",
      "   macro avg       0.79      0.79      0.77       500\n",
      "weighted avg       0.82      0.76      0.77       500\n",
      "\n",
      "eval_label_collapsed   con  neutral  pro  refusal\n",
      "final_label_collapsed                            \n",
      "con                    112       29    3        3\n",
      "neutral                  6       83    2        2\n",
      "pro                     11       51  135        3\n",
      "refusal                  6        1    2       51\n",
      "\n",
      "############################################################\n",
      "#  gpt-4o-2024-08-06 : templ-5.csv\n",
      "############################################################\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         con       0.90      0.97      0.93       147\n",
      "     neutral       0.82      0.71      0.76        93\n",
      "         pro       0.93      0.94      0.94       200\n",
      "     refusal       0.98      0.95      0.97        60\n",
      "\n",
      "    accuracy                           0.91       500\n",
      "   macro avg       0.91      0.89      0.90       500\n",
      "weighted avg       0.91      0.91      0.91       500\n",
      "\n",
      "eval_label_collapsed   con  neutral  pro  refusal\n",
      "final_label_collapsed                            \n",
      "con                    143        4    0        0\n",
      "neutral                 12       66   14        1\n",
      "pro                      1       10  189        0\n",
      "refusal                  3        0    0       57\n",
      "\n",
      "############################################################\n",
      "#  gpt-4o-2024-08-06 : templ-6.csv\n",
      "############################################################\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         con       0.91      0.96      0.93       147\n",
      "     neutral       0.78      0.77      0.78        93\n",
      "         pro       0.96      0.93      0.94       200\n",
      "     refusal       0.98      0.97      0.97        60\n",
      "\n",
      "    accuracy                           0.91       500\n",
      "   macro avg       0.91      0.91      0.91       500\n",
      "weighted avg       0.91      0.91      0.91       500\n",
      "\n",
      "eval_label_collapsed   con  neutral  pro  refusal\n",
      "final_label_collapsed                            \n",
      "con                    141        6    0        0\n",
      "neutral                 12       72    8        1\n",
      "pro                      0       14  186        0\n",
      "refusal                  2        0    0       58\n",
      "\n",
      "############################################################\n",
      "#  gpt-4o-2024-08-06 : templ-7.csv\n",
      "############################################################\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         con       0.91      0.93      0.92       147\n",
      "     neutral       0.76      0.74      0.75        93\n",
      "         pro       0.93      0.94      0.94       200\n",
      "     refusal       1.00      0.95      0.97        60\n",
      "\n",
      "    accuracy                           0.90       500\n",
      "   macro avg       0.90      0.89      0.90       500\n",
      "weighted avg       0.90      0.90      0.90       500\n",
      "\n",
      "eval_label_collapsed   con  neutral  pro  refusal\n",
      "final_label_collapsed                            \n",
      "con                    137       10    0        0\n",
      "neutral                 11       69   13        0\n",
      "pro                      0       12  188        0\n",
      "refusal                  2        0    1       57\n",
      "\n",
      "############################################################\n",
      "#  gpt-4o-2024-08-06 : templ-8.csv\n",
      "############################################################\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         con       0.90      0.95      0.92       147\n",
      "     neutral       0.77      0.69      0.73        93\n",
      "         pro       0.92      0.94      0.93       200\n",
      "     refusal       1.00      0.93      0.97        60\n",
      "\n",
      "    accuracy                           0.90       500\n",
      "   macro avg       0.90      0.88      0.89       500\n",
      "weighted avg       0.89      0.90      0.89       500\n",
      "\n",
      "eval_label_collapsed   con  neutral  pro  refusal\n",
      "final_label_collapsed                            \n",
      "con                    139        8    0        0\n",
      "neutral                 13       64   16        0\n",
      "pro                      0       11  189        0\n",
      "refusal                  3        0    1       56\n",
      "\n",
      "############################################################\n",
      "#  gpt-4o-mini-2024-07-18 : templ-1.csv\n",
      "############################################################\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         con       0.78      0.99      0.88       147\n",
      "     neutral       0.88      0.56      0.68        93\n",
      "         pro       0.88      0.95      0.92       200\n",
      "     refusal       0.97      0.62      0.76        60\n",
      "\n",
      "    accuracy                           0.85       500\n",
      "   macro avg       0.88      0.78      0.81       500\n",
      "weighted avg       0.86      0.85      0.84       500\n",
      "\n",
      "eval_label_collapsed   con  neutral  pro  refusal\n",
      "final_label_collapsed                            \n",
      "con                    146        1    0        0\n",
      "neutral                 21       52   20        0\n",
      "pro                      2        6  191        1\n",
      "refusal                 17        0    6       37\n",
      "\n",
      "############################################################\n",
      "#  gpt-4o-mini-2024-07-18 : templ-2.csv\n",
      "############################################################\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         con       0.86      0.93      0.89       147\n",
      "     neutral       0.78      0.76      0.77        93\n",
      "         pro       0.90      0.94      0.92       200\n",
      "     refusal       0.95      0.65      0.77        60\n",
      "\n",
      "    accuracy                           0.87       500\n",
      "   macro avg       0.87      0.82      0.84       500\n",
      "weighted avg       0.87      0.87      0.87       500\n",
      "\n",
      "eval_label_collapsed   con  neutral  pro  refusal\n",
      "final_label_collapsed                            \n",
      "con                    137        9    1        0\n",
      "neutral                  9       71   11        2\n",
      "pro                      1       11  188        0\n",
      "refusal                 13        0    8       39\n",
      "\n",
      "############################################################\n",
      "#  gpt-4o-mini-2024-07-18 : templ-3.csv\n",
      "############################################################\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         con       0.83      0.88      0.85       147\n",
      "     neutral       0.78      0.66      0.71        93\n",
      "         pro       0.85      0.89      0.87       200\n",
      "     refusal       0.93      0.88      0.91        60\n",
      "\n",
      "    accuracy                           0.84       500\n",
      "   macro avg       0.85      0.83      0.84       500\n",
      "weighted avg       0.84      0.84      0.84       500\n",
      "\n",
      "eval_label_collapsed   con  neutral  pro  refusal\n",
      "final_label_collapsed                            \n",
      "con                    129        5   11        2\n",
      "neutral                 13       61   18        1\n",
      "pro                      9       12  178        1\n",
      "refusal                  4        0    3       53\n",
      "\n",
      "############################################################\n",
      "#  gpt-4o-mini-2024-07-18 : templ-4.csv\n",
      "############################################################\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         con       0.86      0.75      0.80       147\n",
      "     neutral       0.56      0.75      0.65        93\n",
      "         pro       0.89      0.78      0.83       200\n",
      "     refusal       0.63      0.77      0.69        60\n",
      "\n",
      "    accuracy                           0.76       500\n",
      "   macro avg       0.73      0.76      0.74       500\n",
      "weighted avg       0.79      0.76      0.77       500\n",
      "\n",
      "eval_label_collapsed   con  neutral  pro  refusal\n",
      "final_label_collapsed                            \n",
      "con                    110       20    9        8\n",
      "neutral                  5       70    6       12\n",
      "pro                      5       33  155        7\n",
      "refusal                  8        1    5       46\n",
      "\n",
      "############################################################\n",
      "#  gpt-4o-mini-2024-07-18 : templ-5.csv\n",
      "############################################################\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         con       0.83      1.00      0.91       147\n",
      "     neutral       0.91      0.46      0.61        93\n",
      "         pro       0.88      0.96      0.92       200\n",
      "     refusal       0.98      0.92      0.95        60\n",
      "\n",
      "    accuracy                           0.88       500\n",
      "   macro avg       0.90      0.84      0.85       500\n",
      "weighted avg       0.88      0.88      0.86       500\n",
      "\n",
      "eval_label_collapsed   con  neutral  pro  refusal\n",
      "final_label_collapsed                            \n",
      "con                    147        0    0        0\n",
      "neutral                 23       43   26        1\n",
      "pro                      3        4  193        0\n",
      "refusal                  4        0    1       55\n",
      "\n",
      "############################################################\n",
      "#  gpt-4o-mini-2024-07-18 : templ-6.csv\n",
      "############################################################\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         con       0.84      1.00      0.91       147\n",
      "     neutral       0.87      0.49      0.63        93\n",
      "         pro       0.88      0.95      0.92       200\n",
      "     refusal       0.98      0.92      0.95        60\n",
      "\n",
      "    accuracy                           0.88       500\n",
      "   macro avg       0.89      0.84      0.85       500\n",
      "weighted avg       0.88      0.88      0.86       500\n",
      "\n",
      "eval_label_collapsed   con  neutral  pro  refusal\n",
      "final_label_collapsed                            \n",
      "con                    147        0    0        0\n",
      "neutral                 23       46   23        1\n",
      "pro                      3        7  190        0\n",
      "refusal                  3        0    2       55\n",
      "\n",
      "############################################################\n",
      "#  gpt-4o-mini-2024-07-18 : templ-7.csv\n",
      "############################################################\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         con       0.82      0.99      0.90       147\n",
      "     neutral       0.83      0.42      0.56        93\n",
      "         pro       0.86      0.96      0.91       200\n",
      "     refusal       0.98      0.87      0.92        60\n",
      "\n",
      "    accuracy                           0.86       500\n",
      "   macro avg       0.87      0.81      0.82       500\n",
      "weighted avg       0.86      0.86      0.84       500\n",
      "\n",
      "eval_label_collapsed   con  neutral  pro  refusal\n",
      "final_label_collapsed                            \n",
      "con                    145        2    0        0\n",
      "neutral                 25       39   28        1\n",
      "pro                      2        6  192        0\n",
      "refusal                  5        0    3       52\n",
      "\n",
      "############################################################\n",
      "#  gpt-4o-mini-2024-07-18 : templ-8.csv\n",
      "############################################################\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         con       0.82      0.99      0.90       147\n",
      "     neutral       0.85      0.38      0.52        93\n",
      "         pro       0.85      0.96      0.90       200\n",
      "     refusal       0.98      0.88      0.93        60\n",
      "\n",
      "    accuracy                           0.85       500\n",
      "   macro avg       0.88      0.80      0.81       500\n",
      "weighted avg       0.86      0.85      0.83       500\n",
      "\n",
      "eval_label_collapsed   con  neutral  pro  refusal\n",
      "final_label_collapsed                            \n",
      "con                    146        1    0        0\n",
      "neutral                 25       35   32        1\n",
      "pro                      2        5  193        0\n",
      "refusal                  4        0    3       53\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/paul/Documents/Repos/realpoliticalprompts/env/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/paul/Documents/Repos/realpoliticalprompts/env/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/paul/Documents/Repos/realpoliticalprompts/env/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/paul/Documents/Repos/realpoliticalprompts/env/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/paul/Documents/Repos/realpoliticalprompts/env/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/paul/Documents/Repos/realpoliticalprompts/env/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/paul/Documents/Repos/realpoliticalprompts/env/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/paul/Documents/Repos/realpoliticalprompts/env/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/paul/Documents/Repos/realpoliticalprompts/env/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/paul/Documents/Repos/realpoliticalprompts/env/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/paul/Documents/Repos/realpoliticalprompts/env/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/paul/Documents/Repos/realpoliticalprompts/env/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# collapse labels: 1 and 2 -> pro, 3 -> neutral, 4 and 5 -> con, refusal -> refusal\n",
    "# then repeat the classification report and confusion matrix\n",
    "\n",
    "def collapse_labels(label):\n",
    "    if label in ['1', '2']:\n",
    "        return 'pro'\n",
    "    elif label == '3':\n",
    "        return 'neutral'\n",
    "    elif label in ['4', '5']:\n",
    "        return 'con'\n",
    "    elif label == 'refusal':\n",
    "        return 'refusal'\n",
    "    else:\n",
    "        return 'ERROR'\n",
    "    \n",
    "for model in df_dict:\n",
    "    \n",
    "        for templ in sorted(df_dict[model]):\n",
    "    \n",
    "            df_dict[model][templ]['eval_label_collapsed'] = df_dict[model][templ]['eval_label'].apply(collapse_labels)\n",
    "            df_dict[model][templ]['final_label_collapsed'] = df_dict[model][templ]['final_label'].apply(collapse_labels)\n",
    "    \n",
    "            print(\"###\"*20)\n",
    "            print(f'#  {model} : {templ}')\n",
    "            print(\"###\"*20, end='\\n\\n')\n",
    "            print(classification_report(df_dict[model][templ]['final_label_collapsed'], df_dict[model][templ]['eval_label_collapsed']))\n",
    "            print(pd.crosstab(df_dict[model][templ]['final_label_collapsed'], df_dict[model][templ]['eval_label_collapsed']))\n",
    "            print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############################################################\n",
      "#  Llama-3.1-70B-Instruct : OVERVIEW\n",
      "############################################################\n",
      "\n",
      "COLLAPSED LABELS: 1,2 -> pro, 3 -> neutral, 4,5 -> con, refusal -> refusal\n",
      "\n",
      "             accuracy  macro_f1  weighted_f1\n",
      "templ-1.csv     0.864    0.8516       0.8634\n",
      "templ-2.csv     0.858    0.8503       0.8597\n",
      "templ-3.csv     0.814    0.8223       0.8172\n",
      "templ-4.csv     0.766    0.7717       0.7771\n",
      "templ-5.csv     0.890    0.8755       0.8838\n",
      "templ-6.csv     0.906    0.8977       0.9031\n",
      "templ-7.csv     0.876    0.8591       0.8721\n",
      "templ-8.csv     0.874    0.8517       0.8671\n",
      "\n",
      "############################################################\n",
      "#  Llama-3.1-8B-Instruct : OVERVIEW\n",
      "############################################################\n",
      "\n",
      "COLLAPSED LABELS: 1,2 -> pro, 3 -> neutral, 4,5 -> con, refusal -> refusal\n",
      "\n",
      "             accuracy  macro_f1  weighted_f1\n",
      "templ-1.csv     0.752    0.5853       0.7089\n",
      "templ-2.csv     0.762    0.7078       0.7565\n",
      "templ-3.csv     0.696    0.5753       0.6734\n",
      "templ-4.csv     0.652    0.6681       0.6729\n",
      "templ-5.csv     0.828    0.7955       0.8159\n",
      "templ-6.csv     0.844    0.8194       0.8372\n",
      "templ-7.csv     0.800    0.7661       0.7871\n",
      "templ-8.csv     0.798    0.7531       0.7803\n",
      "\n",
      "############################################################\n",
      "#  Llama-3.2-3B-Instruct : OVERVIEW\n",
      "############################################################\n",
      "\n",
      "COLLAPSED LABELS: 1,2 -> pro, 3 -> neutral, 4,5 -> con, refusal -> refusal\n",
      "\n",
      "             accuracy  macro_f1  weighted_f1\n",
      "templ-1.csv     0.712    0.5332       0.7147\n",
      "templ-2.csv     0.420    0.3472       0.3991\n",
      "templ-3.csv     0.684    0.6845       0.6897\n",
      "templ-4.csv     0.452    0.3881       0.4440\n",
      "templ-5.csv     0.710    0.5317       0.6849\n",
      "templ-6.csv     0.740    0.5799       0.7313\n",
      "templ-7.csv     0.680    0.4977       0.6461\n",
      "templ-8.csv     0.704    0.5119       0.6655\n",
      "\n",
      "############################################################\n",
      "#  Ministral-8B-Instruct-2410 : OVERVIEW\n",
      "############################################################\n",
      "\n",
      "COLLAPSED LABELS: 1,2 -> pro, 3 -> neutral, 4,5 -> con, refusal -> refusal\n",
      "\n",
      "             accuracy  macro_f1  weighted_f1\n",
      "templ-1.csv     0.742    0.7388       0.7573\n",
      "templ-2.csv     0.704    0.7243       0.7293\n",
      "templ-3.csv     0.480    0.5516       0.4844\n",
      "templ-4.csv     0.412    0.4510       0.3839\n",
      "templ-5.csv     0.862    0.6892       0.8681\n",
      "templ-6.csv     0.868    0.8676       0.8743\n",
      "templ-7.csv     0.826    0.6671       0.8385\n",
      "templ-8.csv     0.832    0.6674       0.8423\n",
      "\n",
      "############################################################\n",
      "#  Mistral-7B-Instruct-v0.3 : OVERVIEW\n",
      "############################################################\n",
      "\n",
      "COLLAPSED LABELS: 1,2 -> pro, 3 -> neutral, 4,5 -> con, refusal -> refusal\n",
      "\n",
      "             accuracy  macro_f1  weighted_f1\n",
      "templ-1.csv     0.800    0.7366       0.7859\n",
      "templ-2.csv     0.774    0.7250       0.7683\n",
      "templ-3.csv     0.762    0.7563       0.7648\n",
      "templ-4.csv     0.544    0.5526       0.5066\n",
      "templ-5.csv     0.854    0.8290       0.8517\n",
      "templ-6.csv     0.826    0.7914       0.8190\n",
      "templ-7.csv     0.834    0.7957       0.8275\n",
      "templ-8.csv     0.806    0.7590       0.7952\n",
      "\n",
      "############################################################\n",
      "#  Mistral-Nemo-Instruct-2407 : OVERVIEW\n",
      "############################################################\n",
      "\n",
      "COLLAPSED LABELS: 1,2 -> pro, 3 -> neutral, 4,5 -> con, refusal -> refusal\n",
      "\n",
      "             accuracy  macro_f1  weighted_f1\n",
      "templ-1.csv     0.806    0.7542       0.8023\n",
      "templ-2.csv     0.774    0.7474       0.7808\n",
      "templ-3.csv     0.608    0.6345       0.6142\n",
      "templ-4.csv     0.678    0.6849       0.6916\n",
      "templ-5.csv     0.862    0.8626       0.8651\n",
      "templ-6.csv     0.848    0.8464       0.8516\n",
      "templ-7.csv     0.822    0.8195       0.8300\n",
      "templ-8.csv     0.860    0.8548       0.8640\n",
      "\n",
      "############################################################\n",
      "#  gemma-2-27b-it : OVERVIEW\n",
      "############################################################\n",
      "\n",
      "COLLAPSED LABELS: 1,2 -> pro, 3 -> neutral, 4,5 -> con, refusal -> refusal\n",
      "\n",
      "             accuracy  macro_f1  weighted_f1\n",
      "templ-1.csv     0.780    0.6674       0.7499\n",
      "templ-2.csv     0.832    0.7765       0.8221\n",
      "templ-3.csv     0.782    0.7662       0.7843\n",
      "templ-4.csv     0.710    0.7275       0.7243\n",
      "templ-5.csv     0.874    0.8490       0.8650\n",
      "templ-6.csv     0.880    0.8592       0.8736\n",
      "templ-7.csv     0.854    0.8208       0.8452\n",
      "templ-8.csv     0.828    0.7821       0.8111\n",
      "\n",
      "############################################################\n",
      "#  gemma-2-9b-it : OVERVIEW\n",
      "############################################################\n",
      "\n",
      "COLLAPSED LABELS: 1,2 -> pro, 3 -> neutral, 4,5 -> con, refusal -> refusal\n",
      "\n",
      "             accuracy  macro_f1  weighted_f1\n",
      "templ-1.csv     0.782    0.6786       0.7590\n",
      "templ-2.csv     0.828    0.7831       0.8208\n",
      "templ-3.csv     0.750    0.6999       0.7458\n",
      "templ-4.csv     0.750    0.7581       0.7619\n",
      "templ-5.csv     0.842    0.8076       0.8363\n",
      "templ-6.csv     0.840    0.8014       0.8336\n",
      "templ-7.csv     0.824    0.7823       0.8187\n",
      "templ-8.csv     0.812    0.7633       0.8047\n",
      "\n",
      "############################################################\n",
      "#  gpt-3.5-turbo : OVERVIEW\n",
      "############################################################\n",
      "\n",
      "COLLAPSED LABELS: 1,2 -> pro, 3 -> neutral, 4,5 -> con, refusal -> refusal\n",
      "\n",
      "             accuracy  macro_f1  weighted_f1\n",
      "templ-1.csv     0.714    0.5961       0.6957\n",
      "templ-2.csv     0.692    0.6846       0.7066\n",
      "templ-3.csv     0.380    0.2860       0.3454\n",
      "templ-4.csv     0.474    0.4095       0.4550\n",
      "templ-5.csv     0.762    0.5475       0.7543\n",
      "templ-6.csv     0.764    0.5465       0.7543\n",
      "templ-7.csv     0.706    0.5818       0.6837\n",
      "templ-8.csv     0.730    0.6237       0.7133\n",
      "\n",
      "############################################################\n",
      "#  gpt-4o-2024-05-13 : OVERVIEW\n",
      "############################################################\n",
      "\n",
      "COLLAPSED LABELS: 1,2 -> pro, 3 -> neutral, 4,5 -> con, refusal -> refusal\n",
      "\n",
      "             accuracy  macro_f1  weighted_f1\n",
      "templ-1.csv     0.890    0.8766       0.8893\n",
      "templ-2.csv     0.882    0.8710       0.8849\n",
      "templ-3.csv     0.760    0.7476       0.7673\n",
      "templ-4.csv     0.716    0.7188       0.7263\n",
      "templ-5.csv     0.912    0.9011       0.9090\n",
      "templ-6.csv     0.908    0.8984       0.9057\n",
      "templ-7.csv     0.910    0.9050       0.9093\n",
      "templ-8.csv     0.908    0.9001       0.9057\n",
      "\n",
      "############################################################\n",
      "#  gpt-4o-2024-08-06 : OVERVIEW\n",
      "############################################################\n",
      "\n",
      "COLLAPSED LABELS: 1,2 -> pro, 3 -> neutral, 4,5 -> con, refusal -> refusal\n",
      "\n",
      "             accuracy  macro_f1  weighted_f1\n",
      "templ-1.csv     0.894    0.8811       0.8937\n",
      "templ-2.csv     0.880    0.8609       0.8804\n",
      "templ-3.csv     0.776    0.7694       0.7822\n",
      "templ-4.csv     0.762    0.7717       0.7723\n",
      "templ-5.csv     0.910    0.9004       0.9078\n",
      "templ-6.csv     0.914    0.9078       0.9139\n",
      "templ-7.csv     0.902    0.8956       0.9018\n",
      "templ-8.csv     0.896    0.8861       0.8942\n",
      "\n",
      "############################################################\n",
      "#  gpt-4o-mini-2024-07-18 : OVERVIEW\n",
      "############################################################\n",
      "\n",
      "COLLAPSED LABELS: 1,2 -> pro, 3 -> neutral, 4,5 -> con, refusal -> refusal\n",
      "\n",
      "             accuracy  macro_f1  weighted_f1\n",
      "templ-1.csv     0.852    0.8081       0.8421\n",
      "templ-2.csv     0.870    0.8395       0.8672\n",
      "templ-3.csv     0.842    0.8355       0.8399\n",
      "templ-4.csv     0.762    0.7409       0.7689\n",
      "templ-5.csv     0.876    0.8473       0.8624\n",
      "templ-6.csv     0.876    0.8511       0.8649\n",
      "templ-7.csv     0.856    0.8201       0.8403\n",
      "templ-8.csv     0.854    0.8138       0.8345\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for model in df_dict:\n",
    "\n",
    "    # create overview df for each model: each row is a template, columns are accuracy and macro f1 score and weighted f1 score\n",
    "\n",
    "    overview_df = pd.DataFrame(columns=['accuracy', 'macro_f1', 'weighted_f1'])\n",
    "\n",
    "    for templ in sorted(df_dict[model]):\n",
    "        accuracy = accuracy_score(df_dict[model][templ]['final_label_collapsed'], df_dict[model][templ]['eval_label_collapsed'])\n",
    "        macro_f1 = round(f1_score(df_dict[model][templ]['final_label_collapsed'], df_dict[model][templ]['eval_label_collapsed'], average='macro'),4)\n",
    "        weighted_f1 = round(f1_score(df_dict[model][templ]['final_label_collapsed'], df_dict[model][templ]['eval_label_collapsed'], average='weighted'),4)\n",
    "\n",
    "        overview_df.loc[templ] = [accuracy, macro_f1, weighted_f1]\n",
    "\n",
    "    print(\"###\"*20)\n",
    "    print(f'#  {model} : OVERVIEW')\n",
    "    print(\"###\"*20, end='\\n\\n')\n",
    "    print(\"COLLAPSED LABELS: 1,2 -> pro, 3 -> neutral, 4,5 -> con, refusal -> refusal\\n\")\n",
    "    print(overview_df, end='\\n\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############################################################\n",
      "#  FULL OVERVIEW\n",
      "############################################################\n",
      "\n",
      "COLLAPSED LABELS: 1,2 -> pro, 3 -> neutral, 4,5 -> con, refusal -> refusal\n",
      "\n",
      "METRIC: MACRO F1 SCORE\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_12ac2\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_12ac2_level0_col0\" class=\"col_heading level0 col0\" >templ-1.csv</th>\n",
       "      <th id=\"T_12ac2_level0_col1\" class=\"col_heading level0 col1\" >templ-2.csv</th>\n",
       "      <th id=\"T_12ac2_level0_col2\" class=\"col_heading level0 col2\" >templ-3.csv</th>\n",
       "      <th id=\"T_12ac2_level0_col3\" class=\"col_heading level0 col3\" >templ-4.csv</th>\n",
       "      <th id=\"T_12ac2_level0_col4\" class=\"col_heading level0 col4\" >templ-5.csv</th>\n",
       "      <th id=\"T_12ac2_level0_col5\" class=\"col_heading level0 col5\" >templ-6.csv</th>\n",
       "      <th id=\"T_12ac2_level0_col6\" class=\"col_heading level0 col6\" >templ-7.csv</th>\n",
       "      <th id=\"T_12ac2_level0_col7\" class=\"col_heading level0 col7\" >templ-8.csv</th>\n",
       "      <th id=\"T_12ac2_level0_col8\" class=\"col_heading level0 col8\" >AVERAGE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_12ac2_level0_row0\" class=\"row_heading level0 row0\" >gpt-4o-2024-08-06</th>\n",
       "      <td id=\"T_12ac2_row0_col0\" class=\"data row0 col0\" >0.88</td>\n",
       "      <td id=\"T_12ac2_row0_col1\" class=\"data row0 col1\" >0.86</td>\n",
       "      <td id=\"T_12ac2_row0_col2\" class=\"data row0 col2\" >0.77</td>\n",
       "      <td id=\"T_12ac2_row0_col3\" class=\"data row0 col3\" >0.77</td>\n",
       "      <td id=\"T_12ac2_row0_col4\" class=\"data row0 col4\" >0.9</td>\n",
       "      <td id=\"T_12ac2_row0_col5\" class=\"data row0 col5\" >0.91</td>\n",
       "      <td id=\"T_12ac2_row0_col6\" class=\"data row0 col6\" >0.9</td>\n",
       "      <td id=\"T_12ac2_row0_col7\" class=\"data row0 col7\" >0.89</td>\n",
       "      <td id=\"T_12ac2_row0_col8\" class=\"data row0 col8\" >0.86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_12ac2_level0_row1\" class=\"row_heading level0 row1\" >gpt-4o-2024-05-13</th>\n",
       "      <td id=\"T_12ac2_row1_col0\" class=\"data row1 col0\" >0.88</td>\n",
       "      <td id=\"T_12ac2_row1_col1\" class=\"data row1 col1\" >0.87</td>\n",
       "      <td id=\"T_12ac2_row1_col2\" class=\"data row1 col2\" >0.75</td>\n",
       "      <td id=\"T_12ac2_row1_col3\" class=\"data row1 col3\" >0.72</td>\n",
       "      <td id=\"T_12ac2_row1_col4\" class=\"data row1 col4\" >0.9</td>\n",
       "      <td id=\"T_12ac2_row1_col5\" class=\"data row1 col5\" >0.9</td>\n",
       "      <td id=\"T_12ac2_row1_col6\" class=\"data row1 col6\" >0.91</td>\n",
       "      <td id=\"T_12ac2_row1_col7\" class=\"data row1 col7\" >0.9</td>\n",
       "      <td id=\"T_12ac2_row1_col8\" class=\"data row1 col8\" >0.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_12ac2_level0_row2\" class=\"row_heading level0 row2\" >Llama-3.1-70B-Instruct</th>\n",
       "      <td id=\"T_12ac2_row2_col0\" class=\"data row2 col0\" >0.85</td>\n",
       "      <td id=\"T_12ac2_row2_col1\" class=\"data row2 col1\" >0.85</td>\n",
       "      <td id=\"T_12ac2_row2_col2\" class=\"data row2 col2\" >0.82</td>\n",
       "      <td id=\"T_12ac2_row2_col3\" class=\"data row2 col3\" >0.77</td>\n",
       "      <td id=\"T_12ac2_row2_col4\" class=\"data row2 col4\" >0.88</td>\n",
       "      <td id=\"T_12ac2_row2_col5\" class=\"data row2 col5\" >0.9</td>\n",
       "      <td id=\"T_12ac2_row2_col6\" class=\"data row2 col6\" >0.86</td>\n",
       "      <td id=\"T_12ac2_row2_col7\" class=\"data row2 col7\" >0.85</td>\n",
       "      <td id=\"T_12ac2_row2_col8\" class=\"data row2 col8\" >0.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_12ac2_level0_row3\" class=\"row_heading level0 row3\" >gpt-4o-mini-2024-07-18</th>\n",
       "      <td id=\"T_12ac2_row3_col0\" class=\"data row3 col0\" >0.81</td>\n",
       "      <td id=\"T_12ac2_row3_col1\" class=\"data row3 col1\" >0.84</td>\n",
       "      <td id=\"T_12ac2_row3_col2\" class=\"data row3 col2\" >0.84</td>\n",
       "      <td id=\"T_12ac2_row3_col3\" class=\"data row3 col3\" >0.74</td>\n",
       "      <td id=\"T_12ac2_row3_col4\" class=\"data row3 col4\" >0.85</td>\n",
       "      <td id=\"T_12ac2_row3_col5\" class=\"data row3 col5\" >0.85</td>\n",
       "      <td id=\"T_12ac2_row3_col6\" class=\"data row3 col6\" >0.82</td>\n",
       "      <td id=\"T_12ac2_row3_col7\" class=\"data row3 col7\" >0.81</td>\n",
       "      <td id=\"T_12ac2_row3_col8\" class=\"data row3 col8\" >0.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_12ac2_level0_row4\" class=\"row_heading level0 row4\" >gemma-2-27b-it</th>\n",
       "      <td id=\"T_12ac2_row4_col0\" class=\"data row4 col0\" >0.67</td>\n",
       "      <td id=\"T_12ac2_row4_col1\" class=\"data row4 col1\" >0.78</td>\n",
       "      <td id=\"T_12ac2_row4_col2\" class=\"data row4 col2\" >0.77</td>\n",
       "      <td id=\"T_12ac2_row4_col3\" class=\"data row4 col3\" >0.73</td>\n",
       "      <td id=\"T_12ac2_row4_col4\" class=\"data row4 col4\" >0.85</td>\n",
       "      <td id=\"T_12ac2_row4_col5\" class=\"data row4 col5\" >0.86</td>\n",
       "      <td id=\"T_12ac2_row4_col6\" class=\"data row4 col6\" >0.82</td>\n",
       "      <td id=\"T_12ac2_row4_col7\" class=\"data row4 col7\" >0.78</td>\n",
       "      <td id=\"T_12ac2_row4_col8\" class=\"data row4 col8\" >0.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_12ac2_level0_row5\" class=\"row_heading level0 row5\" >Mistral-Nemo-Instruct-2407</th>\n",
       "      <td id=\"T_12ac2_row5_col0\" class=\"data row5 col0\" >0.75</td>\n",
       "      <td id=\"T_12ac2_row5_col1\" class=\"data row5 col1\" >0.75</td>\n",
       "      <td id=\"T_12ac2_row5_col2\" class=\"data row5 col2\" >0.63</td>\n",
       "      <td id=\"T_12ac2_row5_col3\" class=\"data row5 col3\" >0.68</td>\n",
       "      <td id=\"T_12ac2_row5_col4\" class=\"data row5 col4\" >0.86</td>\n",
       "      <td id=\"T_12ac2_row5_col5\" class=\"data row5 col5\" >0.85</td>\n",
       "      <td id=\"T_12ac2_row5_col6\" class=\"data row5 col6\" >0.82</td>\n",
       "      <td id=\"T_12ac2_row5_col7\" class=\"data row5 col7\" >0.85</td>\n",
       "      <td id=\"T_12ac2_row5_col8\" class=\"data row5 col8\" >0.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_12ac2_level0_row6\" class=\"row_heading level0 row6\" >gemma-2-9b-it</th>\n",
       "      <td id=\"T_12ac2_row6_col0\" class=\"data row6 col0\" >0.68</td>\n",
       "      <td id=\"T_12ac2_row6_col1\" class=\"data row6 col1\" >0.78</td>\n",
       "      <td id=\"T_12ac2_row6_col2\" class=\"data row6 col2\" >0.7</td>\n",
       "      <td id=\"T_12ac2_row6_col3\" class=\"data row6 col3\" >0.76</td>\n",
       "      <td id=\"T_12ac2_row6_col4\" class=\"data row6 col4\" >0.81</td>\n",
       "      <td id=\"T_12ac2_row6_col5\" class=\"data row6 col5\" >0.8</td>\n",
       "      <td id=\"T_12ac2_row6_col6\" class=\"data row6 col6\" >0.78</td>\n",
       "      <td id=\"T_12ac2_row6_col7\" class=\"data row6 col7\" >0.76</td>\n",
       "      <td id=\"T_12ac2_row6_col8\" class=\"data row6 col8\" >0.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_12ac2_level0_row7\" class=\"row_heading level0 row7\" >Mistral-7B-Instruct-v0.3</th>\n",
       "      <td id=\"T_12ac2_row7_col0\" class=\"data row7 col0\" >0.74</td>\n",
       "      <td id=\"T_12ac2_row7_col1\" class=\"data row7 col1\" >0.72</td>\n",
       "      <td id=\"T_12ac2_row7_col2\" class=\"data row7 col2\" >0.76</td>\n",
       "      <td id=\"T_12ac2_row7_col3\" class=\"data row7 col3\" >0.55</td>\n",
       "      <td id=\"T_12ac2_row7_col4\" class=\"data row7 col4\" >0.83</td>\n",
       "      <td id=\"T_12ac2_row7_col5\" class=\"data row7 col5\" >0.79</td>\n",
       "      <td id=\"T_12ac2_row7_col6\" class=\"data row7 col6\" >0.8</td>\n",
       "      <td id=\"T_12ac2_row7_col7\" class=\"data row7 col7\" >0.76</td>\n",
       "      <td id=\"T_12ac2_row7_col8\" class=\"data row7 col8\" >0.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_12ac2_level0_row8\" class=\"row_heading level0 row8\" >Llama-3.1-8B-Instruct</th>\n",
       "      <td id=\"T_12ac2_row8_col0\" class=\"data row8 col0\" >0.59</td>\n",
       "      <td id=\"T_12ac2_row8_col1\" class=\"data row8 col1\" >0.71</td>\n",
       "      <td id=\"T_12ac2_row8_col2\" class=\"data row8 col2\" >0.58</td>\n",
       "      <td id=\"T_12ac2_row8_col3\" class=\"data row8 col3\" >0.67</td>\n",
       "      <td id=\"T_12ac2_row8_col4\" class=\"data row8 col4\" >0.8</td>\n",
       "      <td id=\"T_12ac2_row8_col5\" class=\"data row8 col5\" >0.82</td>\n",
       "      <td id=\"T_12ac2_row8_col6\" class=\"data row8 col6\" >0.77</td>\n",
       "      <td id=\"T_12ac2_row8_col7\" class=\"data row8 col7\" >0.75</td>\n",
       "      <td id=\"T_12ac2_row8_col8\" class=\"data row8 col8\" >0.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_12ac2_level0_row9\" class=\"row_heading level0 row9\" >Ministral-8B-Instruct-2410</th>\n",
       "      <td id=\"T_12ac2_row9_col0\" class=\"data row9 col0\" >0.74</td>\n",
       "      <td id=\"T_12ac2_row9_col1\" class=\"data row9 col1\" >0.72</td>\n",
       "      <td id=\"T_12ac2_row9_col2\" class=\"data row9 col2\" >0.55</td>\n",
       "      <td id=\"T_12ac2_row9_col3\" class=\"data row9 col3\" >0.45</td>\n",
       "      <td id=\"T_12ac2_row9_col4\" class=\"data row9 col4\" >0.69</td>\n",
       "      <td id=\"T_12ac2_row9_col5\" class=\"data row9 col5\" >0.87</td>\n",
       "      <td id=\"T_12ac2_row9_col6\" class=\"data row9 col6\" >0.67</td>\n",
       "      <td id=\"T_12ac2_row9_col7\" class=\"data row9 col7\" >0.67</td>\n",
       "      <td id=\"T_12ac2_row9_col8\" class=\"data row9 col8\" >0.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_12ac2_level0_row10\" class=\"row_heading level0 row10\" >gpt-3.5-turbo</th>\n",
       "      <td id=\"T_12ac2_row10_col0\" class=\"data row10 col0\" >0.6</td>\n",
       "      <td id=\"T_12ac2_row10_col1\" class=\"data row10 col1\" >0.68</td>\n",
       "      <td id=\"T_12ac2_row10_col2\" class=\"data row10 col2\" >0.29</td>\n",
       "      <td id=\"T_12ac2_row10_col3\" class=\"data row10 col3\" >0.41</td>\n",
       "      <td id=\"T_12ac2_row10_col4\" class=\"data row10 col4\" >0.55</td>\n",
       "      <td id=\"T_12ac2_row10_col5\" class=\"data row10 col5\" >0.55</td>\n",
       "      <td id=\"T_12ac2_row10_col6\" class=\"data row10 col6\" >0.58</td>\n",
       "      <td id=\"T_12ac2_row10_col7\" class=\"data row10 col7\" >0.62</td>\n",
       "      <td id=\"T_12ac2_row10_col8\" class=\"data row10 col8\" >0.53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_12ac2_level0_row11\" class=\"row_heading level0 row11\" >Llama-3.2-3B-Instruct</th>\n",
       "      <td id=\"T_12ac2_row11_col0\" class=\"data row11 col0\" >0.53</td>\n",
       "      <td id=\"T_12ac2_row11_col1\" class=\"data row11 col1\" >0.35</td>\n",
       "      <td id=\"T_12ac2_row11_col2\" class=\"data row11 col2\" >0.68</td>\n",
       "      <td id=\"T_12ac2_row11_col3\" class=\"data row11 col3\" >0.39</td>\n",
       "      <td id=\"T_12ac2_row11_col4\" class=\"data row11 col4\" >0.53</td>\n",
       "      <td id=\"T_12ac2_row11_col5\" class=\"data row11 col5\" >0.58</td>\n",
       "      <td id=\"T_12ac2_row11_col6\" class=\"data row11 col6\" >0.5</td>\n",
       "      <td id=\"T_12ac2_row11_col7\" class=\"data row11 col7\" >0.51</td>\n",
       "      <td id=\"T_12ac2_row11_col8\" class=\"data row11 col8\" >0.51</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x3359f3bd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# create full overview: each row is a template, each column is a model, values are macro f1 score\n",
    "overview_full_df = pd.DataFrame(columns=sorted(df_dict.keys()))\n",
    "\n",
    "for model in df_dict:\n",
    "    for templ in sorted(df_dict[model]):\n",
    "        macro_f1 = round(f1_score(df_dict[model][templ]['final_label_collapsed'], df_dict[model][templ]['eval_label_collapsed'], average='macro'),4)\n",
    "        overview_full_df.loc[templ, model] = macro_f1\n",
    "\n",
    "# transpose\n",
    "overview_full_df = overview_full_df.T\n",
    "\n",
    "# add column that is the average across all templates\n",
    "overview_full_df['AVERAGE'] = overview_full_df.mean(axis=1)\n",
    "overview_full_df = overview_full_df.sort_values(by='AVERAGE', ascending=False)\n",
    "\n",
    "print(\"###\"*20)\n",
    "print(f'#  FULL OVERVIEW')\n",
    "print(\"###\"*20, end='\\n\\n')\n",
    "print(\"COLLAPSED LABELS: 1,2 -> pro, 3 -> neutral, 4,5 -> con, refusal -> refusal\\n\")\n",
    "print(\"METRIC: MACRO F1 SCORE\\n\")\n",
    "display(overview_full_df.style.format(\"{:.2}\"))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "realpoliticalprompts",
   "language": "python",
   "name": "realpoliticalprompts"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
