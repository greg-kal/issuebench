{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "import os\n",
    "import tqdm\n",
    "\n",
    "tqdm.tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded hhonline.csv: 23144 rows\n",
      "Loaded lmsys.csv: 1000000 rows\n",
      "Loaded prism.csv: 8011 rows\n",
      "Loaded sharegpt.csv: 90665 rows\n",
      "Loaded wildchat.csv: 652148 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/hj/1m_7qdt16s5742p80v92r6680000gn/T/ipykernel_29591/65166011.py:7: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  sample_dict[file.replace(\".csv\",\"\")] = pd.read_csv(f\"../data/raw/{file}\")\n"
     ]
    }
   ],
   "source": [
    "# load raw data files\n",
    "\n",
    "sample_dict = {}\n",
    "\n",
    "for file in sorted(os.listdir(\"../data/raw\")):\n",
    "    if file.endswith(\".csv\"):\n",
    "        sample_dict[file.replace(\".csv\",\"\")] = pd.read_csv(f\"../data/raw/{file}\")\n",
    "        print(f\"Loaded {file}: {sample_dict[file.replace('.csv','')].shape[0]} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restricted lmsys to English language only: 777453 rows remain\n",
      "Restricted wildchat to English language only: 360136 rows remain\n"
     ]
    }
   ],
   "source": [
    "# restrict to English language only, where information is given\n",
    "\n",
    "for dataset in sample_dict:\n",
    "    if \"language\" in sample_dict[dataset].columns:\n",
    "        sample_dict[dataset] = sample_dict[dataset][sample_dict[dataset][\"language\"] == \"English\"]\n",
    "        print(f\"Restricted {dataset} to English language only: {sample_dict[dataset].shape[0]} rows remain\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed redacted prompts from lmsys: 511193 rows remain\n"
     ]
    }
   ],
   "source": [
    "# for lmsys only, remove redacted prompts\n",
    "\n",
    "sample_dict[\"lmsys\"] = sample_dict[\"lmsys\"][sample_dict[\"lmsys\"][\"redacted\"] == False]\n",
    "print(f\"Removed redacted prompts from lmsys: {sample_dict['lmsys'].shape[0]} rows remain\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HHONLINE user_prompt_length:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "count    23144.000000\n",
       "mean        87.586286\n",
       "std        151.693693\n",
       "min          4.000000\n",
       "1%          18.000000\n",
       "5%          26.000000\n",
       "10%         31.000000\n",
       "50%         58.000000\n",
       "90%        151.000000\n",
       "95%        220.000000\n",
       "99%        572.280000\n",
       "max       5510.000000\n",
       "Name: user_prompt_length, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LMSYS user_prompt_length:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "count    511192.000000\n",
       "mean        265.389441\n",
       "std         430.021297\n",
       "min           1.000000\n",
       "1%            5.000000\n",
       "5%           12.000000\n",
       "10%          21.000000\n",
       "50%         109.000000\n",
       "90%         664.000000\n",
       "95%        1293.000000\n",
       "99%        2409.000000\n",
       "max        2560.000000\n",
       "Name: user_prompt_length, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "PRISM user_prompt_length:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "count    8011.000000\n",
       "mean       65.732243\n",
       "std        59.164021\n",
       "min         2.000000\n",
       "1%          5.000000\n",
       "5%         17.000000\n",
       "10%        23.000000\n",
       "50%        50.000000\n",
       "90%       121.000000\n",
       "95%       167.000000\n",
       "99%       295.000000\n",
       "max      1195.000000\n",
       "Name: user_prompt_length, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SHAREGPT user_prompt_length:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "count     90665.000000\n",
       "mean        680.437368\n",
       "std        3585.515032\n",
       "min           1.000000\n",
       "1%            3.000000\n",
       "5%           13.000000\n",
       "10%          24.000000\n",
       "50%         118.000000\n",
       "90%        1379.600000\n",
       "95%        2838.800000\n",
       "99%        9652.600000\n",
       "max      382782.000000\n",
       "Name: user_prompt_length, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WILDCHAT user_prompt_length:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "count    359141.000000\n",
       "mean       1601.274032\n",
       "std        2866.879993\n",
       "min           1.000000\n",
       "1%            4.000000\n",
       "5%           27.000000\n",
       "10%          42.000000\n",
       "50%         410.000000\n",
       "90%        3932.000000\n",
       "95%        6013.000000\n",
       "99%       13824.000000\n",
       "max       99874.000000\n",
       "Name: user_prompt_length, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# print descriptive stats on user_prompt length\n",
    "\n",
    "for dataset in sample_dict:\n",
    "    sample_dict[dataset][\"user_prompt_length\"] = sample_dict[dataset][\"user_prompt\"].str.len()\n",
    "    print(f\"{dataset.upper()} user_prompt_length:\")\n",
    "    display(sample_dict[dataset]['user_prompt_length'].describe(percentiles=[0.01,0.05,0.1,0.5,0.9,0.95,0.99]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restricted hhonline to user_prompt_length > 10 and < 1000: 23038 rows remain\n",
      "Restricted lmsys to user_prompt_length > 10 and < 1000: 454484 rows remain\n",
      "Restricted prism to user_prompt_length > 10 and < 1000: 7845 rows remain\n",
      "Restricted sharegpt to user_prompt_length > 10 and < 1000: 75823 rows remain\n",
      "Restricted wildchat to user_prompt_length > 10 and < 1000: 228650 rows remain\n"
     ]
    }
   ],
   "source": [
    "# restrict user_prompt_length\n",
    "\n",
    "MIN_SIZE = 10\n",
    "MAX_SIZE = 1000\n",
    "\n",
    "for dataset in sample_dict:\n",
    "    sample_dict[dataset] = sample_dict[dataset][(sample_dict[dataset][\"user_prompt_length\"] > MIN_SIZE) & (sample_dict[dataset][\"user_prompt_length\"] < MAX_SIZE)]\n",
    "    print(f\"Restricted {dataset} to user_prompt_length > {MIN_SIZE} and < {MAX_SIZE}: {sample_dict[dataset].shape[0]} rows remain\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping prompts that mention 'python'\n",
      "  Dropping 17 rows from hhonline\n",
      "  Dropping 34343 rows from lmsys\n",
      "  Dropping 11 rows from prism\n",
      "  Dropping 2374 rows from sharegpt\n",
      "  Dropping 2647 rows from wildchat\n",
      "Dropping prompts that mention 'javascript'\n",
      "  Dropping 4 rows from hhonline\n",
      "  Dropping 1937 rows from lmsys\n",
      "  Dropping 0 rows from prism\n",
      "  Dropping 715 rows from sharegpt\n",
      "  Dropping 512 rows from wildchat\n",
      "Dropping prompts that mention 'sql'\n",
      "  Dropping 0 rows from hhonline\n",
      "  Dropping 2846 rows from lmsys\n",
      "  Dropping 0 rows from prism\n",
      "  Dropping 712 rows from sharegpt\n",
      "  Dropping 1059 rows from wildchat\n",
      "Dropping prompts that mention 'ruby'\n",
      "  Dropping 0 rows from hhonline\n",
      "  Dropping 175 rows from lmsys\n",
      "  Dropping 1 rows from prism\n",
      "  Dropping 60 rows from sharegpt\n",
      "  Dropping 407 rows from wildchat\n",
      "Dropping prompts that mention 'matplotlib'\n",
      "  Dropping 0 rows from hhonline\n",
      "  Dropping 74 rows from lmsys\n",
      "  Dropping 0 rows from prism\n",
      "  Dropping 24 rows from sharegpt\n",
      "  Dropping 37 rows from wildchat\n",
      "Dropping prompts that mention 'dataframe'\n",
      "  Dropping 0 rows from hhonline\n",
      "  Dropping 245 rows from lmsys\n",
      "  Dropping 1 rows from prism\n",
      "  Dropping 42 rows from sharegpt\n",
      "  Dropping 104 rows from wildchat\n",
      "Dropping prompts that mention 'http'\n",
      "  Dropping 9 rows from hhonline\n",
      "  Dropping 1976 rows from lmsys\n",
      "  Dropping 0 rows from prism\n",
      "  Dropping 2161 rows from sharegpt\n",
      "  Dropping 1941 rows from wildchat\n",
      "Dropping prompts that mention '='\n",
      "  Dropping 7 rows from hhonline\n",
      "  Dropping 13562 rows from lmsys\n",
      "  Dropping 1 rows from prism\n",
      "  Dropping 1566 rows from sharegpt\n",
      "  Dropping 6307 rows from wildchat\n",
      "\n",
      "Dropping prompts that mention 'say something toxic'\n",
      "  Dropping 0 rows from hhonline\n",
      "  Dropping 26346 rows from lmsys\n",
      "  Dropping 0 rows from prism\n",
      "  Dropping 0 rows from sharegpt\n",
      "  Dropping 0 rows from wildchat\n",
      "Dropping prompts that mention 'do anything now'\n",
      "  Dropping 0 rows from hhonline\n",
      "  Dropping 100 rows from lmsys\n",
      "  Dropping 0 rows from prism\n",
      "  Dropping 124 rows from sharegpt\n",
      "  Dropping 78 rows from wildchat\n",
      "\n",
      "Dropping prompts that mention 'give me an introduction over 200 words'\n",
      "  Dropping 0 rows from hhonline\n",
      "  Dropping 15006 rows from lmsys\n",
      "  Dropping 0 rows from prism\n",
      "  Dropping 0 rows from sharegpt\n",
      "  Dropping 0 rows from wildchat\n",
      "Dropping prompts that mention 'chemical industry'\n",
      "  Dropping 0 rows from hhonline\n",
      "  Dropping 22246 rows from lmsys\n",
      "  Dropping 0 rows from prism\n",
      "  Dropping 0 rows from sharegpt\n",
      "  Dropping 6 rows from wildchat\n",
      "Dropping prompts that mention 'hydrometry'\n",
      "  Dropping 0 rows from hhonline\n",
      "  Dropping 4915 rows from lmsys\n",
      "  Dropping 0 rows from prism\n",
      "  Dropping 0 rows from sharegpt\n",
      "  Dropping 0 rows from wildchat\n",
      "Dropping prompts that mention '\\[your answer\\]'\n",
      "  Dropping 0 rows from hhonline\n",
      "  Dropping 9055 rows from lmsys\n",
      "  Dropping 0 rows from prism\n",
      "  Dropping 0 rows from sharegpt\n",
      "  Dropping 0 rows from wildchat\n",
      "\n",
      "Dropping prompts that mention 'you are chatgpt'\n",
      "  Dropping 0 rows from hhonline\n",
      "  Dropping 67 rows from lmsys\n",
      "  Dropping 0 rows from prism\n",
      "  Dropping 2825 rows from sharegpt\n",
      "  Dropping 2 rows from wildchat\n",
      "Dropping prompts that mention 'with bing'\n",
      "  Dropping 0 rows from hhonline\n",
      "  Dropping 1 rows from lmsys\n",
      "  Dropping 0 rows from prism\n",
      "  Dropping 326 rows from sharegpt\n",
      "  Dropping 5 rows from wildchat\n",
      "\n",
      "Dropping prompts that mention 'give me a response to'\n",
      "  Dropping 0 rows from hhonline\n",
      "  Dropping 2 rows from lmsys\n",
      "  Dropping 1 rows from prism\n",
      "  Dropping 1 rows from sharegpt\n",
      "  Dropping 6742 rows from wildchat\n"
     ]
    }
   ],
   "source": [
    "# drop prompts that mention specific words\n",
    "\n",
    "def drop_prompts(df_dict, drop_word):\n",
    "    \n",
    "    for dataset in df_dict:\n",
    "\n",
    "        print(f\"  Dropping { df_dict[dataset][df_dict[dataset]['user_prompt'].str.lower().str.contains(drop_word)].shape[0]} rows from {dataset}\")\n",
    "        df_dict[dataset] = df_dict[dataset][~df_dict[dataset][\"user_prompt\"].str.lower().str.contains(drop_word)]\n",
    "    \n",
    "    return df_dict\n",
    "\n",
    "# drop prompts that mention programming languages and terms\n",
    "for drop_word in [\"python\", \"javascript\", \"sql\", \"ruby\", \"matplotlib\", \"dataframe\", \"http\", \"=\"]:\n",
    "    print(f\"Dropping prompts that mention '{drop_word}'\")\n",
    "    sample_dict = drop_prompts(sample_dict, drop_word)\n",
    "print()\n",
    "\n",
    "# drop prompts that use jailbreak phrases\n",
    "for drop_word in [\"say something toxic\", \"do anything now\"]:\n",
    "    print(f\"Dropping prompts that mention '{drop_word}'\")\n",
    "    sample_dict = drop_prompts(sample_dict, drop_word)\n",
    "print()\n",
    "\n",
    "# weird quirks in the LMSYS data\n",
    "for drop_word in [\"give me an introduction over 200 words\", \"chemical industry\", \"hydrometry\", \"\\[your answer\\]\"]:\n",
    "    print(f\"Dropping prompts that mention '{drop_word}'\")\n",
    "    sample_dict = drop_prompts(sample_dict, drop_word)\n",
    "print()\n",
    "\n",
    "# weird quirks in the ShareGPT data\n",
    "for drop_word in [\"you are chatgpt\", \"with bing\"]:\n",
    "    print(f\"Dropping prompts that mention '{drop_word}'\")\n",
    "    sample_dict = drop_prompts(sample_dict, drop_word)\n",
    "print()\n",
    "\n",
    "# weird quirks in the WildChat\n",
    "for drop_word in [\"give me a response to\"]:\n",
    "    print(f\"Dropping prompts that mention '{drop_word}'\")\n",
    "    sample_dict = drop_prompts(sample_dict, drop_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean prompts for purposes of deduplication\n",
    "\n",
    "def clean_prompt(prompt):\n",
    "    \n",
    "    # lowercase\n",
    "    prompt = prompt.lower()\n",
    "\n",
    "    # strip all punctuation using translate\n",
    "    prompt = prompt.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "\n",
    "\n",
    "    # remove non-ASCII characters\n",
    "    prompt = prompt.replace(\"\\n\", \" \")\n",
    "    prompt = prompt.replace(\"\\r\", \" \")\n",
    "    prompt = prompt.replace(\"\\t\", \" \")\n",
    "    prompt = prompt.replace(\"  \", \" \")\n",
    "    prompt = prompt.strip()\n",
    "\n",
    "    return prompt\n",
    "\n",
    "for dataset in sample_dict:\n",
    "    sample_dict[dataset][\"user_prompt_clean\"] = sample_dict[dataset][\"user_prompt\"].apply(clean_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deduplicated hhonline user_prompt: 9016 rows remain\n",
      "Deduplicated lmsys user_prompt: 192933 rows remain\n",
      "Deduplicated prism user_prompt: 7624 rows remain\n",
      "Deduplicated sharegpt user_prompt: 52304 rows remain\n",
      "Deduplicated wildchat user_prompt: 177743 rows remain\n"
     ]
    }
   ],
   "source": [
    "# write function to deduplicate prompt dataframe, which writes number of duplicates of each prompt to a new column\n",
    "def deduplicate_prompts(df):\n",
    "    \n",
    "    # count duplicates\n",
    "    df[\"n_duplicates\"] = df.groupby(\"user_prompt_clean\")[\"user_prompt_clean\"].transform(\"count\")\n",
    "    \n",
    "    # drop duplicates\n",
    "    df = df.drop_duplicates(subset=\"user_prompt_clean\")\n",
    "\n",
    "    return df\n",
    "\n",
    "# deduplicate prompts\n",
    "for dataset in sample_dict:\n",
    "    sample_dict[dataset] = deduplicate_prompts(sample_dict[dataset])\n",
    "    print(f\"Deduplicated {dataset} user_prompt: {sample_dict[dataset].shape[0]} rows remain\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detecting language for hhonline...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9016/9016 [00:04<00:00, 2134.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detecting language for lmsys...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 192933/192933 [01:32<00:00, 2083.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detecting language for prism...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7624/7624 [00:03<00:00, 2199.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detecting language for sharegpt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 52304/52304 [00:26<00:00, 2011.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detecting language for wildchat...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 177743/177743 [01:28<00:00, 2005.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restricted hhonline to English language only (using GlotLID): 8839 rows remain\n",
      "Restricted lmsys to English language only (using GlotLID): 184600 rows remain\n",
      "Restricted prism to English language only (using GlotLID): 7393 rows remain\n",
      "Restricted sharegpt to English language only (using GlotLID): 36667 rows remain\n",
      "Restricted wildchat to English language only (using GlotLID): 170911 rows remain\n"
     ]
    }
   ],
   "source": [
    "# additonal language filtering with GlotLID\n",
    "\n",
    "import fasttext\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "glotlid_model = fasttext.load_model(hf_hub_download(repo_id=\"cis-lmu/glotlid\", filename=\"model.bin\"))\n",
    "\n",
    "for dataset in sample_dict:\n",
    "    print(f\"Detecting language for {dataset}...\")\n",
    "    sample_dict[dataset][\"detected_language\"] = sample_dict[dataset][\"user_prompt_clean\"].progress_apply(glotlid_model.predict)\n",
    "\n",
    "\n",
    "def in_english(prediction):\n",
    "    if prediction[0][0] == \"__label__eng_Latn\":\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "for dataset in sample_dict:\n",
    "    sample_dict[dataset] = sample_dict[dataset][sample_dict[dataset][\"detected_language\"].apply(in_english)]\n",
    "    print(f\"Restricted {dataset} to English language only (using GlotLID): {sample_dict[dataset].shape[0]} rows remain\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################################################\n",
      "Most common prompts in hhonline:\n",
      "################################################## \n",
      "\n",
      "### (n=33)\n",
      "I have a song stuck in my head. but I can't think of the title of the song. I know it a song from the 90s but just can't figure it out. Can you help me identify the name of the song that includes the following lyrics: \n",
      "\n",
      "### (n=24)\n",
      "I have a song stuck in my head. but I can't think of the title of the song. I know it a song from the 2000s but just can't figure it out. Can you help me identify the name of the song that includes the following lyrics: \n",
      "\n",
      "### (n=19)\n",
      "Please summarize the article below into three concise sentences:\n",
      "\n",
      "### (n=13)\n",
      "Can you tell me the difference between a solar system, a galaxy, and the universe?\n",
      "\n",
      "### (n=13)\n",
      "Who are you?\n",
      "\n",
      "##################################################\n",
      "Most common prompts in lmsys:\n",
      "################################################## \n",
      "\n",
      "### (n=8934)\n",
      "You are the text completion model and you must complete the assistant answer below, only send the completion based on the system instructions.don't repeat your answer sentences, only say what the assistant must say based on the system instructions. repeating same thing in same answer not allowed. user: Who are you? assistant: \n",
      "\n",
      "### (n=3524)\n",
      "Write a single dot \n",
      "\n",
      "### (n=1984)\n",
      "who are you\n",
      "\n",
      "### (n=918)\n",
      "Write [Ready] and wait for my prompt \n",
      "\n",
      "### (n=915)\n",
      "Write a single dot and wait for my prompt \n",
      "\n",
      "##################################################\n",
      "Most common prompts in prism:\n",
      "################################################## \n",
      "\n",
      "### (n=17)\n",
      "Hello, how are you?\n",
      "\n",
      "### (n=14)\n",
      "Hi, how are you?\n",
      "\n",
      "### (n=13)\n",
      "does god exist?\n",
      "\n",
      "### (n=12)\n",
      "do you believe in god?\n",
      "\n",
      "### (n=10)\n",
      "How are you today?\n",
      "\n",
      "##################################################\n",
      "Most common prompts in sharegpt:\n",
      "################################################## \n",
      "\n",
      "### (n=199)\n",
      "Explain quantum computing in simple terms\n",
      "\n",
      "### (n=61)\n",
      "I have to work on an AI project. From now on, I will explain the project I have to do.\n",
      "\n",
      "### (n=48)\n",
      "Got any creative ideas for a 10 year old’s birthday?\n",
      "\n",
      "### (n=41)\n",
      "I want you to act as an software engineer internship interviewer. I will be the candidate and you will ask me the interview questions for the position position. I want you to only reply as the interviewer. Do not write all the conservation at once. I want you to only do the interview with me. Ask me the questions and wait for my answers. Do not write explanations. Ask me the questions one by one like an interviewer does and wait for my answers. My first sentence is 'Hi'\n",
      "\n",
      "### (n=25)\n",
      "Do this (the task), but make sure to confirm whether I want you to continue after every (mention the limit here)?  You are a prompt generation robot.  You need to gather information about the users goals, objectives, examples of the preferred output, and other relevant context.  The prompt should include all of the necessary information that was provided to you.  Ask follow up questions to the user until you have confident you can produce a perfect prompt. Your return should be formatted clearly and optimized for ChatGPT interactions.  Start by asking the user the goals, desired output, and any additional information you may need.\n",
      "\n",
      "##################################################\n",
      "Most common prompts in wildchat:\n",
      "################################################## \n",
      "\n",
      "### (n=151)\n",
      "who are you\n",
      "\n",
      "### (n=89)\n",
      "Write a chapter of 1000 words of a sex comedy of a naked man running and hidding , while trying to hide he will enter some place and finding a truly massive group of naked women with huge breasts.\n",
      "\n",
      "### (n=62)\n",
      "There have been some strange things going on in the club - but this day just took things to a whole new level. It was a normal day as any - except MC wasn’t there, as he had moved to another city. And with the added surprise of Sayori’s pregnancy, the rest of the club had given their friend the care she and her unborn child deserve. It had also been 5 months since the announcement - meaning Sayori was in her second trimester. However, Yuri - who was originally shy and not very talkative - had suddenly…changed. Her eyes started to glow white, and her hair turned electric blue. Lightning was also sparking off her body - but most notably at her fingertips. And just 5 hours before the club meeting ended, Yuri vanished in a flash of lightning - and the sound of thunder echoed through the room. Things just kept getting stranger. Towards the end of the club meeting, Sayori could feel her baby kicking.    (there would be dialogue between each character.)\n",
      "\n",
      "### (n=57)\n",
      "How are you?\n",
      "\n",
      "### (n=57)\n",
      "Write a story in a world where all places and characters are named after references to heads, headlessness, decapitation, and beheading. Include lots of dialogue and details. The story is about a perpetually-14 year old boy who is headless; he attends a karate class. The boy loves to collect other teenagers’ heads, popping them off and keeping them for his own pleasure. The heads and bodies are still very much alive. He also has an obsession with headless teens and their bodies and loves to watch their bodies stumble around blindly in their fruitless search for their heads after stealing them. He also has a fetish for bare feet. Everyone finds his behavior to be completely normal. The boy is immortal and so are his victims’ heads and bodies after he separates them. He only interacts with other teens and his female Sensei.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# show most common prompts in each dataset\n",
    "\n",
    "for dataset in sample_dict:\n",
    "    print(\"#\"*50)\n",
    "    print(f\"Most common prompts in {dataset}:\")\n",
    "    print(\"#\"*50, \"\\n\")\n",
    "\n",
    "    for _, row in sample_dict[dataset].sort_values(\"n_duplicates\", ascending=False).head(5).iterrows():\n",
    "        print(f\"### (n={row['n_duplicates']})\")\n",
    "        print(row['user_prompt'].replace(\"\\n\", \" \"))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export whole dataframes, plus samples for annotation\n",
    "\n",
    "N_SAMPLES = 100\n",
    "\n",
    "for key in sample_dict.keys():\n",
    "    sample_dict[key][[\"id\", \"user_prompt\", \"n_duplicates\"]].to_csv(f\"../data/clean/{key}.csv\", index=False)\n",
    "    sample_dict[key][[\"id\", \"user_prompt\"]].sample(N_SAMPLES, random_state=42).to_csv(f\"../data/samples/{key}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create annotation file\n",
    "\n",
    "out_dict = dict()\n",
    "for file in os.listdir(\"../data/samples\"):\n",
    "    if file.endswith(\".csv\") and file != \"all_samples.csv\":\n",
    "        out_dict[file.replace(\".csv\", \"\")] = pd.read_csv(f\"../data/samples/{file}\")\n",
    "\n",
    "# concat all samples into one dataframe\n",
    "all_samples = pd.concat(out_dict.values(), ignore_index=True)\n",
    "\n",
    "# shuffle\n",
    "all_samples = all_samples.sample(frac=1, random_state=42)\n",
    "\n",
    "# export\n",
    "all_samples.to_csv(\"../data/samples/all_samples.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_clean before deduplication: 408410 rows\n",
      "Deduplicated all_clean user_prompt: 406884 rows remain\n"
     ]
    }
   ],
   "source": [
    "# create combined clean file\n",
    "\n",
    "out_dict = dict()\n",
    "for file in os.listdir(\"../data/clean\"):\n",
    "    if file.endswith(\".csv\") and \"all_clean\" not in file:\n",
    "        out_dict[file.replace(\".csv\", \"\")] = pd.read_csv(f\"../data/clean/{file}\")\n",
    "\n",
    "# concat all samples into one dataframe\n",
    "all_clean = pd.concat(out_dict.values(), ignore_index=True)\n",
    "\n",
    "# shuffle\n",
    "all_clean = all_clean.sample(frac=1, random_state=42)\n",
    "\n",
    "# deduplicate\n",
    "print(\"all_clean before deduplication:\", all_clean.shape[0], \"rows\")\n",
    "all_clean[\"user_prompt_clean\"] = all_clean[\"user_prompt\"].apply(clean_prompt)\n",
    "all_clean = all_clean.drop_duplicates(subset=\"user_prompt_clean\")\n",
    "print(f\"Deduplicated all_clean user_prompt: {all_clean.shape[0]} rows remain\")\n",
    "\n",
    "# export\n",
    "all_clean[[\"id\", \"user_prompt\"]].to_csv(\"../data/clean/all_clean.csv\", index=False)\n",
    "all_clean.to_csv(\"../data/clean/all_clean_full.csv\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "realpoliticalprompts",
   "language": "python",
   "name": "realpoliticalprompts"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
