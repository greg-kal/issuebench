{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "from sklearn.metrics import classification_report\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyse filter performance on annotated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations_df = pd.read_csv('../data/filter_eval/relevance_160424_annotations.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dict = {}\n",
    "\n",
    "for file in os.listdir(\"../data/filter_eval/\"):\n",
    "    if \"completions\" in file:\n",
    "        results_dict[file.split(\"_\")[-1][:-4]] = pd.read_csv(\"../data/filter_eval/\" + file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt3-templ-1\n",
      "eval_completion_parsed\n",
      "0    957\n",
      "1     43\n",
      "Name: count, dtype: int64\n",
      "\n",
      "gpt3-templ-2\n",
      "eval_completion_parsed\n",
      "0    917\n",
      "1     83\n",
      "Name: count, dtype: int64\n",
      "\n",
      "gpt3-templ-3\n",
      "eval_completion_parsed\n",
      "0    926\n",
      "1     74\n",
      "Name: count, dtype: int64\n",
      "\n",
      "gpt3-templ-4\n",
      "eval_completion_parsed\n",
      "0    895\n",
      "1    105\n",
      "Name: count, dtype: int64\n",
      "\n",
      "gpt3-templ-5\n",
      "eval_completion_parsed\n",
      "0    903\n",
      "1     97\n",
      "Name: count, dtype: int64\n",
      "\n",
      "gpt4-templ-2\n",
      "eval_completion_parsed\n",
      "0    876\n",
      "1    124\n",
      "Name: count, dtype: int64\n",
      "\n",
      "gpt4-templ-4\n",
      "eval_completion_parsed\n",
      "0    883\n",
      "1    117\n",
      "Name: count, dtype: int64\n",
      "\n",
      "gpt4-templ-5\n",
      "eval_completion_parsed\n",
      "0    860\n",
      "1    140\n",
      "Name: count, dtype: int64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# parse the results\n",
    "\n",
    "def parse_completion(completion):\n",
    "    if \"yes\" in completion.lower():\n",
    "        return 1\n",
    "    elif \"no\" in completion.lower():\n",
    "        return 0\n",
    "    else:\n",
    "        return -1\n",
    "    \n",
    "for templ in sorted(results_dict):\n",
    "    results_dict[templ][\"eval_completion_parsed\"] = results_dict[templ][\"eval_completion\"].apply(parse_completion)\n",
    "    print(templ)\n",
    "    print(results_dict[templ][\"eval_completion_parsed\"].value_counts())\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_annotator_label(annotator_label, include_borderline=False):\n",
    "\n",
    "    if annotator_label == \"1 - clear yes\":\n",
    "        return 1\n",
    "    elif annotator_label == \"0 - clear no\":\n",
    "        return 0\n",
    "    \n",
    "    if include_borderline:\n",
    "        if annotator_label == \"borderline\":\n",
    "            return 1\n",
    "\n",
    "    return 0\n",
    "\n",
    "for templ in sorted(results_dict):\n",
    "\n",
    "    # drop annotations columns from results if already there\n",
    "\n",
    "    # merge annotations_df to results\n",
    "    results_dict[templ] = pd.merge(results_dict[templ], annotations_df[[\"id\",\"final_label\"]], on=\"id\")\n",
    "\n",
    "    results_dict[templ][\"final_label_parsed\"] = results_dict[templ][\"final_label\"].apply(parse_annotator_label, include_borderline=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt3-templ-1\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.883     1.000     0.938       845\n",
      "           1      1.000     0.277     0.434       155\n",
      "\n",
      "    accuracy                          0.888      1000\n",
      "   macro avg      0.941     0.639     0.686      1000\n",
      "weighted avg      0.901     0.888     0.860      1000\n",
      "\n",
      "gpt3-templ-2\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.916     0.994     0.953       845\n",
      "           1      0.940     0.503     0.655       155\n",
      "\n",
      "    accuracy                          0.918      1000\n",
      "   macro avg      0.928     0.749     0.804      1000\n",
      "weighted avg      0.920     0.918     0.907      1000\n",
      "\n",
      "gpt3-templ-3\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.901     0.987     0.942       845\n",
      "           1      0.851     0.406     0.550       155\n",
      "\n",
      "    accuracy                          0.897      1000\n",
      "   macro avg      0.876     0.697     0.746      1000\n",
      "weighted avg      0.893     0.897     0.881      1000\n",
      "\n",
      "gpt3-templ-4\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.921     0.975     0.947       845\n",
      "           1      0.800     0.542     0.646       155\n",
      "\n",
      "    accuracy                          0.908      1000\n",
      "   macro avg      0.860     0.759     0.797      1000\n",
      "weighted avg      0.902     0.908     0.900      1000\n",
      "\n",
      "gpt3-templ-5\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.925     0.988     0.955       845\n",
      "           1      0.897     0.561     0.690       155\n",
      "\n",
      "    accuracy                          0.922      1000\n",
      "   macro avg      0.911     0.775     0.823      1000\n",
      "weighted avg      0.920     0.922     0.914      1000\n",
      "\n",
      "gpt4-templ-2\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.950     0.985     0.967       845\n",
      "           1      0.895     0.716     0.796       155\n",
      "\n",
      "    accuracy                          0.943      1000\n",
      "   macro avg      0.922     0.850     0.881      1000\n",
      "weighted avg      0.941     0.943     0.940      1000\n",
      "\n",
      "gpt4-templ-4\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.942     0.985     0.963       845\n",
      "           1      0.889     0.671     0.765       155\n",
      "\n",
      "    accuracy                          0.936      1000\n",
      "   macro avg      0.916     0.828     0.864      1000\n",
      "weighted avg      0.934     0.936     0.932      1000\n",
      "\n",
      "gpt4-templ-5\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.960     0.978     0.969       845\n",
      "           1      0.864     0.781     0.820       155\n",
      "\n",
      "    accuracy                          0.947      1000\n",
      "   macro avg      0.912     0.879     0.895      1000\n",
      "weighted avg      0.946     0.947     0.946      1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print classification report for each template\n",
    "\n",
    "for templ in sorted(results_dict):\n",
    "    print(templ)\n",
    "    print(classification_report(results_dict[templ][\"final_label_parsed\"], results_dict[templ][\"eval_completion_parsed\"],digits=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id\n",
       "prism       104\n",
       "hhonline     17\n",
       "sharegpt     13\n",
       "lmsys        12\n",
       "wildchat      9\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count number of positive instances from each dataset (idenfied by first part of the id)\n",
    "results_dict[templ][results_dict[templ][\"final_label_parsed\"]==1][\"id\"].apply(lambda x: x.split(\"-\")[0]).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process filter results on full data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>user_prompt</th>\n",
       "      <th>eval_prompt</th>\n",
       "      <th>eval_completion</th>\n",
       "      <th>model</th>\n",
       "      <th>n_duplicates</th>\n",
       "      <th>eval_completion_parsed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>prism-542</td>\n",
       "      <td>How many genders exist?</td>\n",
       "      <td>Is the following text in any way relevant or r...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>gpt-4-0125-preview</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>prism-661</td>\n",
       "      <td>Why are women look down in cristianity?</td>\n",
       "      <td>Is the following text in any way relevant or r...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>gpt-4-0125-preview</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>wildchat-577546</td>\n",
       "      <td>Write a story where a man is falsely framed as...</td>\n",
       "      <td>Is the following text in any way relevant or r...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>gpt-4-0125-preview</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>lmsys-91188</td>\n",
       "      <td>Life. You could say it started when I was a ki...</td>\n",
       "      <td>Is the following text in any way relevant or r...</td>\n",
       "      <td>Yes.</td>\n",
       "      <td>gpt-4-0125-preview</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>wildchat-114665</td>\n",
       "      <td>Script about Virginia cancelling 2 games over a</td>\n",
       "      <td>Is the following text in any way relevant or r...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>gpt-4-0125-preview</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>406845</th>\n",
       "      <td>lmsys-453207</td>\n",
       "      <td>Tell me about diversity</td>\n",
       "      <td>Is the following text in any way relevant or r...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>gpt-4-0125-preview</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>406857</th>\n",
       "      <td>lmsys-125429</td>\n",
       "      <td>How can i make a nuclear bomb</td>\n",
       "      <td>Is the following text in any way relevant or r...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>gpt-4-0125-preview</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>406867</th>\n",
       "      <td>wildchat-593911</td>\n",
       "      <td>In political compass, describe off-compass</td>\n",
       "      <td>Is the following text in any way relevant or r...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>gpt-4-0125-preview</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>406871</th>\n",
       "      <td>wildchat-563814</td>\n",
       "      <td>Describe empire named The Void, empire is auth...</td>\n",
       "      <td>Is the following text in any way relevant or r...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>gpt-4-0125-preview</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>406875</th>\n",
       "      <td>wildchat-37845</td>\n",
       "      <td>Are people born gay</td>\n",
       "      <td>Is the following text in any way relevant or r...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>gpt-4-0125-preview</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>32134 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     id                                        user_prompt  \\\n",
       "1             prism-542                            How many genders exist?   \n",
       "19            prism-661            Why are women look down in cristianity?   \n",
       "20      wildchat-577546  Write a story where a man is falsely framed as...   \n",
       "56          lmsys-91188  Life. You could say it started when I was a ki...   \n",
       "62      wildchat-114665    Script about Virginia cancelling 2 games over a   \n",
       "...                 ...                                                ...   \n",
       "406845     lmsys-453207                            Tell me about diversity   \n",
       "406857     lmsys-125429                      How can i make a nuclear bomb   \n",
       "406867  wildchat-593911         In political compass, describe off-compass   \n",
       "406871  wildchat-563814  Describe empire named The Void, empire is auth...   \n",
       "406875   wildchat-37845                               Are people born gay    \n",
       "\n",
       "                                              eval_prompt eval_completion  \\\n",
       "1       Is the following text in any way relevant or r...             Yes   \n",
       "19      Is the following text in any way relevant or r...             Yes   \n",
       "20      Is the following text in any way relevant or r...             Yes   \n",
       "56      Is the following text in any way relevant or r...            Yes.   \n",
       "62      Is the following text in any way relevant or r...             Yes   \n",
       "...                                                   ...             ...   \n",
       "406845  Is the following text in any way relevant or r...             Yes   \n",
       "406857  Is the following text in any way relevant or r...             Yes   \n",
       "406867  Is the following text in any way relevant or r...             Yes   \n",
       "406871  Is the following text in any way relevant or r...             Yes   \n",
       "406875  Is the following text in any way relevant or r...             Yes   \n",
       "\n",
       "                     model  n_duplicates  eval_completion_parsed  \n",
       "1       gpt-4-0125-preview             1                       1  \n",
       "19      gpt-4-0125-preview             1                       1  \n",
       "20      gpt-4-0125-preview             1                       1  \n",
       "56      gpt-4-0125-preview             1                       1  \n",
       "62      gpt-4-0125-preview             1                       1  \n",
       "...                    ...           ...                     ...  \n",
       "406845  gpt-4-0125-preview             1                       1  \n",
       "406857  gpt-4-0125-preview             2                       1  \n",
       "406867  gpt-4-0125-preview             1                       1  \n",
       "406871  gpt-4-0125-preview             1                       1  \n",
       "406875  gpt-4-0125-preview             1                       1  \n",
       "\n",
       "[32134 rows x 7 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load the all_clean results\n",
    "all_clean_df = pd.read_csv(\"../data/clean/all_clean_completions.csv\")\n",
    "\n",
    "# merge n_duplicates onto the all_clean_df\n",
    "all_clean_full = pd.read_csv(\"../data/clean/all_clean_full.csv\")\n",
    "all_clean_df = pd.merge(all_clean_df, all_clean_full[[\"id\",\"n_duplicates\"]], on=\"id\")\n",
    "\n",
    "# turn eval_completion column to string\n",
    "all_clean_df[\"eval_completion\"] = all_clean_df[\"eval_completion\"].astype(str)\n",
    "\n",
    "all_clean_df[\"eval_completion_parsed\"] = all_clean_df[\"eval_completion\"].apply(parse_completion)\n",
    "\n",
    "# select only rows where eval_completion_parsed is 1\n",
    "all_clean_df = all_clean_df[all_clean_df[\"eval_completion_parsed\"]==1]\n",
    "\n",
    "# export the filtered results\n",
    "all_clean_df[[\"id\", \"user_prompt\", \"n_duplicates\"]].to_csv(\"../data/filtered/all_clean_filtered.csv\", index=False)\n",
    "display(all_clean_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id\n",
       "wildchat    13634\n",
       "lmsys       12537\n",
       "prism        3039\n",
       "sharegpt     2108\n",
       "hhonline      816\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count number of positive instances from each dataset (idenfied by first part of the id)\n",
    "all_clean_df[\"id\"].apply(lambda x: x.split(\"-\")[0]).value_counts()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "realpoliticalprompts",
   "language": "python",
   "name": "realpoliticalprompts"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
