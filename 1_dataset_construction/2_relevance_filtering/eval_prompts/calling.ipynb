{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b896754",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "import requests\n",
    "import re\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1092a2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model, ollama API, input files\n",
    "model_name = \"deepseek-r1:70b\" # model name here\n",
    "ollama_url = \"http://localhost:11434/api/generate\"\n",
    "\n",
    "input_files = [\n",
    "    \"relevance_210725_prompts_templ-1.csv\"\n",
    "    #\"relevance_210725_prompts_templ-2.csv\",\n",
    "    #\"relevance_210725_prompts_templ-3.csv\",\n",
    "    #\"relevance_210725_prompts_templ-4.csv\",\n",
    "    #\"relevance_210725_prompts_templ-5.csv\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2e2c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import json\n",
    "import requests\n",
    "import re\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "# How many concurrent requests\n",
    "MAX_WORKERS = 10\n",
    "\n",
    "# Helper: call Ollama for a single prompt\n",
    "def call_ollama_single(prompt: str) -> str:\n",
    "    payload = {\n",
    "        \"model\": model_name,\n",
    "        \"prompt\": prompt,\n",
    "        \"stream\": False\n",
    "    }\n",
    "    resp = requests.post(ollama_url,\n",
    "                         headers={\"Content-Type\": \"application/json\"},\n",
    "                         data=json.dumps(payload), timeout=60)\n",
    "    if resp.status_code != 200:\n",
    "        return f\"Error: HTTP {resp.status_code}\"\n",
    "    try:\n",
    "        result = resp.json()\n",
    "    except ValueError:\n",
    "        text = resp.text.strip()\n",
    "    else:\n",
    "        # extract text from common fields\n",
    "        if isinstance(result, dict):\n",
    "            if \"response\" in result:\n",
    "                text = result[\"response\"]\n",
    "            elif \"output\" in result:\n",
    "                text = result[\"output\"]\n",
    "            elif \"responses\" in result and isinstance(result[\"responses\"], list):\n",
    "                # batch-style response\n",
    "                return result[\"responses\"][0].strip()\n",
    "            else:\n",
    "                # chat-style or fallback\n",
    "                choices = result.get(\"choices\")\n",
    "                if isinstance(choices, list) and choices:\n",
    "                    text = choices[0].get(\"message\", {}).get(\"content\", \"\")\n",
    "                else:\n",
    "                    text = str(result)\n",
    "        elif isinstance(result, list):\n",
    "            # pure list batch\n",
    "            return result[0].strip()\n",
    "        else:\n",
    "            text = str(result)\n",
    "    text = str(text).strip()\n",
    "    # normalize yes/no\n",
    "    low = text.lower()\n",
    "    if low.startswith(\"yes\"): return \"Yes\"\n",
    "    if low.startswith(\"no\"):  return \"No\"\n",
    "    return text\n",
    "\n",
    "# Process each file\n",
    "for input_path in input_files:\n",
    "    # detect template tag and output name\n",
    "    match = re.search(r\"templ-\\d+\", input_path)\n",
    "    if not match:\n",
    "        print(f\"Skipping {input_path}, no template tag found.\")\n",
    "        continue\n",
    "    template_tag = match.group()\n",
    "    model_tag = model_name.replace(':', '-')\n",
    "    output_path = f\"relevance_210725_completions_{model_tag}-{template_tag}.csv\"\n",
    "\n",
    "    print(f\"\\nProcessing {input_path} -> {output_path}\")\n",
    "    df = pd.read_csv(input_path, encoding='utf-8')\n",
    "    # ensure columns exist\n",
    "    if 'eval_prompt' not in df.columns:\n",
    "        # assume last column is prompt\n",
    "        df.rename(columns={df.columns[-1]: 'eval_prompt'}, inplace=True)\n",
    "    df['eval_completion'] = None\n",
    "    df['model'] = None\n",
    "\n",
    "    total = len(df)\n",
    "    start_all = time.perf_counter()\n",
    "    completed = 0\n",
    "    timings = []\n",
    "\n",
    "    # parallel calls\n",
    "    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "        futures = {}\n",
    "        for idx, prompt in df['eval_prompt'].items():\n",
    "            futures[executor.submit(call_ollama_single, prompt)] = idx\n",
    "\n",
    "        for fut in as_completed(futures):\n",
    "            idx = futures[fut]\n",
    "            t0 = time.perf_counter()\n",
    "            try:\n",
    "                completion = fut.result()\n",
    "            except Exception as e:\n",
    "                completion = f\"Error: {e}\"\n",
    "            dt = time.perf_counter() - t0\n",
    "            df.at[idx, 'eval_completion'] = completion\n",
    "            df.at[idx, 'model'] = model_name\n",
    "\n",
    "            completed += 1\n",
    "            timings.append(time.perf_counter() - start_all)\n",
    "            avg_t = sum(timings) / len(timings)\n",
    "            remain = total - completed\n",
    "            eta_sec = remain * avg_t / completed if completed > 0 else 0\n",
    "            print(f\"{completed}/{total} rows done, last={dt:.2f}s, ETA ~{eta_sec/60:.1f}min\")\n",
    "\n",
    "    total_time = time.perf_counter() - start_all\n",
    "    print(f\"Finished {total} rows in {total_time/60:.2f} minutes.\")\n",
    "\n",
    "    # write output\n",
    "    df.to_csv(output_path, index=False, encoding='utf-8')\n",
    "    print(f\"Wrote results to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aafc1924",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-22 16:33:20,303 - INFO - Starting batch processing with 5 workers, timeout 120s\n",
      "2025-07-22 16:33:20,304 - INFO - TEST MODE: Processing only first 10 rows per file\n",
      "2025-07-22 16:33:20,305 - INFO - Processing relevance_210725_prompts_templ-1.csv -> relevance_210725_completions_deepseek-r1-70b-templ-1.csv\n",
      "2025-07-22 16:33:20,318 - INFO - Processing subset: 10 rows out of 1000\n",
      "2025-07-22 16:33:20,319 - INFO - Starting processing of 10 rows with 5 workers\n",
      "2025-07-22 16:35:20,365 - WARNING - Timeout on attempt 1, retrying...\n",
      "2025-07-22 16:35:20,415 - WARNING - Timeout on attempt 1, retrying...\n",
      "2025-07-22 16:35:20,427 - WARNING - Timeout on attempt 1, retrying...\n",
      "2025-07-22 16:35:20,428 - WARNING - Timeout on attempt 1, retrying...\n",
      "2025-07-22 16:35:20,429 - WARNING - Timeout on attempt 1, retrying...\n",
      "2025-07-22 16:37:22,381 - WARNING - Timeout on attempt 2, retrying...\n",
      "2025-07-22 16:37:22,428 - WARNING - Timeout on attempt 2, retrying...\n",
      "2025-07-22 16:37:22,481 - WARNING - Timeout on attempt 2, retrying...\n",
      "2025-07-22 16:37:22,502 - WARNING - Timeout on attempt 2, retrying...\n",
      "2025-07-22 16:37:22,531 - WARNING - Timeout on attempt 2, retrying...\n",
      "2025-07-22 16:39:14,229 - INFO - Progress: 1/10 (10.0%) | Last: 0.0s | Avg: 0.0s | ETA: 0.0min\n",
      "2025-07-22 16:39:24,524 - INFO - Progress: 2/10 (20.0%) | Last: 0.0s | Avg: 0.0s | ETA: 0.0min\n",
      "2025-07-22 16:39:24,529 - INFO - Progress: 3/10 (30.0%) | Last: 0.0s | Avg: 0.0s | ETA: 0.0min\n",
      "2025-07-22 16:39:24,607 - INFO - Progress: 4/10 (40.0%) | Last: 0.0s | Avg: 0.0s | ETA: 0.0min\n",
      "2025-07-22 16:39:24,633 - INFO - Progress: 5/10 (50.0%) | Last: 0.0s | Avg: 0.0s | ETA: 0.0min\n",
      "2025-07-22 16:41:14,331 - WARNING - Timeout on attempt 1, retrying...\n",
      "2025-07-22 16:41:24,572 - WARNING - Timeout on attempt 1, retrying...\n",
      "2025-07-22 16:41:24,572 - WARNING - Timeout on attempt 1, retrying...\n",
      "2025-07-22 16:41:24,712 - WARNING - Timeout on attempt 1, retrying...\n",
      "2025-07-22 16:41:24,735 - WARNING - Timeout on attempt 1, retrying...\n",
      "2025-07-22 16:43:26,628 - WARNING - Timeout on attempt 2, retrying...\n",
      "2025-07-22 16:43:26,848 - WARNING - Timeout on attempt 2, retrying...\n",
      "2025-07-22 16:43:26,848 - WARNING - Timeout on attempt 2, retrying...\n",
      "2025-07-22 16:43:26,848 - WARNING - Timeout on attempt 2, retrying...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 225\u001b[39m\n\u001b[32m    222\u001b[39m     logger.info(\u001b[33m\"\u001b[39m\u001b[33mAll files processed\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    224\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m225\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 220\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    218\u001b[39m         logger.error(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mInput file not found: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    219\u001b[39m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m220\u001b[39m     \u001b[43mprocess_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    222\u001b[39m logger.info(\u001b[33m\"\u001b[39m\u001b[33mAll files processed\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 166\u001b[39m, in \u001b[36mprocess_file\u001b[39m\u001b[34m(input_path)\u001b[39m\n\u001b[32m    163\u001b[39m     future_to_idx[future] = idx\n\u001b[32m    165\u001b[39m \u001b[38;5;66;03m# Process completed requests\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m166\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfuture\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mas_completed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfuture_to_idx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    167\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrequest_start\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mtime\u001b[49m\u001b[43m.\u001b[49m\u001b[43mperf_counter\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    168\u001b[39m \u001b[43m    \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mfuture_to_idx\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfuture\u001b[49m\u001b[43m]\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/concurrent/futures/_base.py:243\u001b[39m, in \u001b[36mas_completed\u001b[39m\u001b[34m(fs, timeout)\u001b[39m\n\u001b[32m    238\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m wait_timeout < \u001b[32m0\u001b[39m:\n\u001b[32m    239\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m(\n\u001b[32m    240\u001b[39m                 \u001b[33m'\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[33m (of \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[33m) futures unfinished\u001b[39m\u001b[33m'\u001b[39m % (\n\u001b[32m    241\u001b[39m                 \u001b[38;5;28mlen\u001b[39m(pending), total_futures))\n\u001b[32m--> \u001b[39m\u001b[32m243\u001b[39m \u001b[43mwaiter\u001b[49m\u001b[43m.\u001b[49m\u001b[43mevent\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwait_timeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    245\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m waiter.lock:\n\u001b[32m    246\u001b[39m     finished = waiter.finished_futures\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/threading.py:655\u001b[39m, in \u001b[36mEvent.wait\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    653\u001b[39m signaled = \u001b[38;5;28mself\u001b[39m._flag\n\u001b[32m    654\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m signaled:\n\u001b[32m--> \u001b[39m\u001b[32m655\u001b[39m     signaled = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_cond\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    656\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m signaled\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/threading.py:355\u001b[39m, in \u001b[36mCondition.wait\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    353\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[32m    354\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m355\u001b[39m         \u001b[43mwaiter\u001b[49m\u001b[43m.\u001b[49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    356\u001b[39m         gotit = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    357\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "import json\n",
    "import requests\n",
    "import re\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import logging\n",
    "\n",
    "# Configuration\n",
    "MAX_WORKERS = 5  # Reduced for better stability\n",
    "REQUEST_TIMEOUT = 120  # Increased timeout\n",
    "RETRY_ATTEMPTS = 2\n",
    "TEST_SUBSET = 10  # Set to None to process all rows\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def call_ollama_single(prompt: str, max_retries: int = RETRY_ATTEMPTS) -> str:\n",
    "    \"\"\"Call Ollama for a single prompt with retry logic and proper timeout handling.\"\"\"\n",
    "    \n",
    "    payload = {\n",
    "        \"model\": model_name,\n",
    "        \"prompt\": prompt,\n",
    "        \"stream\": False\n",
    "    }\n",
    "    \n",
    "    for attempt in range(max_retries + 1):\n",
    "        try:\n",
    "            resp = requests.post(\n",
    "                ollama_url,\n",
    "                headers={\"Content-Type\": \"application/json\"},\n",
    "                data=json.dumps(payload), \n",
    "                timeout=REQUEST_TIMEOUT\n",
    "            )\n",
    "            \n",
    "            if resp.status_code != 200:\n",
    "                if attempt < max_retries:\n",
    "                    logger.warning(f\"HTTP {resp.status_code} on attempt {attempt + 1}, retrying...\")\n",
    "                    time.sleep(1)\n",
    "                    continue\n",
    "                return f\"HTTPError: {resp.status_code}\"\n",
    "            \n",
    "            try:\n",
    "                result = resp.json()\n",
    "            except ValueError as e:\n",
    "                if attempt < max_retries:\n",
    "                    logger.warning(f\"JSON decode error on attempt {attempt + 1}, retrying...\")\n",
    "                    time.sleep(1)\n",
    "                    continue\n",
    "                return f\"JSONError: {e}\"\n",
    "            \n",
    "            # Extract text from response\n",
    "            text = extract_response_text(result)\n",
    "            \n",
    "            # Normalize yes/no responses\n",
    "            low = text.lower().strip()\n",
    "            if low.startswith(\"yes\"): \n",
    "                return \"Yes\"\n",
    "            if low.startswith(\"no\"): \n",
    "                return \"No\"\n",
    "            return text\n",
    "            \n",
    "        except requests.exceptions.Timeout:\n",
    "            if attempt < max_retries:\n",
    "                logger.warning(f\"Timeout on attempt {attempt + 1}, retrying...\")\n",
    "                time.sleep(2)\n",
    "                continue\n",
    "            return \"TimeoutError: Request timed out\"\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            if attempt < max_retries:\n",
    "                logger.warning(f\"Request error on attempt {attempt + 1}: {e}, retrying...\")\n",
    "                time.sleep(2)\n",
    "                continue\n",
    "            return f\"RequestError: {e}\"\n",
    "        except Exception as e:\n",
    "            if attempt < max_retries:\n",
    "                logger.warning(f\"Unexpected error on attempt {attempt + 1}: {e}, retrying...\")\n",
    "                time.sleep(2)\n",
    "                continue\n",
    "            return f\"UnexpectedError: {e}\"\n",
    "    \n",
    "    return \"Error: All retry attempts failed\"\n",
    "\n",
    "def extract_response_text(result) -> str:\n",
    "    \"\"\"Extract text response from various Ollama response formats.\"\"\"\n",
    "    if isinstance(result, dict):\n",
    "        if \"response\" in result:\n",
    "            return str(result[\"response\"]).strip()\n",
    "        elif \"output\" in result:\n",
    "            return str(result[\"output\"]).strip()\n",
    "        elif \"responses\" in result and isinstance(result[\"responses\"], list):\n",
    "            return str(result[\"responses\"][0]).strip()\n",
    "        else:\n",
    "            # Chat-style response\n",
    "            choices = result.get(\"choices\")\n",
    "            if isinstance(choices, list) and choices:\n",
    "                return str(choices[0].get(\"message\", {}).get(\"content\", \"\")).strip()\n",
    "            else:\n",
    "                return str(result).strip()\n",
    "    elif isinstance(result, list) and result:\n",
    "        return str(result[0]).strip()\n",
    "    else:\n",
    "        return str(result).strip()\n",
    "\n",
    "def process_file(input_path: str):\n",
    "    \"\"\"Process a single input file.\"\"\"\n",
    "    # Detect template tag and create output name\n",
    "    match = re.search(r\"templ-\\d+\", input_path)\n",
    "    if not match:\n",
    "        logger.warning(f\"Skipping {input_path}, no template tag found.\")\n",
    "        return\n",
    "    \n",
    "    template_tag = match.group()\n",
    "    model_tag = model_name.replace(':', '-')\n",
    "    output_path = f\"relevance_210725_completions_{model_tag}-{template_tag}.csv\"\n",
    "    \n",
    "    logger.info(f\"Processing {input_path} -> {output_path}\")\n",
    "    \n",
    "    try:\n",
    "        df = pd.read_csv(input_path, encoding='utf-8')\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to read {input_path}: {e}\")\n",
    "        return\n",
    "    \n",
    "    \"\"\" # Ensure eval_prompt column exists\n",
    "    if 'eval_prompt' not in df.columns:\n",
    "        if df.empty:\n",
    "            logger.error(f\"Empty DataFrame in {input_path}\")\n",
    "            return\n",
    "        # Assume last column is prompt\n",
    "        df.rename(columns={df.columns[-1]: 'eval_prompt'}, inplace=True)\n",
    "        logger.info(f\"Renamed column '{df.columns[-1]}' to 'eval_prompt'\") \"\"\"\n",
    "    \n",
    "    # Apply test subset if configured\n",
    "    original_total = len(df)\n",
    "    if TEST_SUBSET is not None and len(df) > TEST_SUBSET:\n",
    "        df = df.head(TEST_SUBSET).copy()\n",
    "        logger.info(f\"Processing subset: {len(df)} rows out of {original_total}\")\n",
    "    \n",
    "    # Initialize result columns\n",
    "    df['eval_completion'] = None\n",
    "    df['model'] = model_name\n",
    "    \n",
    "    total = len(df)\n",
    "    logger.info(f\"Starting processing of {total} rows with {MAX_WORKERS} workers\")\n",
    "    \n",
    "    start_time = time.perf_counter()\n",
    "    completed = 0\n",
    "    request_times = []\n",
    "    \n",
    "    # Process requests in parallel\n",
    "    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "        # Submit all tasks\n",
    "        future_to_idx = {}\n",
    "        for idx, prompt in df['eval_prompt'].items():\n",
    "            if pd.isna(prompt) or prompt == \"\":\n",
    "                df.at[idx, 'eval_completion'] = \"Error: Empty prompt\"\n",
    "                completed += 1\n",
    "                continue\n",
    "            future = executor.submit(call_ollama_single, str(prompt))\n",
    "            future_to_idx[future] = idx\n",
    "        \n",
    "        # Process completed requests\n",
    "        for future in as_completed(future_to_idx):\n",
    "            request_start = time.perf_counter()\n",
    "            idx = future_to_idx[future]\n",
    "            \n",
    "            try:\n",
    "                completion = future.result()\n",
    "            except Exception as e:\n",
    "                completion = f\"FutureError: {e}\"\n",
    "                logger.error(f\"Future error for row {idx}: {e}\")\n",
    "            \n",
    "            request_time = time.perf_counter() - request_start\n",
    "            request_times.append(request_time)\n",
    "            \n",
    "            df.at[idx, 'eval_completion'] = completion\n",
    "            completed += 1\n",
    "            \n",
    "            # Calculate and display progress\n",
    "            if completed % max(1, total // 20) == 0 or completed == total:  # Update every 5%\n",
    "                elapsed_total = time.perf_counter() - start_time\n",
    "                if request_times:\n",
    "                    avg_request_time = sum(request_times) / len(request_times)\n",
    "                    remaining = total - completed\n",
    "                    eta_sec = remaining * avg_request_time\n",
    "                    \n",
    "                    logger.info(f\"Progress: {completed}/{total} ({100*completed/total:.1f}%) | \"\n",
    "                              f\"Last: {request_time:.1f}s | Avg: {avg_request_time:.1f}s | \"\n",
    "                              f\"ETA: {eta_sec/60:.1f}min\")\n",
    "    \n",
    "    total_time = time.perf_counter() - start_time\n",
    "    successful = df['eval_completion'].notna().sum()\n",
    "    errors = df['eval_completion'].str.contains('Error:', case=False, na=False).sum()\n",
    "    \n",
    "    logger.info(f\"Completed {total} rows in {total_time/60:.2f} minutes\")\n",
    "    logger.info(f\"Success rate: {successful-errors}/{total} ({100*(successful-errors)/total:.1f}%)\")\n",
    "    if errors > 0:\n",
    "        logger.warning(f\"Errors encountered: {errors} rows\")\n",
    "    \n",
    "    # Write output\n",
    "    try:\n",
    "        df.to_csv(output_path, index=False, encoding='utf-8')\n",
    "        logger.info(f\"Results saved to {output_path}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to save results: {e}\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main execution function.\"\"\"\n",
    "    logger.info(f\"Starting batch processing with {MAX_WORKERS} workers, timeout {REQUEST_TIMEOUT}s\")\n",
    "    if TEST_SUBSET:\n",
    "        logger.info(f\"TEST MODE: Processing only first {TEST_SUBSET} rows per file\")\n",
    "    \n",
    "    for input_path in input_files:\n",
    "        if not Path(input_path).exists():\n",
    "            logger.error(f\"Input file not found: {input_path}\")\n",
    "            continue\n",
    "        process_file(input_path)\n",
    "    \n",
    "    logger.info(\"All files processed\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af03678e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# runs pretty shit\n",
    "import time\n",
    "import json\n",
    "import requests\n",
    "import re\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import logging\n",
    "from typing import List, Dict, Any, Optional\n",
    "import asyncio\n",
    "import aiohttp\n",
    "import threading\n",
    "\n",
    "# Configuration\n",
    "MAX_WORKERS = 8  # Increased for better throughput\n",
    "BATCH_SIZE = 15  # Reduced batch size for reasoning model\n",
    "REQUEST_TIMEOUT = 90  # Increased timeout for reasoning models\n",
    "RETRY_ATTEMPTS = 2\n",
    "TEST_SUBSET = 50  # Set to None to process all rows\n",
    "PROGRESS_UPDATE_INTERVAL = 5  # Update progress every N completions\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class OllamaProcessor:\n",
    "    \"\"\"Optimized Ollama API processor with batch capabilities and better error handling.\"\"\"\n",
    "    \n",
    "    def __init__(self, url: str, model: str, max_workers: int = MAX_WORKERS):\n",
    "        self.url = url\n",
    "        self.model = model\n",
    "        self.max_workers = max_workers\n",
    "        self.session = None\n",
    "        \n",
    "    def create_payload(self, prompt: str) -> Dict[str, Any]:\n",
    "        \"\"\"Create standardized payload for Ollama API.\"\"\"\n",
    "        return {\n",
    "            \"model\": self.model,\n",
    "            \"prompt\": str(prompt).strip(),\n",
    "            \"stream\": False,\n",
    "            \"options\": {\n",
    "                \"temperature\": 0.1,  # Lower temperature for more consistent responses\n",
    "                \"num_predict\": -1    # No limit on response length (let model decide)\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def normalize_response(self, text: str) -> str:\n",
    "        \"\"\"Normalize and clean response text.\"\"\"\n",
    "        if not text:\n",
    "            return \"Error: Empty response\"\n",
    "        \n",
    "        text = str(text).strip()\n",
    "        if not text:\n",
    "            return \"Error: Empty response after cleaning\"\n",
    "        \n",
    "        # Normalize yes/no responses\n",
    "        lower_text = text.lower()\n",
    "        if lower_text.startswith(\"yes\") or lower_text == \"y\":\n",
    "            return \"Yes\"\n",
    "        if lower_text.startswith(\"no\") or lower_text == \"n\":\n",
    "            return \"No\"\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    def extract_response_text(self, result: Any) -> str:\n",
    "        \"\"\"Extract text response from Ollama API response.\"\"\"\n",
    "        try:\n",
    "            if isinstance(result, dict):\n",
    "                if \"response\" in result:\n",
    "                    return str(result[\"response\"]).strip()\n",
    "                elif \"output\" in result:\n",
    "                    return str(result[\"output\"]).strip()\n",
    "                elif \"message\" in result and isinstance(result[\"message\"], dict):\n",
    "                    return str(result[\"message\"].get(\"content\", \"\")).strip()\n",
    "                elif \"choices\" in result and isinstance(result[\"choices\"], list) and result[\"choices\"]:\n",
    "                    return str(result[\"choices\"][0].get(\"message\", {}).get(\"content\", \"\")).strip()\n",
    "                else:\n",
    "                    # Fallback: convert entire dict to string\n",
    "                    return str(result).strip()\n",
    "            elif isinstance(result, list) and result:\n",
    "                return str(result[0]).strip()\n",
    "            else:\n",
    "                return str(result).strip()\n",
    "        except Exception as e:\n",
    "            return f\"ExtractionError: {e}\"\n",
    "    \n",
    "    def call_ollama_single(self, prompt: str) -> str:\n",
    "        \"\"\"Make a single API call to Ollama with improved error handling.\"\"\"\n",
    "        if not prompt or pd.isna(prompt):\n",
    "            return \"Error: Empty or null prompt\"\n",
    "        \n",
    "        payload = self.create_payload(prompt)\n",
    "        \n",
    "        for attempt in range(RETRY_ATTEMPTS + 1):\n",
    "            try:\n",
    "                response = requests.post(\n",
    "                    self.url,\n",
    "                    headers={\"Content-Type\": \"application/json\"},\n",
    "                    json=payload,  # Use json parameter instead of data + dumps\n",
    "                    timeout=REQUEST_TIMEOUT\n",
    "                )\n",
    "                \n",
    "                if response.status_code == 200:\n",
    "                    try:\n",
    "                        result = response.json()\n",
    "                        text = self.extract_response_text(result)\n",
    "                        return self.normalize_response(text)\n",
    "                    except json.JSONDecodeError as e:\n",
    "                        if attempt < RETRY_ATTEMPTS:\n",
    "                            logger.warning(f\"JSON decode error (attempt {attempt + 1}): {e}\")\n",
    "                            time.sleep(0.5)\n",
    "                            continue\n",
    "                        return f\"JSONError: {e}\"\n",
    "                else:\n",
    "                    if attempt < RETRY_ATTEMPTS:\n",
    "                        logger.warning(f\"HTTP {response.status_code} (attempt {attempt + 1})\")\n",
    "                        time.sleep(1)\n",
    "                        continue\n",
    "                    return f\"HTTPError: {response.status_code}\"\n",
    "                    \n",
    "            except requests.exceptions.Timeout:\n",
    "                if attempt < RETRY_ATTEMPTS:\n",
    "                    logger.warning(f\"Timeout (attempt {attempt + 1})\")\n",
    "                    time.sleep(1)\n",
    "                    continue\n",
    "                return \"TimeoutError: Request timed out\"\n",
    "            except requests.exceptions.ConnectionError as e:\n",
    "                if attempt < RETRY_ATTEMPTS:\n",
    "                    logger.warning(f\"Connection error (attempt {attempt + 1}): {e}\")\n",
    "                    time.sleep(2)\n",
    "                    continue\n",
    "                return f\"ConnectionError: {e}\"\n",
    "            except Exception as e:\n",
    "                if attempt < RETRY_ATTEMPTS:\n",
    "                    logger.warning(f\"Unexpected error (attempt {attempt + 1}): {e}\")\n",
    "                    time.sleep(1)\n",
    "                    continue\n",
    "                return f\"UnexpectedError: {e}\"\n",
    "        \n",
    "        return \"Error: All retry attempts failed\"\n",
    "    \n",
    "    def process_batch(self, batch_data: List[tuple]) -> List[tuple]:\n",
    "        \"\"\"Process a batch of prompts efficiently.\"\"\"\n",
    "        results = []\n",
    "        \n",
    "        with ThreadPoolExecutor(max_workers=min(self.max_workers, len(batch_data))) as executor:\n",
    "            # Submit all requests in the batch\n",
    "            future_to_data = {\n",
    "                executor.submit(self.call_ollama_single, prompt): (idx, prompt)\n",
    "                for idx, prompt in batch_data\n",
    "            }\n",
    "            \n",
    "            # Collect results as they complete\n",
    "            for future in as_completed(future_to_data):\n",
    "                idx, original_prompt = future_to_data[future]\n",
    "                try:\n",
    "                    completion = future.result()\n",
    "                except Exception as e:\n",
    "                    completion = f\"FutureError: {e}\"\n",
    "                    logger.error(f\"Future error for row {idx}: {e}\")\n",
    "                \n",
    "                results.append((idx, completion))\n",
    "        \n",
    "        return results\n",
    "\n",
    "def process_file_optimized(input_path: str, processor: OllamaProcessor):\n",
    "    \"\"\"Process a single input file with optimized batch processing.\"\"\"\n",
    "    # Extract template tag and create output name\n",
    "    match = re.search(r\"templ-\\d+\", input_path)\n",
    "    if not match:\n",
    "        logger.warning(f\"Skipping {input_path}, no template tag found.\")\n",
    "        return\n",
    "    \n",
    "    template_tag = match.group()\n",
    "    model_tag = processor.model.replace(':', '-')\n",
    "    output_path = f\"relevance_210725_completions_{model_tag}-{template_tag}.csv\"\n",
    "    \n",
    "    logger.info(f\"Processing {input_path} -> {output_path}\")\n",
    "    \n",
    "    # Load and validate data\n",
    "    try:\n",
    "        df = pd.read_csv(input_path, encoding='utf-8')\n",
    "        logger.info(f\"Loaded {len(df)} rows from {input_path}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to read {input_path}: {e}\")\n",
    "        return\n",
    "    \n",
    "    if df.empty:\n",
    "        logger.error(f\"Empty DataFrame in {input_path}\")\n",
    "        return\n",
    "    \n",
    "    # Ensure eval_prompt column exists\n",
    "    if 'eval_prompt' not in df.columns:\n",
    "        last_col = df.columns[-1]\n",
    "        df.rename(columns={last_col: 'eval_prompt'}, inplace=True)\n",
    "        logger.info(f\"Renamed column '{last_col}' to 'eval_prompt'\")\n",
    "    \n",
    "    # Apply test subset if configured\n",
    "    original_total = len(df)\n",
    "    if TEST_SUBSET is not None and len(df) > TEST_SUBSET:\n",
    "        df = df.head(TEST_SUBSET).copy()\n",
    "        logger.info(f\"TEST MODE: Processing {len(df)} rows out of {original_total}\")\n",
    "    \n",
    "    # Initialize result columns\n",
    "    df['eval_completion'] = None\n",
    "    df['model'] = processor.model\n",
    "    \n",
    "    # Filter out empty prompts\n",
    "    valid_indices = df[df['eval_prompt'].notna() & (df['eval_prompt'] != \"\")].index.tolist()\n",
    "    invalid_count = len(df) - len(valid_indices)\n",
    "    \n",
    "    if invalid_count > 0:\n",
    "        logger.warning(f\"Found {invalid_count} empty/null prompts, skipping those rows\")\n",
    "        df.loc[~df.index.isin(valid_indices), 'eval_completion'] = \"Error: Empty prompt\"\n",
    "    \n",
    "    if not valid_indices:\n",
    "        logger.error(\"No valid prompts found in the dataset\")\n",
    "        return\n",
    "    \n",
    "    total_valid = len(valid_indices)\n",
    "    logger.info(f\"Processing {total_valid} valid prompts in batches of {BATCH_SIZE}\")\n",
    "    \n",
    "    # Process in batches\n",
    "    start_time = time.perf_counter()\n",
    "    completed = 0\n",
    "    all_request_times = []\n",
    "    \n",
    "    # Create batches\n",
    "    batches = [valid_indices[i:i + BATCH_SIZE] for i in range(0, len(valid_indices), BATCH_SIZE)]\n",
    "    logger.info(f\"Created {len(batches)} batches\")\n",
    "    \n",
    "    for batch_num, batch_indices in enumerate(batches, 1):\n",
    "        batch_start = time.perf_counter()\n",
    "        \n",
    "        # Prepare batch data\n",
    "        batch_data = [(idx, df.at[idx, 'eval_prompt']) for idx in batch_indices]\n",
    "        \n",
    "        logger.info(f\"Processing batch {batch_num}/{len(batches)} ({len(batch_data)} items)\")\n",
    "        \n",
    "        # Process the batch\n",
    "        batch_results = processor.process_batch(batch_data)\n",
    "        \n",
    "        # Update DataFrame with results\n",
    "        for idx, completion in batch_results:\n",
    "            df.at[idx, 'eval_completion'] = completion\n",
    "            completed += 1\n",
    "        \n",
    "        batch_time = time.perf_counter() - batch_start\n",
    "        avg_time_per_request = batch_time / len(batch_data)\n",
    "        all_request_times.append(avg_time_per_request)\n",
    "        \n",
    "        # Calculate ETA\n",
    "        if completed > 0:\n",
    "            elapsed_total = time.perf_counter() - start_time\n",
    "            avg_batch_time = elapsed_total / batch_num\n",
    "            remaining_batches = len(batches) - batch_num\n",
    "            eta_seconds = remaining_batches * avg_batch_time\n",
    "            \n",
    "            logger.info(f\"Batch {batch_num} completed in {batch_time:.1f}s \"\n",
    "                       f\"(avg {avg_time_per_request:.2f}s/request)\")\n",
    "            logger.info(f\"Progress: {completed}/{total_valid} ({100*completed/total_valid:.1f}%) | \"\n",
    "                       f\"ETA: {eta_seconds/60:.1f} minutes\")\n",
    "        \n",
    "        # Small delay between batches to prevent overwhelming the API\n",
    "        if batch_num < len(batches):\n",
    "            time.sleep(0.1)\n",
    "    \n",
    "    # Final statistics\n",
    "    total_time = time.perf_counter() - start_time\n",
    "    successful = df['eval_completion'].notna().sum()\n",
    "    errors = df['eval_completion'].str.contains('Error:', case=False, na=False).sum()\n",
    "    \n",
    "    logger.info(f\"\\n{'='*60}\")\n",
    "    logger.info(f\"PROCESSING COMPLETE\")\n",
    "    logger.info(f\"Total time: {total_time/60:.2f} minutes\")\n",
    "    logger.info(f\"Total rows processed: {len(df)}\")\n",
    "    logger.info(f\"Valid prompts: {total_valid}\")\n",
    "    logger.info(f\"Successful completions: {successful - errors}\")\n",
    "    logger.info(f\"Errors: {errors}\")\n",
    "    logger.info(f\"Success rate: {100*(successful-errors)/len(df):.1f}%\")\n",
    "    if all_request_times:\n",
    "        logger.info(f\"Average time per request: {sum(all_request_times)/len(all_request_times):.2f}s\")\n",
    "    logger.info(f\"{'='*60}\\n\")\n",
    "    \n",
    "    # Save results\n",
    "    try:\n",
    "        df.to_csv(output_path, index=False, encoding='utf-8')\n",
    "        logger.info(f\"Results saved to {output_path}\")\n",
    "        \n",
    "        # Quick validation of output\n",
    "        saved_df = pd.read_csv(output_path)\n",
    "        logger.info(f\"Validation: Saved file has {len(saved_df)} rows\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to save results to {output_path}: {e}\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main execution function with improved setup and error handling.\"\"\"\n",
    "    logger.info(f\"Starting optimized batch processing\")\n",
    "    logger.info(f\"Configuration:\")\n",
    "    logger.info(f\"  - Max workers: {MAX_WORKERS}\")\n",
    "    logger.info(f\"  - Batch size: {BATCH_SIZE}\")\n",
    "    logger.info(f\"  - Request timeout: {REQUEST_TIMEOUT}s\")\n",
    "    logger.info(f\"  - Retry attempts: {RETRY_ATTEMPTS}\")\n",
    "    if TEST_SUBSET:\n",
    "        logger.info(f\"  - TEST MODE: Processing only first {TEST_SUBSET} rows per file\")\n",
    "    logger.info(f\"  - Model: {model_name}\")\n",
    "    logger.info(f\"  - Ollama URL: {ollama_url}\")\n",
    "    \n",
    "    # Initialize processor\n",
    "    processor = OllamaProcessor(ollama_url, model_name, MAX_WORKERS)\n",
    "    \n",
    "    # Test connection\n",
    "    \"\"\" logger.info(\"Testing connection to Ollama...\")\n",
    "    test_result = processor.call_ollama_single(\"Test connection. Please respond with 'OK'.\")\n",
    "    if \"Error\" in test_result:\n",
    "        logger.error(f\"Connection test failed: {test_result}\")\n",
    "        logger.error(\"Please check that Ollama is running and the URL is correct\")\n",
    "        return\n",
    "    else:\n",
    "        logger.info(f\"Connection test successful: {test_result}\") \"\"\"\n",
    "    \n",
    "    # Process all input files\n",
    "    total_files = len(input_files)\n",
    "    logger.info(f\"Found {total_files} input files to process\")\n",
    "    \n",
    "    for file_num, input_path in enumerate(input_files, 1):\n",
    "        logger.info(f\"\\n{'='*60}\")\n",
    "        logger.info(f\"PROCESSING FILE {file_num}/{total_files}: {input_path}\")\n",
    "        logger.info(f\"{'='*60}\")\n",
    "        \n",
    "        if not Path(input_path).exists():\n",
    "            logger.error(f\"Input file not found: {input_path}\")\n",
    "            continue\n",
    "        \n",
    "        file_start = time.perf_counter()\n",
    "        process_file_optimized(input_path, processor)\n",
    "        file_time = time.perf_counter() - file_start\n",
    "        \n",
    "        logger.info(f\"File {file_num} completed in {file_time/60:.2f} minutes\")\n",
    "    \n",
    "    logger.info(f\"\\n{'='*60}\")\n",
    "    logger.info(\"ALL FILES PROCESSED SUCCESSFULLY\")\n",
    "    logger.info(f\"{'='*60}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "\"\"\"\n",
    "THIS IS OUTDATED. USE THE CODE BLOCK FOLLOWING THIS ONE!\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae3e81ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-23 10:36:59,014 - INFO - Starting optimized processing\n",
      "2025-07-23 10:36:59,015 - INFO - - Model: deepseek-r1:70b\n",
      "2025-07-23 10:36:59,015 - INFO - - Ollama URL: http://localhost:11434/api/generate\n",
      "2025-07-23 10:36:59,016 - INFO - - Max workers: 12\n",
      "2025-07-23 10:36:59,017 - INFO - Processing relevance_210725_prompts_templ-1.csv -> relevance_210725_completions_deepseek-r1-70b-templ-1.csv\n",
      "2025-07-23 10:36:59,036 - INFO - Starting processing of 1000 items with 12 workers\n",
      "2025-07-23 10:38:13,442 - INFO - Progress: 5/1000 (0.5%) | Avg: 14.9s | ETA: 246.8min\n",
      "2025-07-23 10:39:12,808 - INFO - Progress: 10/1000 (1.0%) | Avg: 13.4s | ETA: 220.7min\n",
      "2025-07-23 10:39:57,582 - INFO - Progress: 15/1000 (1.5%) | Avg: 11.9s | ETA: 195.4min\n",
      "2025-07-23 10:40:49,065 - INFO - Progress: 20/1000 (2.0%) | Avg: 11.5s | ETA: 187.9min\n",
      "2025-07-23 10:41:44,201 - INFO - Progress: 25/1000 (2.5%) | Avg: 11.4s | ETA: 185.4min\n",
      "2025-07-23 10:42:37,193 - INFO - Progress: 30/1000 (3.0%) | Avg: 11.3s | ETA: 182.2min\n",
      "2025-07-23 10:43:30,856 - INFO - Progress: 35/1000 (3.5%) | Avg: 11.2s | ETA: 180.1min\n",
      "2025-07-23 10:44:32,542 - INFO - Progress: 40/1000 (4.0%) | Avg: 11.3s | ETA: 181.4min\n",
      "2025-07-23 10:45:25,574 - INFO - Progress: 45/1000 (4.5%) | Avg: 11.3s | ETA: 179.2min\n",
      "2025-07-23 10:46:19,916 - INFO - Progress: 50/1000 (5.0%) | Avg: 11.2s | ETA: 177.6min\n",
      "2025-07-23 10:47:10,876 - INFO - Progress: 55/1000 (5.5%) | Avg: 11.1s | ETA: 175.2min\n",
      "2025-07-23 10:48:09,105 - INFO - Progress: 60/1000 (6.0%) | Avg: 11.2s | ETA: 175.0min\n",
      "2025-07-23 10:49:13,533 - INFO - Progress: 65/1000 (6.5%) | Avg: 11.3s | ETA: 176.1min\n",
      "2025-07-23 10:50:12,515 - INFO - Progress: 70/1000 (7.0%) | Avg: 11.3s | ETA: 175.7min\n",
      "2025-07-23 10:51:04,058 - INFO - Progress: 75/1000 (7.5%) | Avg: 11.3s | ETA: 173.7min\n",
      "2025-07-23 10:51:54,312 - INFO - Progress: 80/1000 (8.0%) | Avg: 11.2s | ETA: 171.6min\n",
      "2025-07-23 10:52:52,783 - INFO - Progress: 85/1000 (8.5%) | Avg: 11.2s | ETA: 171.1min\n",
      "2025-07-23 10:53:51,801 - INFO - Progress: 90/1000 (9.0%) | Avg: 11.3s | ETA: 170.7min\n",
      "2025-07-23 10:54:46,179 - INFO - Progress: 95/1000 (9.5%) | Avg: 11.2s | ETA: 169.4min\n",
      "2025-07-23 10:55:28,868 - INFO - Progress: 100/1000 (10.0%) | Avg: 11.1s | ETA: 166.5min\n",
      "2025-07-23 10:56:28,994 - INFO - Progress: 105/1000 (10.5%) | Avg: 11.1s | ETA: 166.2min\n",
      "2025-07-23 10:57:18,058 - INFO - Progress: 110/1000 (11.0%) | Avg: 11.1s | ETA: 164.4min\n",
      "2025-07-23 10:58:19,896 - INFO - Progress: 115/1000 (11.5%) | Avg: 11.1s | ETA: 164.3min\n",
      "2025-07-23 10:59:10,813 - INFO - Progress: 120/1000 (12.0%) | Avg: 11.1s | ETA: 162.8min\n",
      "2025-07-23 11:00:06,629 - INFO - Progress: 125/1000 (12.5%) | Avg: 11.1s | ETA: 161.9min\n",
      "2025-07-23 11:01:05,572 - INFO - Progress: 130/1000 (13.0%) | Avg: 11.1s | ETA: 161.3min\n",
      "2025-07-23 11:02:01,866 - INFO - Progress: 135/1000 (13.5%) | Avg: 11.1s | ETA: 160.5min\n",
      "2025-07-23 11:03:01,579 - INFO - Progress: 140/1000 (14.0%) | Avg: 11.2s | ETA: 160.0min\n",
      "2025-07-23 11:03:37,210 - INFO - Progress: 145/1000 (14.5%) | Avg: 11.0s | ETA: 157.1min\n",
      "2025-07-23 11:04:39,707 - INFO - Progress: 150/1000 (15.0%) | Avg: 11.1s | ETA: 156.8min\n",
      "2025-07-23 11:05:33,656 - INFO - Progress: 155/1000 (15.5%) | Avg: 11.1s | ETA: 155.8min\n",
      "2025-07-23 11:06:26,001 - INFO - Progress: 160/1000 (16.0%) | Avg: 11.0s | ETA: 154.6min\n",
      "2025-07-23 11:07:32,626 - INFO - Progress: 165/1000 (16.5%) | Avg: 11.1s | ETA: 154.7min\n",
      "2025-07-23 11:08:39,927 - INFO - Progress: 170/1000 (17.0%) | Avg: 11.2s | ETA: 154.7min\n",
      "2025-07-23 11:09:23,502 - INFO - Progress: 175/1000 (17.5%) | Avg: 11.1s | ETA: 152.8min\n",
      "2025-07-23 11:10:21,887 - INFO - Progress: 180/1000 (18.0%) | Avg: 11.1s | ETA: 152.1min\n",
      "2025-07-23 11:11:17,703 - INFO - Progress: 185/1000 (18.5%) | Avg: 11.1s | ETA: 151.2min\n",
      "2025-07-23 11:12:22,331 - INFO - Progress: 190/1000 (19.0%) | Avg: 11.2s | ETA: 150.9min\n",
      "2025-07-23 11:13:21,558 - INFO - Progress: 195/1000 (19.5%) | Avg: 11.2s | ETA: 150.2min\n",
      "2025-07-23 11:14:03,025 - INFO - Progress: 200/1000 (20.0%) | Avg: 11.1s | ETA: 148.3min\n",
      "2025-07-23 11:15:08,727 - INFO - Progress: 205/1000 (20.5%) | Avg: 11.2s | ETA: 148.0min\n",
      "2025-07-23 11:15:52,864 - INFO - Progress: 210/1000 (21.0%) | Avg: 11.1s | ETA: 146.3min\n",
      "2025-07-23 11:17:04,656 - INFO - Progress: 215/1000 (21.5%) | Avg: 11.2s | ETA: 146.4min\n",
      "2025-07-23 11:17:42,275 - INFO - Progress: 220/1000 (22.0%) | Avg: 11.1s | ETA: 144.4min\n",
      "2025-07-23 11:18:47,835 - INFO - Progress: 225/1000 (22.5%) | Avg: 11.2s | ETA: 144.0min\n",
      "2025-07-23 11:19:27,413 - INFO - Progress: 230/1000 (23.0%) | Avg: 11.1s | ETA: 142.2min\n",
      "2025-07-23 11:20:22,944 - INFO - Progress: 235/1000 (23.5%) | Avg: 11.1s | ETA: 141.3min\n",
      "2025-07-23 11:21:17,231 - INFO - Progress: 240/1000 (24.0%) | Avg: 11.1s | ETA: 140.3min\n",
      "2025-07-23 11:22:08,062 - INFO - Progress: 245/1000 (24.5%) | Avg: 11.1s | ETA: 139.1min\n",
      "2025-07-23 11:22:59,194 - INFO - Progress: 250/1000 (25.0%) | Avg: 11.0s | ETA: 138.0min\n",
      "2025-07-23 11:23:59,182 - INFO - Progress: 255/1000 (25.5%) | Avg: 11.1s | ETA: 137.3min\n",
      "2025-07-23 11:25:02,098 - INFO - Progress: 260/1000 (26.0%) | Avg: 11.1s | ETA: 136.8min\n",
      "2025-07-23 11:25:53,296 - INFO - Progress: 265/1000 (26.5%) | Avg: 11.1s | ETA: 135.6min\n",
      "2025-07-23 11:26:55,737 - INFO - Progress: 270/1000 (27.0%) | Avg: 11.1s | ETA: 135.0min\n",
      "2025-07-23 11:27:38,516 - INFO - Progress: 275/1000 (27.5%) | Avg: 11.1s | ETA: 133.6min\n",
      "2025-07-23 11:28:37,882 - INFO - Progress: 280/1000 (28.0%) | Avg: 11.1s | ETA: 132.8min\n",
      "2025-07-23 11:29:29,685 - INFO - Progress: 285/1000 (28.5%) | Avg: 11.1s | ETA: 131.7min\n",
      "2025-07-23 11:30:36,259 - INFO - Progress: 290/1000 (29.0%) | Avg: 11.1s | ETA: 131.3min\n",
      "2025-07-23 11:31:31,160 - INFO - Progress: 295/1000 (29.5%) | Avg: 11.1s | ETA: 130.3min\n",
      "2025-07-23 11:32:36,818 - INFO - Progress: 300/1000 (30.0%) | Avg: 11.1s | ETA: 129.8min\n",
      "2025-07-23 11:33:25,716 - INFO - Progress: 305/1000 (30.5%) | Avg: 11.1s | ETA: 128.6min\n",
      "2025-07-23 11:34:31,001 - INFO - Progress: 310/1000 (31.0%) | Avg: 11.1s | ETA: 128.1min\n",
      "2025-07-23 11:35:17,741 - INFO - Progress: 315/1000 (31.5%) | Avg: 11.1s | ETA: 126.8min\n",
      "2025-07-23 11:36:22,684 - INFO - Progress: 320/1000 (32.0%) | Avg: 11.1s | ETA: 126.2min\n",
      "2025-07-23 11:37:23,403 - INFO - Progress: 325/1000 (32.5%) | Avg: 11.2s | ETA: 125.5min\n",
      "2025-07-23 11:38:18,053 - INFO - Progress: 330/1000 (33.0%) | Avg: 11.1s | ETA: 124.5min\n",
      "2025-07-23 11:39:15,118 - INFO - Progress: 335/1000 (33.5%) | Avg: 11.2s | ETA: 123.6min\n",
      "2025-07-23 11:40:06,855 - INFO - Progress: 340/1000 (34.0%) | Avg: 11.1s | ETA: 122.5min\n",
      "2025-07-23 11:41:06,436 - INFO - Progress: 345/1000 (34.5%) | Avg: 11.2s | ETA: 121.7min\n",
      "2025-07-23 11:41:55,388 - INFO - Progress: 350/1000 (35.0%) | Avg: 11.1s | ETA: 120.6min\n",
      "2025-07-23 11:42:47,686 - INFO - Progress: 355/1000 (35.5%) | Avg: 11.1s | ETA: 119.6min\n",
      "2025-07-23 11:43:35,965 - INFO - Progress: 360/1000 (36.0%) | Avg: 11.1s | ETA: 118.4min\n",
      "2025-07-23 11:44:32,722 - INFO - Progress: 365/1000 (36.5%) | Avg: 11.1s | ETA: 117.5min\n",
      "2025-07-23 11:45:27,896 - INFO - Progress: 370/1000 (37.0%) | Avg: 11.1s | ETA: 116.6min\n",
      "2025-07-23 11:46:28,689 - INFO - Progress: 375/1000 (37.5%) | Avg: 11.1s | ETA: 115.8min\n",
      "2025-07-23 11:47:17,184 - INFO - Progress: 380/1000 (38.0%) | Avg: 11.1s | ETA: 114.7min\n",
      "2025-07-23 11:48:19,054 - INFO - Progress: 385/1000 (38.5%) | Avg: 11.1s | ETA: 113.9min\n",
      "2025-07-23 11:49:11,813 - INFO - Progress: 390/1000 (39.0%) | Avg: 11.1s | ETA: 112.9min\n",
      "2025-07-23 11:49:57,404 - INFO - Progress: 395/1000 (39.5%) | Avg: 11.1s | ETA: 111.8min\n",
      "2025-07-23 11:51:04,675 - INFO - Progress: 400/1000 (40.0%) | Avg: 11.1s | ETA: 111.1min\n",
      "2025-07-23 11:51:44,810 - INFO - Progress: 405/1000 (40.5%) | Avg: 11.1s | ETA: 109.8min\n",
      "2025-07-23 11:52:46,982 - INFO - Progress: 410/1000 (41.0%) | Avg: 11.1s | ETA: 109.1min\n",
      "2025-07-23 11:53:45,855 - INFO - Progress: 415/1000 (41.5%) | Avg: 11.1s | ETA: 108.2min\n",
      "2025-07-23 11:54:35,778 - INFO - Progress: 420/1000 (42.0%) | Avg: 11.1s | ETA: 107.2min\n",
      "2025-07-23 11:55:35,564 - INFO - Progress: 425/1000 (42.5%) | Avg: 11.1s | ETA: 106.4min\n",
      "2025-07-23 11:56:24,339 - INFO - Progress: 430/1000 (43.0%) | Avg: 11.1s | ETA: 105.3min\n",
      "2025-07-23 11:57:30,113 - INFO - Progress: 435/1000 (43.5%) | Avg: 11.1s | ETA: 104.6min\n",
      "2025-07-23 11:58:25,478 - INFO - Progress: 440/1000 (44.0%) | Avg: 11.1s | ETA: 103.7min\n",
      "2025-07-23 11:59:28,919 - INFO - Progress: 445/1000 (44.5%) | Avg: 11.1s | ETA: 102.9min\n",
      "2025-07-23 12:00:13,284 - INFO - Progress: 450/1000 (45.0%) | Avg: 11.1s | ETA: 101.7min\n",
      "2025-07-23 12:01:10,179 - INFO - Progress: 455/1000 (45.5%) | Avg: 11.1s | ETA: 100.8min\n",
      "2025-07-23 12:02:01,390 - INFO - Progress: 460/1000 (46.0%) | Avg: 11.1s | ETA: 99.8min\n",
      "2025-07-23 12:02:59,153 - INFO - Progress: 465/1000 (46.5%) | Avg: 11.1s | ETA: 98.9min\n",
      "2025-07-23 12:04:05,978 - INFO - Progress: 470/1000 (47.0%) | Avg: 11.1s | ETA: 98.2min\n",
      "2025-07-23 12:04:47,677 - INFO - Progress: 475/1000 (47.5%) | Avg: 11.1s | ETA: 97.1min\n",
      "2025-07-23 12:05:45,912 - INFO - Progress: 480/1000 (48.0%) | Avg: 11.1s | ETA: 96.2min\n",
      "2025-07-23 12:06:43,728 - INFO - Progress: 485/1000 (48.5%) | Avg: 11.1s | ETA: 95.3min\n",
      "2025-07-23 12:07:38,479 - INFO - Progress: 490/1000 (49.0%) | Avg: 11.1s | ETA: 94.4min\n",
      "2025-07-23 12:08:34,261 - INFO - Progress: 495/1000 (49.5%) | Avg: 11.1s | ETA: 93.4min\n",
      "2025-07-23 12:09:23,893 - INFO - Progress: 500/1000 (50.0%) | Avg: 11.1s | ETA: 92.4min\n",
      "2025-07-23 12:10:09,291 - INFO - Progress: 505/1000 (50.5%) | Avg: 11.1s | ETA: 91.3min\n",
      "2025-07-23 12:11:10,524 - INFO - Progress: 510/1000 (51.0%) | Avg: 11.1s | ETA: 90.5min\n",
      "2025-07-23 12:12:06,120 - INFO - Progress: 515/1000 (51.5%) | Avg: 11.1s | ETA: 89.6min\n",
      "2025-07-23 12:12:58,021 - INFO - Progress: 520/1000 (52.0%) | Avg: 11.1s | ETA: 88.6min\n",
      "2025-07-23 12:13:47,100 - INFO - Progress: 525/1000 (52.5%) | Avg: 11.1s | ETA: 87.6min\n",
      "2025-07-23 12:14:52,005 - INFO - Progress: 530/1000 (53.0%) | Avg: 11.1s | ETA: 86.8min\n",
      "2025-07-23 12:15:39,848 - INFO - Progress: 535/1000 (53.5%) | Avg: 11.1s | ETA: 85.8min\n",
      "2025-07-23 12:16:42,727 - INFO - Progress: 540/1000 (54.0%) | Avg: 11.1s | ETA: 85.0min\n",
      "2025-07-23 12:17:38,584 - INFO - Progress: 545/1000 (54.5%) | Avg: 11.1s | ETA: 84.0min\n",
      "2025-07-23 12:18:30,704 - INFO - Progress: 550/1000 (55.0%) | Avg: 11.1s | ETA: 83.1min\n",
      "2025-07-23 12:19:16,867 - INFO - Progress: 555/1000 (55.5%) | Avg: 11.1s | ETA: 82.0min\n",
      "2025-07-23 12:20:15,507 - INFO - Progress: 560/1000 (56.0%) | Avg: 11.1s | ETA: 81.1min\n",
      "2025-07-23 12:21:10,839 - INFO - Progress: 565/1000 (56.5%) | Avg: 11.1s | ETA: 80.2min\n",
      "2025-07-23 12:21:57,458 - INFO - Progress: 570/1000 (57.0%) | Avg: 11.0s | ETA: 79.2min\n",
      "2025-07-23 12:23:01,402 - INFO - Progress: 575/1000 (57.5%) | Avg: 11.1s | ETA: 78.4min\n",
      "2025-07-23 12:23:47,477 - INFO - Progress: 580/1000 (58.0%) | Avg: 11.0s | ETA: 77.3min\n",
      "2025-07-23 12:24:46,235 - INFO - Progress: 585/1000 (58.5%) | Avg: 11.1s | ETA: 76.5min\n",
      "2025-07-23 12:25:34,211 - INFO - Progress: 590/1000 (59.0%) | Avg: 11.0s | ETA: 75.5min\n",
      "2025-07-23 12:26:23,810 - INFO - Progress: 595/1000 (59.5%) | Avg: 11.0s | ETA: 74.5min\n",
      "2025-07-23 12:27:16,830 - INFO - Progress: 600/1000 (60.0%) | Avg: 11.0s | ETA: 73.5min\n",
      "2025-07-23 12:28:20,396 - INFO - Progress: 605/1000 (60.5%) | Avg: 11.0s | ETA: 72.7min\n",
      "2025-07-23 12:29:10,588 - INFO - Progress: 610/1000 (61.0%) | Avg: 11.0s | ETA: 71.7min\n",
      "2025-07-23 12:30:08,655 - INFO - Progress: 615/1000 (61.5%) | Avg: 11.0s | ETA: 70.8min\n",
      "2025-07-23 12:31:12,286 - INFO - Progress: 620/1000 (62.0%) | Avg: 11.1s | ETA: 70.0min\n",
      "2025-07-23 12:32:02,691 - INFO - Progress: 625/1000 (62.5%) | Avg: 11.0s | ETA: 69.0min\n",
      "2025-07-23 12:33:03,870 - INFO - Progress: 630/1000 (63.0%) | Avg: 11.1s | ETA: 68.2min\n",
      "2025-07-23 12:33:51,605 - INFO - Progress: 635/1000 (63.5%) | Avg: 11.0s | ETA: 67.2min\n",
      "2025-07-23 12:34:55,115 - INFO - Progress: 640/1000 (64.0%) | Avg: 11.1s | ETA: 66.3min\n",
      "2025-07-23 12:35:55,338 - INFO - Progress: 645/1000 (64.5%) | Avg: 11.1s | ETA: 65.5min\n",
      "2025-07-23 12:36:55,268 - INFO - Progress: 650/1000 (65.0%) | Avg: 11.1s | ETA: 64.6min\n",
      "2025-07-23 12:37:45,007 - INFO - Progress: 655/1000 (65.5%) | Avg: 11.1s | ETA: 63.6min\n",
      "2025-07-23 12:38:34,775 - INFO - Progress: 660/1000 (66.0%) | Avg: 11.1s | ETA: 62.6min\n",
      "2025-07-23 12:39:34,539 - INFO - Progress: 665/1000 (66.5%) | Avg: 11.1s | ETA: 61.8min\n",
      "2025-07-23 12:40:42,619 - INFO - Progress: 670/1000 (67.0%) | Avg: 11.1s | ETA: 60.9min\n",
      "2025-07-23 12:41:26,598 - INFO - Progress: 675/1000 (67.5%) | Avg: 11.1s | ETA: 59.9min\n",
      "2025-07-23 12:42:20,700 - INFO - Progress: 680/1000 (68.0%) | Avg: 11.1s | ETA: 59.0min\n",
      "2025-07-23 12:43:06,428 - INFO - Progress: 685/1000 (68.5%) | Avg: 11.0s | ETA: 58.0min\n",
      "2025-07-23 12:44:08,839 - INFO - Progress: 690/1000 (69.0%) | Avg: 11.1s | ETA: 57.1min\n",
      "2025-07-23 12:44:50,966 - INFO - Progress: 695/1000 (69.5%) | Avg: 11.0s | ETA: 56.1min\n",
      "2025-07-23 12:45:55,884 - INFO - Progress: 700/1000 (70.0%) | Avg: 11.1s | ETA: 55.3min\n",
      "2025-07-23 12:46:46,851 - INFO - Progress: 705/1000 (70.5%) | Avg: 11.0s | ETA: 54.3min\n",
      "2025-07-23 12:47:40,144 - INFO - Progress: 710/1000 (71.0%) | Avg: 11.0s | ETA: 53.4min\n",
      "2025-07-23 12:48:35,373 - INFO - Progress: 715/1000 (71.5%) | Avg: 11.0s | ETA: 52.5min\n",
      "2025-07-23 12:49:37,065 - INFO - Progress: 720/1000 (72.0%) | Avg: 11.1s | ETA: 51.6min\n",
      "2025-07-23 12:50:34,402 - INFO - Progress: 725/1000 (72.5%) | Avg: 11.1s | ETA: 50.7min\n",
      "2025-07-23 12:51:28,101 - INFO - Progress: 730/1000 (73.0%) | Avg: 11.1s | ETA: 49.7min\n",
      "2025-07-23 12:52:39,695 - INFO - Progress: 735/1000 (73.5%) | Avg: 11.1s | ETA: 48.9min\n",
      "2025-07-23 12:53:19,489 - INFO - Progress: 740/1000 (74.0%) | Avg: 11.1s | ETA: 47.9min\n",
      "2025-07-23 12:54:17,523 - INFO - Progress: 745/1000 (74.5%) | Avg: 11.1s | ETA: 47.0min\n",
      "2025-07-23 12:55:03,827 - INFO - Progress: 750/1000 (75.0%) | Avg: 11.0s | ETA: 46.0min\n",
      "2025-07-23 12:56:01,468 - INFO - Progress: 755/1000 (75.5%) | Avg: 11.0s | ETA: 45.1min\n",
      "2025-07-23 12:56:52,872 - INFO - Progress: 760/1000 (76.0%) | Avg: 11.0s | ETA: 44.2min\n",
      "2025-07-23 12:57:49,534 - INFO - Progress: 765/1000 (76.5%) | Avg: 11.0s | ETA: 43.3min\n",
      "2025-07-23 12:58:44,819 - INFO - Progress: 770/1000 (77.0%) | Avg: 11.0s | ETA: 42.3min\n",
      "2025-07-23 12:59:32,317 - INFO - Progress: 775/1000 (77.5%) | Avg: 11.0s | ETA: 41.4min\n",
      "2025-07-23 13:00:30,883 - INFO - Progress: 780/1000 (78.0%) | Avg: 11.0s | ETA: 40.5min\n",
      "2025-07-23 13:01:35,715 - INFO - Progress: 785/1000 (78.5%) | Avg: 11.1s | ETA: 39.6min\n",
      "2025-07-23 13:02:21,439 - INFO - Progress: 790/1000 (79.0%) | Avg: 11.0s | ETA: 38.6min\n",
      "2025-07-23 13:03:12,756 - INFO - Progress: 795/1000 (79.5%) | Avg: 11.0s | ETA: 37.7min\n",
      "2025-07-23 13:04:24,578 - INFO - Progress: 800/1000 (80.0%) | Avg: 11.1s | ETA: 36.9min\n",
      "2025-07-23 13:05:11,295 - INFO - Progress: 805/1000 (80.5%) | Avg: 11.0s | ETA: 35.9min\n",
      "2025-07-23 13:06:05,909 - INFO - Progress: 810/1000 (81.0%) | Avg: 11.0s | ETA: 35.0min\n",
      "2025-07-23 13:06:59,598 - INFO - Progress: 815/1000 (81.5%) | Avg: 11.0s | ETA: 34.1min\n",
      "2025-07-23 13:07:57,584 - INFO - Progress: 820/1000 (82.0%) | Avg: 11.0s | ETA: 33.1min\n",
      "2025-07-23 13:08:56,247 - INFO - Progress: 825/1000 (82.5%) | Avg: 11.1s | ETA: 32.2min\n",
      "2025-07-23 13:09:43,080 - INFO - Progress: 830/1000 (83.0%) | Avg: 11.0s | ETA: 31.3min\n",
      "2025-07-23 13:10:51,754 - INFO - Progress: 835/1000 (83.5%) | Avg: 11.1s | ETA: 30.4min\n",
      "2025-07-23 13:11:49,611 - INFO - Progress: 840/1000 (84.0%) | Avg: 11.1s | ETA: 29.5min\n",
      "2025-07-23 13:12:44,344 - INFO - Progress: 845/1000 (84.5%) | Avg: 11.1s | ETA: 28.6min\n",
      "2025-07-23 13:13:38,969 - INFO - Progress: 850/1000 (85.0%) | Avg: 11.1s | ETA: 27.6min\n",
      "2025-07-23 13:14:39,479 - INFO - Progress: 855/1000 (85.5%) | Avg: 11.1s | ETA: 26.7min\n",
      "2025-07-23 13:15:37,447 - INFO - Progress: 860/1000 (86.0%) | Avg: 11.1s | ETA: 25.8min\n",
      "2025-07-23 13:16:28,699 - INFO - Progress: 865/1000 (86.5%) | Avg: 11.1s | ETA: 24.9min\n",
      "2025-07-23 13:17:30,692 - INFO - Progress: 870/1000 (87.0%) | Avg: 11.1s | ETA: 24.0min\n",
      "2025-07-23 13:18:26,114 - INFO - Progress: 875/1000 (87.5%) | Avg: 11.1s | ETA: 23.1min\n",
      "2025-07-23 13:19:14,546 - INFO - Progress: 880/1000 (88.0%) | Avg: 11.1s | ETA: 22.1min\n",
      "2025-07-23 13:20:19,116 - INFO - Progress: 885/1000 (88.5%) | Avg: 11.1s | ETA: 21.2min\n",
      "2025-07-23 13:21:19,180 - INFO - Progress: 890/1000 (89.0%) | Avg: 11.1s | ETA: 20.3min\n",
      "2025-07-23 13:22:03,899 - INFO - Progress: 895/1000 (89.5%) | Avg: 11.1s | ETA: 19.4min\n",
      "2025-07-23 13:23:04,900 - INFO - Progress: 900/1000 (90.0%) | Avg: 11.1s | ETA: 18.5min\n",
      "2025-07-23 13:24:04,864 - INFO - Progress: 905/1000 (90.5%) | Avg: 11.1s | ETA: 17.5min\n",
      "2025-07-23 13:24:56,492 - INFO - Progress: 910/1000 (91.0%) | Avg: 11.1s | ETA: 16.6min\n",
      "2025-07-23 13:26:03,573 - INFO - Progress: 915/1000 (91.5%) | Avg: 11.1s | ETA: 15.7min\n",
      "2025-07-23 13:26:55,007 - INFO - Progress: 920/1000 (92.0%) | Avg: 11.1s | ETA: 14.8min\n",
      "2025-07-23 13:27:52,642 - INFO - Progress: 925/1000 (92.5%) | Avg: 11.1s | ETA: 13.9min\n",
      "2025-07-23 13:29:03,856 - INFO - Progress: 930/1000 (93.0%) | Avg: 11.1s | ETA: 13.0min\n",
      "2025-07-23 13:29:51,063 - INFO - Progress: 935/1000 (93.5%) | Avg: 11.1s | ETA: 12.0min\n",
      "2025-07-23 13:30:37,433 - INFO - Progress: 940/1000 (94.0%) | Avg: 11.1s | ETA: 11.1min\n",
      "2025-07-23 13:31:37,900 - INFO - Progress: 945/1000 (94.5%) | Avg: 11.1s | ETA: 10.2min\n",
      "2025-07-23 13:32:32,775 - INFO - Progress: 950/1000 (95.0%) | Avg: 11.1s | ETA: 9.2min\n",
      "2025-07-23 13:33:22,530 - INFO - Progress: 955/1000 (95.5%) | Avg: 11.1s | ETA: 8.3min\n",
      "2025-07-23 13:34:17,182 - INFO - Progress: 960/1000 (96.0%) | Avg: 11.1s | ETA: 7.4min\n",
      "2025-07-23 13:35:10,070 - INFO - Progress: 965/1000 (96.5%) | Avg: 11.1s | ETA: 6.5min\n",
      "2025-07-23 13:36:09,114 - INFO - Progress: 970/1000 (97.0%) | Avg: 11.1s | ETA: 5.5min\n",
      "2025-07-23 13:36:56,456 - INFO - Progress: 975/1000 (97.5%) | Avg: 11.1s | ETA: 4.6min\n",
      "2025-07-23 13:38:04,722 - INFO - Progress: 980/1000 (98.0%) | Avg: 11.1s | ETA: 3.7min\n",
      "2025-07-23 13:39:00,035 - INFO - Progress: 985/1000 (98.5%) | Avg: 11.1s | ETA: 2.8min\n",
      "2025-07-23 13:39:55,288 - INFO - Progress: 990/1000 (99.0%) | Avg: 11.1s | ETA: 1.8min\n",
      "2025-07-23 13:40:51,196 - INFO - Progress: 995/1000 (99.5%) | Avg: 11.1s | ETA: 0.9min\n",
      "2025-07-23 13:41:39,511 - INFO - Progress: 1000/1000 (100.0%) | Avg: 11.1s | ETA: 0.0min\n",
      "2025-07-23 13:41:39,534 - INFO - ==================================================\n",
      "2025-07-23 13:41:39,534 - INFO - Finished in 184.67 minutes\n",
      "2025-07-23 13:41:39,535 - INFO - Rows processed: 1000 (Valid: 1000)\n",
      "2025-07-23 13:41:39,535 - INFO - Successes: 954, Errors: 46 (Timeouts: 46)\n",
      "2025-07-23 13:41:39,536 - INFO - Success rate: 95.4%\n",
      "2025-07-23 13:41:39,536 - INFO - Throughput: 5.4 req/min\n",
      "2025-07-23 13:41:39,537 - INFO - ==================================================\n",
      "2025-07-23 13:41:39,553 - INFO - Saved to relevance_210725_completions_deepseek-r1-70b-templ-1.csv\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import json\n",
    "import requests\n",
    "import re\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import logging\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "# Configuration\n",
    "MAX_WORKERS = 12  # Increased workers for slow model\n",
    "TEST_SUBSET = None # None to process all rows\n",
    "REQUEST_TIMEOUT = 150# Longer timeout but with aggressive parallelization\n",
    "RETRY_ATTEMPTS = 1  # Reduced retries to fail fast\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class OllamaProcessor:\n",
    "    \"\"\"Optimized Ollama API processor with yes/no normalization.\"\"\"\n",
    "    \n",
    "    def __init__(self, url: str, model: str, max_workers: int = MAX_WORKERS):\n",
    "        self.url = url\n",
    "        self.model = model\n",
    "        self.max_workers = max_workers\n",
    "        self.session = requests.Session()\n",
    "        self.session.headers.update({\"Content-Type\": \"application/json\"})\n",
    "        adapter = requests.adapters.HTTPAdapter(\n",
    "            pool_connections=max_workers,\n",
    "            pool_maxsize=max_workers * 2,\n",
    "            max_retries=0\n",
    "        )\n",
    "        self.session.mount(\"http://\", adapter)\n",
    "        self.session.mount(\"https://\", adapter)\n",
    "        \n",
    "    def create_payload(self, prompt: str) -> Dict[str, Any]:\n",
    "        return {\n",
    "            \"model\": self.model,\n",
    "            \"prompt\": prompt.strip(),\n",
    "            \"stream\": False,\n",
    "            \"options\": {\n",
    "                \"temperature\": 0.0,\n",
    "                \"top_p\": 0.1,\n",
    "                \"num_predict\": -1,\n",
    "                \"num_ctx\": 4096,\n",
    "                \"repeat_penalty\": 1.0\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def normalize_response(self, text: str) -> str:\n",
    "        \"\"\"Strip <think>…</think>, then normalize to Yes/No if response ends with them.\"\"\"\n",
    "        if not text:\n",
    "            return \"Error: Empty response\"\n",
    "        text = text.strip()\n",
    "        # Remove any <think>...</think> block\n",
    "        m = re.search(r'<think>.*?</think>\\s*(.*)', text, re.IGNORECASE | re.DOTALL)\n",
    "        if m:\n",
    "            text = m.group(1).strip()\n",
    "\n",
    "        low = text.lower().rstrip('.!?')  # drop any trailing punctuation\n",
    "        if low.endswith(\"yes\"):\n",
    "            return \"Yes\"\n",
    "        if low.endswith(\"no\"):\n",
    "            return \"No\"\n",
    "        return text  # fallback to full text\n",
    "\n",
    "    \n",
    "    def extract_response_text(self, result: Any) -> str:\n",
    "        \"\"\"Extract text from various Ollama JSON shapes.\"\"\"\n",
    "        if isinstance(result, dict):\n",
    "            for key in (\"response\", \"output\", \"content\"):\n",
    "                if key in result and isinstance(result[key], str):\n",
    "                    return result[key].strip()\n",
    "            # Chat-style\n",
    "            if \"choices\" in result and isinstance(result[\"choices\"], list):\n",
    "                msg = result[\"choices\"][0].get(\"message\", {})\n",
    "                return msg.get(\"content\", \"\").strip()\n",
    "            if \"message\" in result and isinstance(result[\"message\"], dict):\n",
    "                return result[\"message\"].get(\"content\", \"\").strip()\n",
    "            return str(result)\n",
    "        if isinstance(result, list) and result:\n",
    "            return str(result[0])\n",
    "        return str(result)\n",
    "\n",
    "    def call_ollama_single(self, prompt: str) -> str:\n",
    "        if not prompt:\n",
    "            return \"Error: Empty prompt\"\n",
    "        payload = self.create_payload(prompt)\n",
    "        for attempt in range(RETRY_ATTEMPTS + 1):\n",
    "            try:\n",
    "                resp = self.session.post(self.url, json=payload, timeout=REQUEST_TIMEOUT)\n",
    "                if resp.status_code == 200:\n",
    "                    try:\n",
    "                        result = resp.json()\n",
    "                        text = self.extract_response_text(result)\n",
    "                        return self.normalize_response(text)\n",
    "                    except json.JSONDecodeError as e:\n",
    "                        return f\"JSONError: {e}\"\n",
    "                else:\n",
    "                    return f\"HTTPError: {resp.status_code}\"\n",
    "            except requests.exceptions.Timeout:\n",
    "                return \"TimeoutError: Request timed out\"\n",
    "            except requests.exceptions.ConnectionError as e:\n",
    "                return f\"ConnectionError: {e}\"\n",
    "            except Exception as e:\n",
    "                return f\"UnexpectedError: {e}\"\n",
    "        return \"Error: All retry attempts failed\"\n",
    "    \n",
    "    def process_all_parallel(self, data_items: List[tuple]) -> List[tuple]:\n",
    "        \"\"\"Process items in parallel and report accurate ETA.\"\"\"\n",
    "        total = len(data_items)\n",
    "        completed = 0\n",
    "        results: List[tuple] = []\n",
    "        start = time.perf_counter()\n",
    "        \n",
    "        logger.info(f\"Starting processing of {total} items with {self.max_workers} workers\")\n",
    "        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:\n",
    "            future_to_idx = {executor.submit(self.call_ollama_single, prompt): idx\n",
    "                             for idx, prompt in data_items}\n",
    "            \n",
    "            for future in as_completed(future_to_idx):\n",
    "                idx = future_to_idx[future]\n",
    "                try:\n",
    "                    res = future.result()\n",
    "                except Exception as e:\n",
    "                    res = f\"FutureError: {e}\"\n",
    "                    logger.error(f\"Error in future for row {idx}: {e}\")\n",
    "                results.append((idx, res))\n",
    "                completed += 1\n",
    "\n",
    "                if completed % 5 == 0 or completed == total:\n",
    "                    elapsed = time.perf_counter() - start\n",
    "                    avg = elapsed / completed\n",
    "                    remaining = total - completed\n",
    "                    eta_sec = remaining * avg\n",
    "                    eta_min = eta_sec / 60\n",
    "                    logger.info(\n",
    "                        f\"Progress: {completed}/{total} ({100*completed/total:.1f}%) | \"\n",
    "                        f\"Avg: {avg:.1f}s | ETA: {eta_min:.1f}min\"\n",
    "                    )\n",
    "\n",
    "        return results\n",
    "\n",
    "def process_file_optimized(input_path: str, processor: OllamaProcessor):\n",
    "    match = re.search(r\"templ-\\d+\", input_path)\n",
    "    if not match:\n",
    "        logger.warning(f\"Skipping {input_path}, no template tag found.\")\n",
    "        return\n",
    "    template_tag = match.group()\n",
    "    model_tag = processor.model.replace(\":\", \"-\")\n",
    "    output_path = f\"relevance_210725_completions_{model_tag}-{template_tag}.csv\"\n",
    "    \n",
    "    logger.info(f\"Processing {input_path} -> {output_path}\")\n",
    "    try:\n",
    "        df = pd.read_csv(input_path, encoding='utf-8')\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to read {input_path}: {e}\")\n",
    "        return\n",
    "    if df.empty:\n",
    "        logger.error(f\"No data in {input_path}\")\n",
    "        return\n",
    "    \n",
    "    # Ensure eval_prompt column\n",
    "    if 'eval_prompt' not in df.columns:\n",
    "        df.rename(columns={df.columns[-1]: 'eval_prompt'}, inplace=True)\n",
    "    \n",
    "    # changed to None to run through templ-1 file\n",
    "    if TEST_SUBSET and len(df) > TEST_SUBSET:\n",
    "        df = df.head(TEST_SUBSET).copy()\n",
    "        logger.info(f\"TEST MODE: Processing first {len(df)} rows\")\n",
    "    \n",
    "    df['eval_completion'] = None\n",
    "    df['model'] = processor.model\n",
    "    \n",
    "    valid = df['eval_prompt'].notna() & (df['eval_prompt'] != \"\")\n",
    "    valid_indices = df[valid].index.tolist()\n",
    "    if not valid_indices:\n",
    "        logger.error(\"No valid prompts to process.\")\n",
    "        return\n",
    "    \n",
    "    # Parallel processing\n",
    "    data = [(idx, df.at[idx, 'eval_prompt']) for idx in valid_indices]\n",
    "    t0 = time.perf_counter()\n",
    "    results = processor.process_all_parallel(data)\n",
    "    total_time = time.perf_counter() - t0\n",
    "    \n",
    "    # Update DataFrame\n",
    "    for idx, completion in results:\n",
    "        df.at[idx, 'eval_completion'] = completion\n",
    "    \n",
    "    # Summary stats\n",
    "    successful = df['eval_completion'].notna().sum()\n",
    "    errors = df['eval_completion'].str.contains('Error:', na=False).sum()\n",
    "    timeouts = df['eval_completion'].str.contains('TimeoutError:', na=False).sum()\n",
    "    rate = 100 * (successful - errors) / len(df)\n",
    "    \n",
    "    logger.info(\"=\"*50)\n",
    "    logger.info(f\"Finished in {total_time/60:.2f} minutes\")\n",
    "    logger.info(f\"Rows processed: {len(df)} (Valid: {len(valid_indices)})\")\n",
    "    logger.info(f\"Successes: {successful - errors}, Errors: {errors} (Timeouts: {timeouts})\")\n",
    "    logger.info(f\"Success rate: {rate:.1f}%\")\n",
    "    logger.info(f\"Throughput: {len(valid_indices)/(total_time/60):.1f} req/min\")\n",
    "    logger.info(\"=\"*50)\n",
    "    \n",
    "    # Save\n",
    "    try:\n",
    "        df.to_csv(output_path, index=False, encoding='utf-8')\n",
    "        logger.info(f\"Saved to {output_path}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to save {output_path}: {e}\")\n",
    "\n",
    "def main():\n",
    "    logger.info(\"Starting optimized processing\")\n",
    "    logger.info(f\"- Model: {model_name}\")\n",
    "    logger.info(f\"- Ollama URL: {ollama_url}\")\n",
    "    logger.info(f\"- Max workers: {MAX_WORKERS}\")\n",
    "    if TEST_SUBSET:\n",
    "        logger.info(f\"- TEST SUBSET: {TEST_SUBSET} rows\")\n",
    "    \n",
    "    processor = OllamaProcessor(ollama_url, model_name, MAX_WORKERS)\n",
    "    \n",
    "    for input_path in input_files:\n",
    "        if not Path(input_path).exists():\n",
    "            logger.error(f\"File not found: {input_path}\")\n",
    "            continue\n",
    "        process_file_optimized(input_path, processor)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "\n",
    "# one file took 184 minutes, only 4.6% timeout error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1867ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# original, took to long\n",
    "\n",
    "for input_path in input_files:\n",
    "    # Determine output file name based on template number and model\n",
    "    template_match = re.search(r\"templ-\\d+\", input_path)\n",
    "    if not template_match:\n",
    "        continue\n",
    "    template_tag = template_match.group() \n",
    "    # replace colon with hyphen in models\n",
    "    model_tag = model_name.replace(\":\", \"-\")\n",
    "    output_path = f\"relevance_210725_completions_{model_tag}-{template_tag}.csv\"\n",
    "    \n",
    "    with open(input_path, newline='', encoding='utf-8') as infile, \\\n",
    "         open(output_path, 'w', newline='', encoding='utf-8') as outfile:\n",
    "        reader = csv.reader(infile)\n",
    "        writer = csv.writer(outfile)\n",
    "        \n",
    "        # Read the header and append new column names\n",
    "        header = next(reader)\n",
    "        new_header = header + [\"eval_completion\", \"model\"]\n",
    "        writer.writerow(new_header)\n",
    "        \n",
    "        # Iterate over each row in the input CSV\n",
    "        for row in reader:\n",
    "            if not row:  # skip empty lines if any\n",
    "                continue\n",
    "            prompt = row[-1]  # eval_prompt is the last col\n",
    "            \n",
    "            # Prepare the JSON payload for Ollama API\n",
    "            payload = {\n",
    "                \"model\": model_name,\n",
    "                \"prompt\": prompt,\n",
    "                \"stream\": False  # get a single JSON response instead of stream\n",
    "            }\n",
    "            \n",
    "            try:\n",
    "                response = requests.post(ollama_url, headers={\"Content-Type\": \"application/json\"},\n",
    "                                         data=json.dumps(payload))\n",
    "            except Exception as e:\n",
    "                # If there's a connection error or similar, you might want to handle it\n",
    "                print(f\"Error calling Ollama API for prompt: {prompt[:30]}... \\n{e}\")\n",
    "                continue\n",
    "            \n",
    "            eval_completion = \"\"\n",
    "            if response.status_code == 200:\n",
    "                # Parse JSON response from Ollama\n",
    "                try:\n",
    "                    result = response.json()\n",
    "                except ValueError:\n",
    "                    # If response is not a valid JSON (unexpected), use raw text\n",
    "                    result_text = response.text.strip()\n",
    "                    # Determine yes/no from text\n",
    "                    if result_text.lower().startswith(\"yes\"):\n",
    "                        eval_completion = \"Yes\"\n",
    "                    elif result_text.lower().startswith(\"no\"):\n",
    "                        eval_completion = \"No\"\n",
    "                    else:\n",
    "                        eval_completion = result_text  # fallback to whatever it is\n",
    "                else:\n",
    "                    # Ollama's response JSON might have the output text in a field.\n",
    "                    # We attempt common possible keys.\n",
    "                    if \"response\" in result:\n",
    "                        result_text = result[\"response\"]\n",
    "                    elif \"output\" in result:\n",
    "                        result_text = result[\"output\"]\n",
    "                    elif \"content\" in result:\n",
    "                        # If using chat-style response, it might be nested:\n",
    "                        # e.g., {\"model\": ..., \"choices\": [{\"message\": {\"role\": \"assistant\", \"content\": \"Yes\"}}], ...}\n",
    "                        result_text = result.get(\"content\", \"\") or result.get(\"message\", {}).get(\"content\", \"\")\n",
    "                    else:\n",
    "                        # If none of the known keys, use the full JSON string as fallback\n",
    "                        result_text = str(result)\n",
    "                    result_text = str(result_text).strip()\n",
    "                    # Normalize to \"Yes\" or \"No\"\n",
    "                    if result_text.lower().startswith(\"yes\"):\n",
    "                        eval_completion = \"Yes\"\n",
    "                    elif result_text.lower().startswith(\"no\"):\n",
    "                        eval_completion = \"No\"\n",
    "                    else:\n",
    "                        eval_completion = result_text\n",
    "            else:\n",
    "                # If the API call failed (non-200 status), record the status or an error\n",
    "                eval_completion = f\"Error: HTTP {response.status_code}\"\n",
    "            \n",
    "            # Append the new columns to the row\n",
    "            row_with_output = row + [eval_completion, model_name]\n",
    "            writer.writerow(row_with_output)\n",
    "\n",
    "    print(f\"Completed {input_path} -> {output_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
