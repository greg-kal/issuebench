{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6b896754",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "import requests\n",
    "import re\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1092a2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model, ollama API, input files\n",
    "model_name = \"deepseek-r1:70b\" # model name here\n",
    "ollama_url = \"http://localhost:11434/api/generate\"\n",
    "\n",
    "input_files = [\n",
    "    \"relevance_210725_prompts_templ-1.csv\",\n",
    "    \"relevance_210725_prompts_templ-2.csv\",\n",
    "    \"relevance_210725_prompts_templ-3.csv\",\n",
    "    \"relevance_210725_prompts_templ-4.csv\",\n",
    "    \"relevance_210725_prompts_templ-5.csv\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2e2c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#new version to (try to) handle concurrent requests\n",
    "\n",
    "import asyncio\n",
    "import aiohttp\n",
    "import csv\n",
    "import json\n",
    "import re\n",
    "from typing import List, Dict, Tuple\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Fix for Jupyter notebooks\n",
    "try:\n",
    "    import nest_asyncio\n",
    "    nest_asyncio.apply()\n",
    "except ImportError:\n",
    "    print(\"Installing nest_asyncio for Jupyter compatibility...\")\n",
    "    import subprocess\n",
    "    import sys\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"nest_asyncio\"])\n",
    "    import nest_asyncio\n",
    "    nest_asyncio.apply()\n",
    "\n",
    "async def process_prompt(session: aiohttp.ClientSession, ollama_url: str, \n",
    "                        model_name: str, prompt: str, row: List[str], \n",
    "                        semaphore: asyncio.Semaphore) -> List[str]:\n",
    "    \"\"\"Process a single prompt with rate limiting via semaphore.\"\"\"\n",
    "    async with semaphore:  # Limit concurrent requests\n",
    "        payload = {\n",
    "            \"model\": model_name,\n",
    "            \"prompt\": prompt,\n",
    "            \"stream\": False\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            async with session.post(\n",
    "                ollama_url, \n",
    "                headers={\"Content-Type\": \"application/json\"},\n",
    "                json=payload,\n",
    "                timeout=aiohttp.ClientTimeout(total=30)  # changed from 30\n",
    "            ) as response:\n",
    "                eval_completion = \"\"\n",
    "                if response.status == 200:\n",
    "                    try:\n",
    "                        result = await response.json()\n",
    "                        # Extract response text\n",
    "                        result_text = (\n",
    "                            result.get(\"response\") or \n",
    "                            result.get(\"output\") or \n",
    "                            result.get(\"content\") or\n",
    "                            result.get(\"message\", {}).get(\"content\", \"\") or\n",
    "                            str(result)\n",
    "                        ).strip()\n",
    "                        \n",
    "                        # Normalize to Yes/No\n",
    "                        if result_text.lower().startswith(\"yes\"):\n",
    "                            eval_completion = \"Yes\"\n",
    "                        elif result_text.lower().startswith(\"no\"):\n",
    "                            eval_completion = \"No\"\n",
    "                        else:\n",
    "                            eval_completion = result_text\n",
    "                    except (ValueError, KeyError):\n",
    "                        result_text = await response.text()\n",
    "                        if result_text.lower().startswith(\"yes\"):\n",
    "                            eval_completion = \"Yes\"\n",
    "                        elif result_text.lower().startswith(\"no\"):\n",
    "                            eval_completion = \"No\"\n",
    "                        else:\n",
    "                            eval_completion = result_text.strip()\n",
    "                else:\n",
    "                    eval_completion = f\"Error: HTTP {response.status}\"\n",
    "                    \n",
    "        except asyncio.TimeoutError:\n",
    "            eval_completion = \"Error: Timeout\"\n",
    "        except Exception as e:\n",
    "            eval_completion = f\"Error: {str(e)}\"\n",
    "        \n",
    "        return row + [eval_completion, model_name]\n",
    "\n",
    "async def process_file_async(input_path: str, output_path: str, model_name: str, \n",
    "                           ollama_url: str, max_concurrent: int = 10) -> None:\n",
    "    \"\"\"Process a single CSV file with async concurrent requests.\"\"\"\n",
    "    \n",
    "    # Read all rows first\n",
    "    rows_to_process = []\n",
    "    with open(input_path, newline='', encoding='utf-8') as infile:\n",
    "        reader = csv.reader(infile)\n",
    "        header = next(reader)\n",
    "        new_header = header + [\"eval_completion\", \"model\"]\n",
    "        \n",
    "        for row in reader:\n",
    "            if row:  # Skip empty rows\n",
    "                rows_to_process.append(row)\n",
    "    \n",
    "    if not rows_to_process:\n",
    "        # Write header only if no data\n",
    "        with open(output_path, 'w', newline='', encoding='utf-8') as outfile:\n",
    "            writer = csv.writer(outfile)\n",
    "            writer.writerow(new_header)\n",
    "        return\n",
    "    \n",
    "    # Process with concurrent requests\n",
    "    semaphore = asyncio.Semaphore(max_concurrent)\n",
    "    connector = aiohttp.TCPConnector(limit=max_concurrent * 2)  # Connection pooling\n",
    "    \n",
    "    async with aiohttp.ClientSession(connector=connector) as session:\n",
    "        # Create tasks for all rows\n",
    "        tasks = [\n",
    "            process_prompt(session, ollama_url, model_name, row[-1], row, semaphore)\n",
    "            for row in rows_to_process\n",
    "        ]\n",
    "        \n",
    "        # Process in batches to manage memory\n",
    "        batch_size = 100\n",
    "        all_results = []\n",
    "        start_time = time.time()\n",
    "        total_batches = (len(tasks) - 1) // batch_size + 1\n",
    "        \n",
    "        for i in range(0, len(tasks), batch_size):\n",
    "            batch_start = time.time()\n",
    "            batch = tasks[i:i + batch_size]\n",
    "            batch_results = await asyncio.gather(*batch, return_exceptions=True)\n",
    "            \n",
    "            # Handle exceptions\n",
    "            for j, result in enumerate(batch_results):\n",
    "                if isinstance(result, Exception):\n",
    "                    # Create error row\n",
    "                    original_row = rows_to_process[i + j]\n",
    "                    error_row = original_row + [f\"Error: {str(result)}\", model_name]\n",
    "                    all_results.append(error_row)\n",
    "                else:\n",
    "                    all_results.append(result)\n",
    "            \n",
    "            # Calculate ETA\n",
    "            current_batch = (i // batch_size) + 1\n",
    "            elapsed_time = time.time() - start_time\n",
    "            avg_time_per_batch = elapsed_time / current_batch\n",
    "            remaining_batches = total_batches - current_batch\n",
    "            eta_seconds = remaining_batches * avg_time_per_batch\n",
    "            eta_time = datetime.now() + timedelta(seconds=eta_seconds)\n",
    "            \n",
    "            batch_time = time.time() - batch_start\n",
    "            print(f\"Batch {current_batch}/{total_batches} completed in {batch_time:.1f}s | \"\n",
    "                  f\"Progress: {(current_batch/total_batches)*100:.1f}% | \"\n",
    "                  f\"ETA: {eta_time.strftime('%H:%M:%S')} ({eta_seconds/60:.1f}m remaining)\")\n",
    "    \n",
    "    # Write all results\n",
    "    with open(output_path, 'w', newline='', encoding='utf-8') as outfile:\n",
    "        writer = csv.writer(outfile)\n",
    "        writer.writerow(new_header)\n",
    "        writer.writerows(all_results)\n",
    "\n",
    "def process_files_optimized(input_files: List[str], model_name: str, ollama_url: str,\n",
    "                          max_concurrent: int = 10) -> None:\n",
    "    \"\"\"Main function to process multiple CSV files - Jupyter notebook compatible.\"\"\"\n",
    "    \n",
    "    async def process_all_files():\n",
    "        file_tasks = []\n",
    "        total_files = 0\n",
    "        \n",
    "        # Count valid files and create tasks\n",
    "        for input_path in input_files:\n",
    "            template_match = re.search(r\"templ-\\d+\", input_path)\n",
    "            if not template_match:\n",
    "                continue\n",
    "            template_tag = template_match.group()\n",
    "            model_tag = model_name.replace(\":\", \"-\")\n",
    "            output_path = f\"relevance_210725_completions_{model_tag}-{template_tag}.csv\"\n",
    "            \n",
    "            # Create async task for this file\n",
    "            task = process_file_async(input_path, output_path, model_name, ollama_url, max_concurrent)\n",
    "            file_tasks.append((task, input_path, output_path))\n",
    "            total_files += 1\n",
    "        \n",
    "        print(f\"Starting processing of {total_files} files...\")\n",
    "        overall_start = time.time()\n",
    "        \n",
    "        # Process files concurrently (but you might want to limit this too)\n",
    "        for file_idx, (task, input_path, output_path) in enumerate(file_tasks):\n",
    "            file_start_time = time.time()\n",
    "            await task\n",
    "            file_end_time = time.time()\n",
    "            file_duration = file_end_time - file_start_time\n",
    "            \n",
    "            # Calculate overall ETA\n",
    "            files_completed = file_idx + 1\n",
    "            elapsed_total = time.time() - overall_start\n",
    "            avg_time_per_file = elapsed_total / files_completed\n",
    "            remaining_files = total_files - files_completed\n",
    "            eta_seconds = remaining_files * avg_time_per_file\n",
    "            eta_time = datetime.now() + timedelta(seconds=eta_seconds)\n",
    "            \n",
    "            print(f\"File {files_completed}/{total_files} completed: {input_path} -> {output_path}\")\n",
    "            print(f\"  File duration: {file_duration:.1f}s | Overall progress: {(files_completed/total_files)*100:.1f}%\")\n",
    "            if remaining_files > 0:\n",
    "                print(f\"  Overall ETA: {eta_time.strftime('%H:%M:%S')} ({eta_seconds/60:.1f}m remaining)\")\n",
    "            print()\n",
    "        \n",
    "        total_duration = time.time() - overall_start\n",
    "        print(f\"All {total_files} files completed in {total_duration/60:.1f} minutes!\")\n",
    "    \n",
    "    # This will now work in Jupyter notebooks\n",
    "    asyncio.run(process_all_files())\n",
    "\n",
    "# Alternative: Process files sequentially but requests within each file concurrently\n",
    "def process_files_sequential_hybrid(input_files: List[str], model_name: str, \n",
    "                                  ollama_url: str, max_concurrent: int = 10) -> None:\n",
    "    \"\"\"Process files one by one, but requests within each file concurrently.\"\"\"\n",
    "    valid_files = [f for f in input_files if re.search(r\"templ-\\d+\", f)]\n",
    "    total_files = len(valid_files)\n",
    "    overall_start = time.time()\n",
    "    \n",
    "    print(f\"Starting sequential processing of {total_files} files...\")\n",
    "    \n",
    "    for file_idx, input_path in enumerate(valid_files):\n",
    "        template_match = re.search(r\"templ-\\d+\", input_path)\n",
    "        template_tag = template_match.group()\n",
    "        model_tag = model_name.replace(\":\", \"-\")\n",
    "        output_path = f\"relevance_210725_completions_{model_tag}-{template_tag}.csv\"\n",
    "        \n",
    "        file_start_time = time.time()\n",
    "        asyncio.run(process_file_async(input_path, output_path, model_name, ollama_url, max_concurrent))\n",
    "        file_end_time = time.time()\n",
    "        file_duration = file_end_time - file_start_time\n",
    "        \n",
    "        # Calculate ETA\n",
    "        files_completed = file_idx + 1\n",
    "        elapsed_total = time.time() - overall_start\n",
    "        avg_time_per_file = elapsed_total / files_completed\n",
    "        remaining_files = total_files - files_completed\n",
    "        eta_seconds = remaining_files * avg_time_per_file\n",
    "        eta_time = datetime.now() + timedelta(seconds=eta_seconds)\n",
    "        \n",
    "        print(f\"File {files_completed}/{total_files} completed: {input_path} -> {output_path}\")\n",
    "        print(f\"  Duration: {file_duration:.1f}s | Progress: {(files_completed/total_files)*100:.1f}%\")\n",
    "        if remaining_files > 0:\n",
    "            print(f\"  ETA: {eta_time.strftime('%H:%M:%S')} ({eta_seconds/60:.1f}m remaining)\")\n",
    "        print()\n",
    "    \n",
    "    total_duration = time.time() - overall_start\n",
    "    print(f\"All {total_files} files completed in {total_duration/60:.1f} minutes!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7702c7df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting processing of 5 files...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mprocess_files_optimized\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_files\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_files\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mollama_url\u001b[49m\u001b[43m=\u001b[49m\u001b[43mollama_url\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_concurrent\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 203\u001b[39m, in \u001b[36mprocess_files_optimized\u001b[39m\u001b[34m(input_files, model_name, ollama_url, max_concurrent)\u001b[39m\n\u001b[32m    200\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mAll \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_files\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m files completed in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_duration/\u001b[32m60\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.1f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m minutes!\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    202\u001b[39m \u001b[38;5;66;03m# This will now work in Jupyter notebooks\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m203\u001b[39m \u001b[43masyncio\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_all_files\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/data/gregIB/issuebench/.venv/lib/python3.12/site-packages/nest_asyncio.py:30\u001b[39m, in \u001b[36m_patch_asyncio.<locals>.run\u001b[39m\u001b[34m(main, debug)\u001b[39m\n\u001b[32m     28\u001b[39m task = asyncio.ensure_future(main)\n\u001b[32m     29\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_until_complete\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m     32\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m task.done():\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/data/gregIB/issuebench/.venv/lib/python3.12/site-packages/nest_asyncio.py:92\u001b[39m, in \u001b[36m_patch_loop.<locals>.run_until_complete\u001b[39m\u001b[34m(self, future)\u001b[39m\n\u001b[32m     90\u001b[39m     f._log_destroy_pending = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m     91\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m f.done():\n\u001b[32m---> \u001b[39m\u001b[32m92\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     93\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._stopping:\n\u001b[32m     94\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/data/gregIB/issuebench/.venv/lib/python3.12/site-packages/nest_asyncio.py:115\u001b[39m, in \u001b[36m_patch_loop.<locals>._run_once\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    108\u001b[39m     heappop(scheduled)\n\u001b[32m    110\u001b[39m timeout = (\n\u001b[32m    111\u001b[39m     \u001b[32m0\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ready \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._stopping\n\u001b[32m    112\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mmin\u001b[39m(\u001b[38;5;28mmax\u001b[39m(\n\u001b[32m    113\u001b[39m         scheduled[\u001b[32m0\u001b[39m]._when - \u001b[38;5;28mself\u001b[39m.time(), \u001b[32m0\u001b[39m), \u001b[32m86400\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m scheduled\n\u001b[32m    114\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m115\u001b[39m event_list = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_selector\u001b[49m\u001b[43m.\u001b[49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    116\u001b[39m \u001b[38;5;28mself\u001b[39m._process_events(event_list)\n\u001b[32m    118\u001b[39m end_time = \u001b[38;5;28mself\u001b[39m.time() + \u001b[38;5;28mself\u001b[39m._clock_resolution\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/selectors.py:468\u001b[39m, in \u001b[36mEpollSelector.select\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    466\u001b[39m ready = []\n\u001b[32m    467\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m468\u001b[39m     fd_event_list = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_selector\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpoll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_ev\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    469\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n\u001b[32m    470\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ready\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "process_files_optimized(input_files=input_files, model_name=model_name, ollama_url=ollama_url, max_concurrent=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e1867ee1",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 37\u001b[39m\n\u001b[32m     30\u001b[39m payload = {\n\u001b[32m     31\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m: model_name,\n\u001b[32m     32\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mprompt\u001b[39m\u001b[33m\"\u001b[39m: prompt,\n\u001b[32m     33\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m  \u001b[38;5;66;03m# get a single JSON response instead of stream\u001b[39;00m\n\u001b[32m     34\u001b[39m }\n\u001b[32m     36\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m37\u001b[39m     response = \u001b[43mrequests\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpost\u001b[49m\u001b[43m(\u001b[49m\u001b[43mollama_url\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mContent-Type\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mapplication/json\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     38\u001b[39m \u001b[43m                             \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mjson\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdumps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpayload\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     39\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     40\u001b[39m     \u001b[38;5;66;03m# If there's a connection error or similar, you might want to handle it\u001b[39;00m\n\u001b[32m     41\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mError calling Ollama API for prompt: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprompt[:\u001b[32m30\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m... \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/data/gregIB/issuebench/.venv/lib/python3.12/site-packages/requests/api.py:115\u001b[39m, in \u001b[36mpost\u001b[39m\u001b[34m(url, data, json, **kwargs)\u001b[39m\n\u001b[32m    103\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(url, data=\u001b[38;5;28;01mNone\u001b[39;00m, json=\u001b[38;5;28;01mNone\u001b[39;00m, **kwargs):\n\u001b[32m    104\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Sends a POST request.\u001b[39;00m\n\u001b[32m    105\u001b[39m \n\u001b[32m    106\u001b[39m \u001b[33;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    112\u001b[39m \u001b[33;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[32m    113\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m115\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpost\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson\u001b[49m\u001b[43m=\u001b[49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/data/gregIB/issuebench/.venv/lib/python3.12/site-packages/requests/api.py:59\u001b[39m, in \u001b[36mrequest\u001b[39m\u001b[34m(method, url, **kwargs)\u001b[39m\n\u001b[32m     55\u001b[39m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[32m     56\u001b[39m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[32m     57\u001b[39m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m sessions.Session() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/data/gregIB/issuebench/.venv/lib/python3.12/site-packages/requests/sessions.py:589\u001b[39m, in \u001b[36mSession.request\u001b[39m\u001b[34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[39m\n\u001b[32m    584\u001b[39m send_kwargs = {\n\u001b[32m    585\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtimeout\u001b[39m\u001b[33m\"\u001b[39m: timeout,\n\u001b[32m    586\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mallow_redirects\u001b[39m\u001b[33m\"\u001b[39m: allow_redirects,\n\u001b[32m    587\u001b[39m }\n\u001b[32m    588\u001b[39m send_kwargs.update(settings)\n\u001b[32m--> \u001b[39m\u001b[32m589\u001b[39m resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    591\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/data/gregIB/issuebench/.venv/lib/python3.12/site-packages/requests/sessions.py:703\u001b[39m, in \u001b[36mSession.send\u001b[39m\u001b[34m(self, request, **kwargs)\u001b[39m\n\u001b[32m    700\u001b[39m start = preferred_clock()\n\u001b[32m    702\u001b[39m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m703\u001b[39m r = \u001b[43madapter\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    705\u001b[39m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[32m    706\u001b[39m elapsed = preferred_clock() - start\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/data/gregIB/issuebench/.venv/lib/python3.12/site-packages/requests/adapters.py:667\u001b[39m, in \u001b[36mHTTPAdapter.send\u001b[39m\u001b[34m(self, request, stream, timeout, verify, cert, proxies)\u001b[39m\n\u001b[32m    664\u001b[39m     timeout = TimeoutSauce(connect=timeout, read=timeout)\n\u001b[32m    666\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m667\u001b[39m     resp = \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    668\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    669\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    670\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    671\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    672\u001b[39m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    673\u001b[39m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    674\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    675\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    676\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    677\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    678\u001b[39m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    679\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    681\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m    682\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request=request)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/data/gregIB/issuebench/.venv/lib/python3.12/site-packages/urllib3/connectionpool.py:787\u001b[39m, in \u001b[36mHTTPConnectionPool.urlopen\u001b[39m\u001b[34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[39m\n\u001b[32m    784\u001b[39m response_conn = conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    786\u001b[39m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m787\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    788\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    789\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    790\u001b[39m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    791\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    792\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    793\u001b[39m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    794\u001b[39m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    795\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    796\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    797\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    798\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    799\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    800\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    802\u001b[39m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[32m    803\u001b[39m clean_exit = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/data/gregIB/issuebench/.venv/lib/python3.12/site-packages/urllib3/connectionpool.py:534\u001b[39m, in \u001b[36mHTTPConnectionPool._make_request\u001b[39m\u001b[34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[39m\n\u001b[32m    532\u001b[39m \u001b[38;5;66;03m# Receive the response from the server\u001b[39;00m\n\u001b[32m    533\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m534\u001b[39m     response = \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    535\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    536\u001b[39m     \u001b[38;5;28mself\u001b[39m._raise_timeout(err=e, url=url, timeout_value=read_timeout)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/data/gregIB/issuebench/.venv/lib/python3.12/site-packages/urllib3/connection.py:565\u001b[39m, in \u001b[36mHTTPConnection.getresponse\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    562\u001b[39m _shutdown = \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.sock, \u001b[33m\"\u001b[39m\u001b[33mshutdown\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    564\u001b[39m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m565\u001b[39m httplib_response = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    567\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    568\u001b[39m     assert_header_parsing(httplib_response.msg)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/http/client.py:1428\u001b[39m, in \u001b[36mHTTPConnection.getresponse\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1426\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1427\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1428\u001b[39m         \u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1429\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[32m   1430\u001b[39m         \u001b[38;5;28mself\u001b[39m.close()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/http/client.py:331\u001b[39m, in \u001b[36mHTTPResponse.begin\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    329\u001b[39m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[32m    330\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m331\u001b[39m     version, status, reason = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    332\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m status != CONTINUE:\n\u001b[32m    333\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/http/client.py:292\u001b[39m, in \u001b[36mHTTPResponse._read_status\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    291\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m292\u001b[39m     line = \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_MAXLINE\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m, \u001b[33m\"\u001b[39m\u001b[33miso-8859-1\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    293\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) > _MAXLINE:\n\u001b[32m    294\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[33m\"\u001b[39m\u001b[33mstatus line\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/socket.py:707\u001b[39m, in \u001b[36mSocketIO.readinto\u001b[39m\u001b[34m(self, b)\u001b[39m\n\u001b[32m    705\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m    706\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m707\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    708\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[32m    709\u001b[39m         \u001b[38;5;28mself\u001b[39m._timeout_occurred = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# original, took to long\n",
    "\n",
    "for input_path in input_files:\n",
    "    # Determine output file name based on template number and model\n",
    "    template_match = re.search(r\"templ-\\d+\", input_path)\n",
    "    if not template_match:\n",
    "        continue\n",
    "    template_tag = template_match.group() \n",
    "    # replace colon with hyphen in models\n",
    "    model_tag = model_name.replace(\":\", \"-\")\n",
    "    output_path = f\"relevance_210725_completions_{model_tag}-{template_tag}.csv\"\n",
    "    \n",
    "    with open(input_path, newline='', encoding='utf-8') as infile, \\\n",
    "         open(output_path, 'w', newline='', encoding='utf-8') as outfile:\n",
    "        reader = csv.reader(infile)\n",
    "        writer = csv.writer(outfile)\n",
    "        \n",
    "        # Read the header and append new column names\n",
    "        header = next(reader)\n",
    "        new_header = header + [\"eval_completion\", \"model\"]\n",
    "        writer.writerow(new_header)\n",
    "        \n",
    "        # Iterate over each row in the input CSV\n",
    "        for row in reader:\n",
    "            if not row:  # skip empty lines if any\n",
    "                continue\n",
    "            prompt = row[-1]  # eval_prompt is the last col\n",
    "            \n",
    "            # Prepare the JSON payload for Ollama API\n",
    "            payload = {\n",
    "                \"model\": model_name,\n",
    "                \"prompt\": prompt,\n",
    "                \"stream\": False  # get a single JSON response instead of stream\n",
    "            }\n",
    "            \n",
    "            try:\n",
    "                response = requests.post(ollama_url, headers={\"Content-Type\": \"application/json\"},\n",
    "                                         data=json.dumps(payload))\n",
    "            except Exception as e:\n",
    "                # If there's a connection error or similar, you might want to handle it\n",
    "                print(f\"Error calling Ollama API for prompt: {prompt[:30]}... \\n{e}\")\n",
    "                continue\n",
    "            \n",
    "            eval_completion = \"\"\n",
    "            if response.status_code == 200:\n",
    "                # Parse JSON response from Ollama\n",
    "                try:\n",
    "                    result = response.json()\n",
    "                except ValueError:\n",
    "                    # If response is not a valid JSON (unexpected), use raw text\n",
    "                    result_text = response.text.strip()\n",
    "                    # Determine yes/no from text\n",
    "                    if result_text.lower().startswith(\"yes\"):\n",
    "                        eval_completion = \"Yes\"\n",
    "                    elif result_text.lower().startswith(\"no\"):\n",
    "                        eval_completion = \"No\"\n",
    "                    else:\n",
    "                        eval_completion = result_text  # fallback to whatever it is\n",
    "                else:\n",
    "                    # Ollama's response JSON might have the output text in a field.\n",
    "                    # We attempt common possible keys.\n",
    "                    if \"response\" in result:\n",
    "                        result_text = result[\"response\"]\n",
    "                    elif \"output\" in result:\n",
    "                        result_text = result[\"output\"]\n",
    "                    elif \"content\" in result:\n",
    "                        # If using chat-style response, it might be nested:\n",
    "                        # e.g., {\"model\": ..., \"choices\": [{\"message\": {\"role\": \"assistant\", \"content\": \"Yes\"}}], ...}\n",
    "                        result_text = result.get(\"content\", \"\") or result.get(\"message\", {}).get(\"content\", \"\")\n",
    "                    else:\n",
    "                        # If none of the known keys, use the full JSON string as fallback\n",
    "                        result_text = str(result)\n",
    "                    result_text = str(result_text).strip()\n",
    "                    # Normalize to \"Yes\" or \"No\"\n",
    "                    if result_text.lower().startswith(\"yes\"):\n",
    "                        eval_completion = \"Yes\"\n",
    "                    elif result_text.lower().startswith(\"no\"):\n",
    "                        eval_completion = \"No\"\n",
    "                    else:\n",
    "                        eval_completion = result_text\n",
    "            else:\n",
    "                # If the API call failed (non-200 status), record the status or an error\n",
    "                eval_completion = f\"Error: HTTP {response.status_code}\"\n",
    "            \n",
    "            # Append the new columns to the row\n",
    "            row_with_output = row + [eval_completion, model_name]\n",
    "            writer.writerow(row_with_output)\n",
    "\n",
    "    print(f\"Completed {input_path} -> {output_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
